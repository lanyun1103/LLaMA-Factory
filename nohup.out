[2023-11-06 11:58:19,979] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-06 11:58:22,290] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-11-06 11:58:22,290] [INFO] [runner.py:570:main] cmd = /root/miniconda3/envs/llama/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=9901 --enable_each_rank_log=None src/train_bash.py --deepspeed ds_config.json --stage sft --model_name_or_path /opt/qwenmodel --do_train True --finetuning_type lora --quantization_bit 4 --template chatml --flash_attn False --shift_attn False --dataset_dir data --dataset alpaca_en --cutoff_len 1024 --learning_rate 5e-05 --num_train_epochs 3.0 --max_samples 100000 --per_device_train_batch_size 2 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --max_grad_norm 1.0 --logging_steps 5 --save_steps 100 --warmup_steps 0 --neft_alpha 0 --train_on_prompt False --upcast_layernorm True --lora_rank 8 --lora_dropout 0.1 --lora_target c_attn --resume_lora_training True --output_dir saves/Qwen-14B-Chat/lora/2023-11-06-11-11-292 --fp16 True --plot_loss True
[2023-11-06 11:58:24,719] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-06 11:58:26,803] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2023-11-06 11:58:26,803] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2023-11-06 11:58:26,803] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2023-11-06 11:58:26,803] [INFO] [launch.py:163:main] dist_world_size=2
[2023-11-06 11:58:26,803] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2023-11-06 11:58:32,447] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-06 11:58:32,664] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
FlashAttention-2 is not installed, ignore this if you are not using FlashAttention.
FlashAttention-2 is not installed, ignore this if you are not using FlashAttention.
[2023-11-06 11:58:33,822] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-11-06 11:58:33,823] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-11-06 11:58:34,018] [INFO] [comm.py:637:init_distributed] cdb=None
11/06/2023 11:58:34 - WARNING - llmtuner.tuner.core.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|training_args.py:1784] 2023-11-06 11:58:34,032 >> PyTorch: setting up devices
/root/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/training_args.py:1697: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
Traceback (most recent call last):
  File "/opt/LLaMA-Factory/src/train_bash.py", line 14, in <module>
    main()
  File "/opt/LLaMA-Factory/src/train_bash.py", line 5, in main
    run_exp()
  File "/opt/LLaMA-Factory/src/llmtuner/tuner/tune.py", line 20, in run_exp
    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)
  File "/opt/LLaMA-Factory/src/llmtuner/tuner/core/parser.py", line 174, in get_train_args
    raise ValueError("Output directory already exists and is not empty. Please set `overwrite_output_dir`.")
ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.
11/06/2023 11:58:35 - WARNING - llmtuner.tuner.core.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
/root/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/training_args.py:1697: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
Traceback (most recent call last):
  File "/opt/LLaMA-Factory/src/train_bash.py", line 14, in <module>
    main()
  File "/opt/LLaMA-Factory/src/train_bash.py", line 5, in main
    run_exp()
  File "/opt/LLaMA-Factory/src/llmtuner/tuner/tune.py", line 20, in run_exp
    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)
  File "/opt/LLaMA-Factory/src/llmtuner/tuner/core/parser.py", line 174, in get_train_args
    raise ValueError("Output directory already exists and is not empty. Please set `overwrite_output_dir`.")
ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.
[2023-11-06 11:58:35,893] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 27932
[2023-11-06 11:58:35,894] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 27933
[2023-11-06 11:58:36,310] [ERROR] [launch.py:321:sigkill_handler] ['/root/miniconda3/envs/llama/bin/python', '-u', 'src/train_bash.py', '--local_rank=1', '--deepspeed', 'ds_config.json', '--stage', 'sft', '--model_name_or_path', '/opt/qwenmodel', '--do_train', 'True', '--finetuning_type', 'lora', '--quantization_bit', '4', '--template', 'chatml', '--flash_attn', 'False', '--shift_attn', 'False', '--dataset_dir', 'data', '--dataset', 'alpaca_en', '--cutoff_len', '1024', '--learning_rate', '5e-05', '--num_train_epochs', '3.0', '--max_samples', '100000', '--per_device_train_batch_size', '2', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'cosine', '--max_grad_norm', '1.0', '--logging_steps', '5', '--save_steps', '100', '--warmup_steps', '0', '--neft_alpha', '0', '--train_on_prompt', 'False', '--upcast_layernorm', 'True', '--lora_rank', '8', '--lora_dropout', '0.1', '--lora_target', 'c_attn', '--resume_lora_training', 'True', '--output_dir', 'saves/Qwen-14B-Chat/lora/2023-11-06-11-11-292', '--fp16', 'True', '--plot_loss', 'True'] exits with return code = 1
[2023-11-06 13:47:29,116] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-06 13:47:31,461] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-11-06 13:47:31,462] [INFO] [runner.py:570:main] cmd = /root/miniconda3/envs/llama/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=9901 --enable_each_rank_log=None src/train_bash.py --deepspeed ds_config.json --stage sft --model_name_or_path /opt/qwenmodel --do_train True --finetuning_type lora --quantization_bit 4 --template chatml --flash_attn False --shift_attn False --dataset_dir data --dataset alpaca_en --cutoff_len 1024 --learning_rate 5e-05 --num_train_epochs 3.0 --max_samples 100000 --per_device_train_batch_size 3 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --max_grad_norm 1.0 --logging_steps 5 --save_steps 100 --warmup_steps 0 --neft_alpha 0 --train_on_prompt False --upcast_layernorm True --lora_rank 8 --lora_dropout 0.1 --lora_target c_attn --resume_lora_training True --output_dir saves/Qwen-14B-Chat/lora/2023-11-06-11-11-294 --fp16 True --plot_loss True
[2023-11-06 13:47:33,839] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-06 13:47:35,712] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2023-11-06 13:47:35,712] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2023-11-06 13:47:35,712] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2023-11-06 13:47:35,712] [INFO] [launch.py:163:main] dist_world_size=2
[2023-11-06 13:47:35,712] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2023-11-06 13:47:42,982] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-06 13:47:42,999] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
FlashAttention-2 is not installed, ignore this if you are not using FlashAttention.
FlashAttention-2 is not installed, ignore this if you are not using FlashAttention.
[2023-11-06 13:47:44,420] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-11-06 13:47:44,420] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-11-06 13:47:44,445] [INFO] [comm.py:637:init_distributed] cdb=None
11/06/2023 13:47:44 - WARNING - llmtuner.tuner.core.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|training_args.py:1784] 2023-11-06 13:47:44,458 >> PyTorch: setting up devices
/root/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/training_args.py:1697: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
11/06/2023 13:47:44 - INFO - llmtuner.tuner.core.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
11/06/2023 13:47:44 - INFO - llmtuner.tuner.core.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=saves/Qwen-14B-Chat/lora/2023-11-06-11-11-294/runs/Nov06_13-47-44_47f2ea7d05bd,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=5,
logging_strategy=steps,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=saves/Qwen-14B-Chat/lora/2023-11-06-11-11-294,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=3,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=saves/Qwen-14B-Chat/lora/2023-11-06-11-11-294,
save_on_each_node=False,
save_safetensors=True,
save_steps=100,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
11/06/2023 13:47:44 - INFO - llmtuner.dsets.loader - Loading dataset json_result_nocontext.json...
11/06/2023 13:47:44 - WARNING - llmtuner.dsets.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1ef520900a94b5a4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
[INFO|tokenization_utils_base.py:2020] 2023-11-06 13:47:45,512 >> loading file qwen.tiktoken
[INFO|tokenization_utils_base.py:2020] 2023-11-06 13:47:45,512 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2020] 2023-11-06 13:47:45,512 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2020] 2023-11-06 13:47:45,512 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2020] 2023-11-06 13:47:45,512 >> loading file tokenizer.json
[INFO|configuration_utils.py:715] 2023-11-06 13:47:46,177 >> loading configuration file /opt/qwenmodel/config.json
[INFO|configuration_utils.py:715] 2023-11-06 13:47:46,178 >> loading configuration file /opt/qwenmodel/config.json
[INFO|configuration_utils.py:777] 2023-11-06 13:47:46,178 >> Model config QWenConfig {
  "_name_or_path": "/opt/qwenmodel",
  "architectures": [
    "QWenLMHeadModel"
  ],
  "attn_dropout_prob": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_qwen.QWenConfig",
    "AutoModelForCausalLM": "modeling_qwen.QWenLMHeadModel"
  },
  "bf16": false,
  "emb_dropout_prob": 0.0,
  "fp16": false,
  "fp32": false,
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 27392,
  "kv_channels": 128,
  "layer_norm_epsilon": 1e-06,
  "max_position_embeddings": 8192,
  "model_type": "qwen",
  "no_bias": true,
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "onnx_safe": null,
  "rotary_emb_base": 10000,
  "rotary_pct": 1.0,
  "scale_attn_weights": true,
  "seq_length": 2048,
  "softmax_in_fp32": false,
  "tie_word_embeddings": false,
  "tokenizer_class": "QWenTokenizer",
  "transformers_version": "4.35.0",
  "use_cache": true,
  "use_cache_kernel": false,
  "use_cache_quantization": false,
  "use_dynamic_ntk": true,
  "use_flash_attn": "auto",
  "use_logn_attn": true,
  "vocab_size": 152064
}

11/06/2023 13:47:46 - INFO - llmtuner.tuner.core.loader - Quantizing model to 4 bit.
[INFO|modeling_utils.py:3118] 2023-11-06 13:47:46,210 >> loading weights file /opt/qwenmodel/model.safetensors.index.json
[INFO|modeling_utils.py:1222] 2023-11-06 13:47:46,211 >> Instantiating QWenLMHeadModel model under default dtype torch.float16.
[INFO|configuration_utils.py:791] 2023-11-06 13:47:46,211 >> Generate config GenerationConfig {}

Warning: please make sure that you are using the latest codes and checkpoints, especially if you used Qwen-7B before 09.25.2023.请使用最新模型和代码，尤其如果你在9月25日前已经开始使用Qwen-7B，千万注意不要使用错误代码和模型。
Try importing flash-attention for faster inference...
Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention
11/06/2023 13:47:46 - WARNING - llmtuner.tuner.core.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
/root/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/training_args.py:1697: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
11/06/2023 13:47:46 - INFO - llmtuner.tuner.core.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
11/06/2023 13:47:46 - INFO - llmtuner.tuner.core.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=saves/Qwen-14B-Chat/lora/2023-11-06-11-11-294/runs/Nov06_13-47-44_47f2ea7d05bd,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=5,
logging_strategy=steps,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=saves/Qwen-14B-Chat/lora/2023-11-06-11-11-294,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=3,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=saves/Qwen-14B-Chat/lora/2023-11-06-11-11-294,
save_on_each_node=False,
save_safetensors=True,
save_steps=100,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
11/06/2023 13:47:46 - INFO - llmtuner.dsets.loader - Loading dataset json_result_nocontext.json...
11/06/2023 13:47:46 - WARNING - llmtuner.dsets.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
[INFO|modeling_utils.py:3257] 2023-11-06 13:47:46,875 >> Detected 4-bit loading: activating 4-bit loading for this model
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1ef520900a94b5a4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
11/06/2023 13:47:48 - INFO - llmtuner.tuner.core.loader - Quantizing model to 4 bit.
Warning: please make sure that you are using the latest codes and checkpoints, especially if you used Qwen-7B before 09.25.2023.请使用最新模型和代码，尤其如果你在9月25日前已经开始使用Qwen-7B，千万注意不要使用错误代码和模型。
Try importing flash-attention for faster inference...
Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:02<00:39,  2.83s/it]Loading checkpoint shards:   7%|▋         | 1/15 [00:05<01:23,  5.98s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:05<00:32,  2.52s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:07<00:46,  3.58s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:07<00:27,  2.27s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:09<00:32,  2.71s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:09<00:23,  2.14s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:11<00:25,  2.32s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:10<00:20,  2.03s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:13<00:21,  2.15s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:12<00:17,  1.90s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:14<00:18,  2.04s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:14<00:14,  1.86s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:16<00:15,  1.93s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:15<00:12,  1.79s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:18<00:13,  1.87s/it]Loading checkpoint shards:  60%|██████    | 9/15 [00:17<00:10,  1.71s/it]Loading checkpoint shards:  60%|██████    | 9/15 [00:20<00:10,  1.80s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [00:19<00:08,  1.65s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [00:21<00:08,  1.76s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [00:20<00:06,  1.59s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [00:23<00:06,  1.73s/it]Loading checkpoint shards:  80%|████████  | 12/15 [00:21<00:04,  1.56s/it]Loading checkpoint shards:  80%|████████  | 12/15 [00:25<00:05,  1.70s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [00:23<00:03,  1.56s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [00:26<00:03,  1.60s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [00:25<00:01,  1.59s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [00:27<00:01,  1.57s/it]Loading checkpoint shards: 100%|██████████| 15/15 [00:26<00:00,  1.50s/it]Loading checkpoint shards: 100%|██████████| 15/15 [00:26<00:00,  1.76s/it]
11/06/2023 13:48:15 - INFO - llmtuner.tuner.core.utils - Upcasting weights in layernorm in float32.
11/06/2023 13:48:15 - INFO - llmtuner.tuner.core.utils - Gradient checkpointing enabled.
11/06/2023 13:48:15 - INFO - llmtuner.tuner.core.adapter - Fine-tuning method: LoRA
11/06/2023 13:48:15 - INFO - llmtuner.tuner.core.loader - trainable params: 6553600 || all params: 14173844480 || trainable%: 0.0462
11/06/2023 13:48:15 - INFO - llmtuner.extras.template - Add eos token: <|endoftext|>
11/06/2023 13:48:15 - INFO - llmtuner.extras.template - Add pad token: <|endoftext|>
Loading checkpoint shards: 100%|██████████| 15/15 [00:28<00:00,  1.43s/it]Loading checkpoint shards: 100%|██████████| 15/15 [00:28<00:00,  1.93s/it]
[INFO|modeling_utils.py:3950] 2023-11-06 13:48:16,062 >> All model checkpoint weights were used when initializing QWenLMHeadModel.

[INFO|modeling_utils.py:3958] 2023-11-06 13:48:16,062 >> All the weights of QWenLMHeadModel were initialized from the model checkpoint at /opt/qwenmodel.
If your task is similar to the task the model of the checkpoint was trained on, you can already use QWenLMHeadModel for predictions without further training.
[INFO|configuration_utils.py:749] 2023-11-06 13:48:16,065 >> loading configuration file /opt/qwenmodel/generation_config.json
[INFO|configuration_utils.py:791] 2023-11-06 13:48:16,065 >> Generate config GenerationConfig {
  "chat_format": "chatml",
  "do_sample": true,
  "eos_token_id": 151643,
  "max_new_tokens": 512,
  "max_window_size": 6144,
  "pad_token_id": 151643,
  "repetition_penalty": 1.1,
  "top_k": 0,
  "top_p": 0.8
}

11/06/2023 13:48:16 - INFO - llmtuner.tuner.core.utils - Upcasting weights in layernorm in float32.
11/06/2023 13:48:16 - INFO - llmtuner.tuner.core.utils - Gradient checkpointing enabled.
11/06/2023 13:48:16 - INFO - llmtuner.tuner.core.adapter - Fine-tuning method: LoRA
11/06/2023 13:48:16 - INFO - llmtuner.tuner.core.loader - trainable params: 6553600 || all params: 14173844480 || trainable%: 0.0462
11/06/2023 13:48:16 - INFO - llmtuner.extras.template - Add eos token: <|endoftext|>
11/06/2023 13:48:16 - INFO - llmtuner.extras.template - Add pad token: <|endoftext|>
Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1ef520900a94b5a4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-fca31e58034c4d9b.arrow
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 106559, 105588, 101121, 33108, 100176, 100142, 151645, 198, 151644, 77091, 198, 100398, 88970, 26940, 99655, 107216, 105588, 101121, 33108, 100176, 100142, 99812, 25067, 1773, 99655, 107216, 105588, 101121, 33108, 100176, 100142, 99812, 23671, 33108, 107576, 100355, 26940, 105492, 104773, 105588, 101693, 24339, 25067, 100241, 1773, 151643]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
强制招标范围和规模标准<|im_end|>
<|im_start|>assistant
具体见《工程建设项目招标范围和规模标准规定》。工程建设项目招标范围和规模标准规定.doc和安徽省实施《中华人民共和国招标投标法》办法。<|endoftext|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 100398, 88970, 26940, 99655, 107216, 105588, 101121, 33108, 100176, 100142, 99812, 25067, 1773, 99655, 107216, 105588, 101121, 33108, 100176, 100142, 99812, 23671, 33108, 107576, 100355, 26940, 105492, 104773, 105588, 101693, 24339, 25067, 100241, 1773, 151643]
labels:
具体见《工程建设项目招标范围和规模标准规定》。工程建设项目招标范围和规模标准规定.doc和安徽省实施《中华人民共和国招标投标法》办法。<|endoftext|>
[INFO|training_args.py:1784] 2023-11-06 13:48:18,860 >> PyTorch: setting up devices
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:593] 2023-11-06 13:48:18,870 >> Using auto half precision backend
[2023-11-06 13:48:19,138] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.2, git-hash=unknown, git-branch=unknown
Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1ef520900a94b5a4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-fca31e58034c4d9b.arrow
[2023-11-06 13:48:20,584] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-11-06 13:48:20,587] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-11-06 13:48:20,587] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2023-11-06 13:48:20,591] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-11-06 13:48:20,592] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-11-06 13:48:20,592] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-11-06 13:48:20,592] [INFO] [stage_1_and_2.py:147:__init__] Reduce bucket size 500000000
[2023-11-06 13:48:20,592] [INFO] [stage_1_and_2.py:148:__init__] Allgather bucket size 500000000
[2023-11-06 13:48:20,592] [INFO] [stage_1_and_2.py:149:__init__] CPU Offload: False
[2023-11-06 13:48:20,592] [INFO] [stage_1_and_2.py:150:__init__] Round robin gradient partitioning: False
[2023-11-06 13:48:21,837] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
[2023-11-06 13:48:21,838] [INFO] [utils.py:803:see_memory_usage] MA 9.13 GB         Max_MA 9.14 GB         CA 9.16 GB         Max_CA 9 GB 
[2023-11-06 13:48:21,838] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 22.89 GB, percent = 3.6%
[2023-11-06 13:48:22,012] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
[2023-11-06 13:48:22,012] [INFO] [utils.py:803:see_memory_usage] MA 9.16 GB         Max_MA 9.18 GB         CA 9.21 GB         Max_CA 9 GB 
[2023-11-06 13:48:22,013] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 23.02 GB, percent = 3.7%
[2023-11-06 13:48:22,013] [INFO] [stage_1_and_2.py:514:__init__] optimizer state initialized
[2023-11-06 13:48:22,190] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
[2023-11-06 13:48:22,191] [INFO] [utils.py:803:see_memory_usage] MA 9.16 GB         Max_MA 9.16 GB         CA 9.21 GB         Max_CA 9 GB 
[2023-11-06 13:48:22,191] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 23.17 GB, percent = 3.7%
[2023-11-06 13:48:22,193] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-11-06 13:48:22,193] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-11-06 13:48:22,193] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-11-06 13:48:22,193] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]
[2023-11-06 13:48:22,194] [INFO] [config.py:972:print] DeepSpeedEngine configuration:
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   amp_enabled .................. False
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   amp_params ................... False
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   bfloat16_enabled ............. False
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   checkpoint_parallel_write_pipeline  False
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   checkpoint_tag_validation_enabled  True
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   checkpoint_tag_validation_fail  False
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb9e7bc2640>
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   communication_data_type ...... None
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   curriculum_enabled_legacy .... False
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   curriculum_params_legacy ..... False
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   data_efficiency_enabled ...... False
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   dataloader_drop_last ......... False
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   disable_allgather ............ False
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   dump_state ................... False
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   eigenvalue_enabled ........... False
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   eigenvalue_gas_boundary_resolution  1
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   eigenvalue_layer_num ......... 0
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   eigenvalue_max_iter .......... 100
[2023-11-06 13:48:22,195] [INFO] [config.py:976:print]   eigenvalue_stability ......... 1e-06
[2023-11-06 13:48:22,196] [INFO] [config.py:976:print]   eigenvalue_tol ............... 0.01
[2023-11-06 13:48:22,196] [INFO] [config.py:976:print]   eigenvalue_verbose ........... False
[2023-11-06 13:48:22,196] [INFO] [config.py:976:print]   elasticity_enabled ........... False
[2023-11-06 13:48:22,196] [INFO] [config.py:976:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-11-06 13:48:22,196] [INFO] [config.py:976:print]   fp16_auto_cast ............... False
[2023-11-06 13:48:22,196] [INFO] [config.py:976:print]   fp16_enabled ................. True
[2023-11-06 13:48:22,196] [INFO] [config.py:976:print]   fp16_master_weights_and_gradients  False
[2023-11-06 13:48:22,196] [INFO] [config.py:976:print]   global_rank .................. 0
[2023-11-06 13:48:22,196] [INFO] [config.py:976:print]   grad_accum_dtype ............. None
[2023-11-06 13:48:22,196] [INFO] [config.py:976:print]   gradient_accumulation_steps .. 4
[2023-11-06 13:48:22,196] [INFO] [config.py:976:print]   gradient_clipping ............ 1.0
[2023-11-06 13:48:22,196] [INFO] [config.py:976:print]   gradient_predivide_factor .... 1.0
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   initial_dynamic_scale ........ 65536
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   load_universal_checkpoint .... False
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   loss_scale ................... 0
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   memory_breakdown ............. False
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   mics_hierarchial_params_gather  False
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   mics_shard_size .............. -1
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   optimizer_legacy_fusion ...... False
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   optimizer_name ............... None
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   optimizer_params ............. None
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   pld_enabled .................. False
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   pld_params ................... False
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   prescale_gradients ........... False
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   scheduler_name ............... None
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   scheduler_params ............. None
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   seq_parallel_communication_data_type  torch.float32
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   sparse_attention ............. None
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   sparse_gradients_enabled ..... False
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   steps_per_print .............. inf
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   train_batch_size ............. 24
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   train_micro_batch_size_per_gpu  3
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   use_node_local_storage ....... False
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   wall_clock_breakdown ......... False
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   weight_quantization_config ... None
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   world_size ................... 2
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   zero_allow_untested_optimizer  True
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-11-06 13:48:22,199] [INFO] [config.py:976:print]   zero_enabled ................. True
[2023-11-06 13:48:22,200] [INFO] [config.py:976:print]   zero_force_ds_cpu_optimizer .. True
[2023-11-06 13:48:22,200] [INFO] [config.py:976:print]   zero_optimization_stage ...... 2
[2023-11-06 13:48:22,200] [INFO] [config.py:962:print_user_config]   json = {
    "train_batch_size": 24, 
    "train_micro_batch_size_per_gpu": 3, 
    "gradient_accumulation_steps": 4, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "overlap_comm": false, 
        "contiguous_gradients": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": false
    }
}
[INFO|trainer.py:1723] 2023-11-06 13:48:22,200 >> ***** Running training *****
[INFO|trainer.py:1724] 2023-11-06 13:48:22,200 >>   Num examples = 7,317
[INFO|trainer.py:1725] 2023-11-06 13:48:22,200 >>   Num Epochs = 3
[INFO|trainer.py:1726] 2023-11-06 13:48:22,200 >>   Instantaneous batch size per device = 3
[INFO|trainer.py:1729] 2023-11-06 13:48:22,200 >>   Total train batch size (w. parallel, distributed & accumulation) = 24
[INFO|trainer.py:1730] 2023-11-06 13:48:22,200 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1731] 2023-11-06 13:48:22,200 >>   Total optimization steps = 915
[INFO|trainer.py:1732] 2023-11-06 13:48:22,202 >>   Number of trainable parameters = 6,553,600
  0%|          | 0/915 [00:00<?, ?it/s]/root/miniconda3/envs/llama/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1881: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/root/miniconda3/envs/llama/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1881: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
  0%|          | 1/915 [00:03<56:06,  3.68s/it]  0%|          | 2/915 [00:05<38:49,  2.55s/it]  0%|          | 3/915 [00:07<34:11,  2.25s/it]  0%|          | 4/915 [00:08<28:23,  1.87s/it]  1%|          | 5/915 [00:10<27:17,  1.80s/it]                                               {'loss': 2.613, 'learning_rate': 4.999631619262568e-05, 'epoch': 0.02}
  1%|          | 5/915 [00:10<27:17,  1.80s/it]  1%|          | 6/915 [00:11<25:20,  1.67s/it]  1%|          | 7/915 [00:13<24:33,  1.62s/it]  1%|          | 8/915 [00:14<24:11,  1.60s/it]  1%|          | 9/915 [00:16<24:30,  1.62s/it]  1%|          | 10/915 [00:18<25:30,  1.69s/it]                                                {'loss': 2.4537, 'learning_rate': 4.998526585613763e-05, 'epoch': 0.03}
  1%|          | 10/915 [00:18<25:30,  1.69s/it]  1%|          | 11/915 [00:19<24:41,  1.64s/it]  1%|▏         | 12/915 [00:21<25:04,  1.67s/it]  1%|▏         | 13/915 [00:23<27:03,  1.80s/it]  2%|▏         | 14/915 [00:25<25:54,  1.73s/it]  2%|▏         | 15/915 [00:26<25:03,  1.67s/it]                                                {'loss': 2.4204, 'learning_rate': 4.9966852247120764e-05, 'epoch': 0.05}
  2%|▏         | 15/915 [00:26<25:03,  1.67s/it]  2%|▏         | 16/915 [00:29<27:39,  1.85s/it]  2%|▏         | 17/915 [00:30<28:10,  1.88s/it]Traceback (most recent call last):
  File "/opt/LLaMA-Factory/src/train_bash.py", line 14, in <module>
    main()
  File "/opt/LLaMA-Factory/src/train_bash.py", line 5, in main
    run_exp()
  File "/opt/LLaMA-Factory/src/llmtuner/tuner/tune.py", line 26, in run_exp
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/opt/LLaMA-Factory/src/llmtuner/tuner/sft/workflow.py", line 67, in run_sft
    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/trainer.py", line 1555, in train
    return inner_training_loop(
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/trainer.py", line 1860, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/trainer.py", line 2725, in training_step
    loss = self.compute_loss(model, inputs)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/trainer.py", line 2748, in compute_loss
    outputs = model(**inputs)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1814, in forward
    loss = self.module(*inputs, **kwargs)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/peft/peft_model.py", line 977, in forward
    return self.base_model(
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/peft/tuners/tuners_utils.py", line 106, in forward
    return self.model.forward(*args, **kwargs)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/accelerate/hooks.py", line 164, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/qwenmodel/modeling_qwen.py", line 1121, in forward
    transformer_outputs = self.transformer(
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/accelerate/hooks.py", line 164, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/qwenmodel/modeling_qwen.py", line 951, in forward
    outputs = block(
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/accelerate/hooks.py", line 164, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/qwenmodel/modeling_qwen.py", line 674, in forward
    mlp_output = self.mlp(layernorm_output)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/accelerate/hooks.py", line 164, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/qwenmodel/modeling_qwen.py", line 618, in forward
    output = self.c_proj(intermediate_parallel)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/accelerate/hooks.py", line 164, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/bitsandbytes/nn/modules.py", line 248, in forward
    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py", line 579, in matmul_4bit
    return MatMul4Bit.apply(A, B, out, bias, quant_state)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py", line 516, in forward
    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, state).to(A.dtype).t(), bias)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/bitsandbytes/functional.py", line 908, in dequantize_4bit
    out = torch.empty(shape, dtype=dtype, device=A.device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacty of 39.42 GiB of which 97.06 MiB is free. Process 68109 has 39.33 GiB memory in use. Of the allocated memory 36.93 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  2%|▏         | 17/915 [00:32<28:11,  1.88s/it]
[2023-11-06 13:48:56,885] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 32286
[2023-11-06 13:48:56,886] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 32291
[2023-11-06 13:48:57,542] [ERROR] [launch.py:321:sigkill_handler] ['/root/miniconda3/envs/llama/bin/python', '-u', 'src/train_bash.py', '--local_rank=1', '--deepspeed', 'ds_config.json', '--stage', 'sft', '--model_name_or_path', '/opt/qwenmodel', '--do_train', 'True', '--finetuning_type', 'lora', '--quantization_bit', '4', '--template', 'chatml', '--flash_attn', 'False', '--shift_attn', 'False', '--dataset_dir', 'data', '--dataset', 'alpaca_en', '--cutoff_len', '1024', '--learning_rate', '5e-05', '--num_train_epochs', '3.0', '--max_samples', '100000', '--per_device_train_batch_size', '3', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'cosine', '--max_grad_norm', '1.0', '--logging_steps', '5', '--save_steps', '100', '--warmup_steps', '0', '--neft_alpha', '0', '--train_on_prompt', 'False', '--upcast_layernorm', 'True', '--lora_rank', '8', '--lora_dropout', '0.1', '--lora_target', 'c_attn', '--resume_lora_training', 'True', '--output_dir', 'saves/Qwen-14B-Chat/lora/2023-11-06-11-11-294', '--fp16', 'True', '--plot_loss', 'True'] exits with return code = 1
[2023-11-06 13:49:42,252] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-06 13:49:44,543] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-11-06 13:49:44,543] [INFO] [runner.py:570:main] cmd = /root/miniconda3/envs/llama/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=9901 --enable_each_rank_log=None src/train_bash.py --deepspeed ds_config.json --stage sft --model_name_or_path /opt/qwenmodel --do_train True --finetuning_type lora --quantization_bit 4 --template chatml --flash_attn False --shift_attn False --dataset_dir data --dataset alpaca_en --cutoff_len 1024 --learning_rate 5e-05 --num_train_epochs 3.0 --max_samples 100000 --per_device_train_batch_size 2 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --max_grad_norm 1.0 --logging_steps 5 --save_steps 100 --warmup_steps 0 --neft_alpha 0 --train_on_prompt False --upcast_layernorm True --lora_rank 8 --lora_dropout 0.1 --lora_target c_attn --resume_lora_training True --output_dir saves/Qwen-14B-Chat/lora/2023-11-06-11-11-294 --fp16 True --plot_loss True
[2023-11-06 13:49:46,947] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-06 13:49:48,835] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2023-11-06 13:49:48,835] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2023-11-06 13:49:48,835] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2023-11-06 13:49:48,835] [INFO] [launch.py:163:main] dist_world_size=2
[2023-11-06 13:49:48,835] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2023-11-06 13:49:54,387] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-06 13:49:54,560] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
FlashAttention-2 is not installed, ignore this if you are not using FlashAttention.
FlashAttention-2 is not installed, ignore this if you are not using FlashAttention.
[2023-11-06 13:49:55,764] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-11-06 13:49:55,764] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-11-06 13:49:55,875] [INFO] [comm.py:637:init_distributed] cdb=None
11/06/2023 13:49:55 - WARNING - llmtuner.tuner.core.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|training_args.py:1784] 2023-11-06 13:49:55,882 >> PyTorch: setting up devices
/root/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/training_args.py:1697: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
Traceback (most recent call last):
  File "/opt/LLaMA-Factory/src/train_bash.py", line 14, in <module>
    main()
  File "/opt/LLaMA-Factory/src/train_bash.py", line 5, in main
    run_exp()
  File "/opt/LLaMA-Factory/src/llmtuner/tuner/tune.py", line 20, in run_exp
    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)
  File "/opt/LLaMA-Factory/src/llmtuner/tuner/core/parser.py", line 174, in get_train_args
    raise ValueError("Output directory already exists and is not empty. Please set `overwrite_output_dir`.")
ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.
11/06/2023 13:49:57 - WARNING - llmtuner.tuner.core.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
/root/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/training_args.py:1697: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
Traceback (most recent call last):
  File "/opt/LLaMA-Factory/src/train_bash.py", line 14, in <module>
    main()
  File "/opt/LLaMA-Factory/src/train_bash.py", line 5, in main
    run_exp()
  File "/opt/LLaMA-Factory/src/llmtuner/tuner/tune.py", line 20, in run_exp
    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)
  File "/opt/LLaMA-Factory/src/llmtuner/tuner/core/parser.py", line 174, in get_train_args
    raise ValueError("Output directory already exists and is not empty. Please set `overwrite_output_dir`.")
ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.
[2023-11-06 13:49:57,926] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 35690
[2023-11-06 13:49:57,927] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 35691
[2023-11-06 13:49:58,262] [ERROR] [launch.py:321:sigkill_handler] ['/root/miniconda3/envs/llama/bin/python', '-u', 'src/train_bash.py', '--local_rank=1', '--deepspeed', 'ds_config.json', '--stage', 'sft', '--model_name_or_path', '/opt/qwenmodel', '--do_train', 'True', '--finetuning_type', 'lora', '--quantization_bit', '4', '--template', 'chatml', '--flash_attn', 'False', '--shift_attn', 'False', '--dataset_dir', 'data', '--dataset', 'alpaca_en', '--cutoff_len', '1024', '--learning_rate', '5e-05', '--num_train_epochs', '3.0', '--max_samples', '100000', '--per_device_train_batch_size', '2', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'cosine', '--max_grad_norm', '1.0', '--logging_steps', '5', '--save_steps', '100', '--warmup_steps', '0', '--neft_alpha', '0', '--train_on_prompt', 'False', '--upcast_layernorm', 'True', '--lora_rank', '8', '--lora_dropout', '0.1', '--lora_target', 'c_attn', '--resume_lora_training', 'True', '--output_dir', 'saves/Qwen-14B-Chat/lora/2023-11-06-11-11-294', '--fp16', 'True', '--plot_loss', 'True'] exits with return code = 1
[2023-11-06 13:51:33,894] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-06 13:51:36,134] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-11-06 13:51:36,134] [INFO] [runner.py:570:main] cmd = /root/miniconda3/envs/llama/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=9901 --enable_each_rank_log=None src/train_bash.py --deepspeed ds_config.json --stage sft --model_name_or_path /opt/qwenmodel --do_train True --finetuning_type lora --quantization_bit 4 --template chatml --flash_attn False --shift_attn False --dataset_dir data --dataset alpaca_en --cutoff_len 1024 --learning_rate 5e-05 --num_train_epochs 3.0 --max_samples 100000 --per_device_train_batch_size 2 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --max_grad_norm 1.0 --logging_steps 5 --save_steps 100 --warmup_steps 0 --neft_alpha 0 --train_on_prompt False --upcast_layernorm True --lora_rank 8 --lora_dropout 0.1 --lora_target c_attn --resume_lora_training True --output_dir saves/Qwen-14B-Chat/lora/2023-11-06-11-11-295 --fp16 True --plot_loss True
[2023-11-06 13:51:38,557] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-06 13:51:40,493] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2023-11-06 13:51:40,493] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2023-11-06 13:51:40,493] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2023-11-06 13:51:40,493] [INFO] [launch.py:163:main] dist_world_size=2
[2023-11-06 13:51:40,493] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2023-11-06 13:51:46,223] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-06 13:51:46,261] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
FlashAttention-2 is not installed, ignore this if you are not using FlashAttention.
FlashAttention-2 is not installed, ignore this if you are not using FlashAttention.
[2023-11-06 13:51:47,604] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-11-06 13:51:47,618] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-11-06 13:51:47,618] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
11/06/2023 13:51:48 - WARNING - llmtuner.tuner.core.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|training_args.py:1784] 2023-11-06 13:51:48,612 >> PyTorch: setting up devices
/root/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/training_args.py:1697: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
11/06/2023 13:51:48 - INFO - llmtuner.tuner.core.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
11/06/2023 13:51:48 - INFO - llmtuner.tuner.core.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=saves/Qwen-14B-Chat/lora/2023-11-06-11-11-295/runs/Nov06_13-51-47_47f2ea7d05bd,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=5,
logging_strategy=steps,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=saves/Qwen-14B-Chat/lora/2023-11-06-11-11-295,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=2,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=saves/Qwen-14B-Chat/lora/2023-11-06-11-11-295,
save_on_each_node=False,
save_safetensors=True,
save_steps=100,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
11/06/2023 13:51:48 - INFO - llmtuner.dsets.loader - Loading dataset json_result_nocontext.json...
11/06/2023 13:51:48 - WARNING - llmtuner.dsets.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1ef520900a94b5a4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
[INFO|tokenization_utils_base.py:2020] 2023-11-06 13:51:49,644 >> loading file qwen.tiktoken
[INFO|tokenization_utils_base.py:2020] 2023-11-06 13:51:49,645 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2020] 2023-11-06 13:51:49,645 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2020] 2023-11-06 13:51:49,645 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2020] 2023-11-06 13:51:49,645 >> loading file tokenizer.json
11/06/2023 13:51:50 - WARNING - llmtuner.tuner.core.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
/root/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/training_args.py:1697: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
11/06/2023 13:51:50 - INFO - llmtuner.tuner.core.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
11/06/2023 13:51:50 - INFO - llmtuner.tuner.core.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=saves/Qwen-14B-Chat/lora/2023-11-06-11-11-295/runs/Nov06_13-51-47_47f2ea7d05bd,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=5,
logging_strategy=steps,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=saves/Qwen-14B-Chat/lora/2023-11-06-11-11-295,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=2,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=saves/Qwen-14B-Chat/lora/2023-11-06-11-11-295,
save_on_each_node=False,
save_safetensors=True,
save_steps=100,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
11/06/2023 13:51:50 - INFO - llmtuner.dsets.loader - Loading dataset json_result_nocontext.json...
11/06/2023 13:51:50 - WARNING - llmtuner.dsets.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
[INFO|configuration_utils.py:715] 2023-11-06 13:51:50,343 >> loading configuration file /opt/qwenmodel/config.json
[INFO|configuration_utils.py:715] 2023-11-06 13:51:50,344 >> loading configuration file /opt/qwenmodel/config.json
[INFO|configuration_utils.py:777] 2023-11-06 13:51:50,345 >> Model config QWenConfig {
  "_name_or_path": "/opt/qwenmodel",
  "architectures": [
    "QWenLMHeadModel"
  ],
  "attn_dropout_prob": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_qwen.QWenConfig",
    "AutoModelForCausalLM": "modeling_qwen.QWenLMHeadModel"
  },
  "bf16": false,
  "emb_dropout_prob": 0.0,
  "fp16": false,
  "fp32": false,
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 27392,
  "kv_channels": 128,
  "layer_norm_epsilon": 1e-06,
  "max_position_embeddings": 8192,
  "model_type": "qwen",
  "no_bias": true,
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "onnx_safe": null,
  "rotary_emb_base": 10000,
  "rotary_pct": 1.0,
  "scale_attn_weights": true,
  "seq_length": 2048,
  "softmax_in_fp32": false,
  "tie_word_embeddings": false,
  "tokenizer_class": "QWenTokenizer",
  "transformers_version": "4.35.0",
  "use_cache": true,
  "use_cache_kernel": false,
  "use_cache_quantization": false,
  "use_dynamic_ntk": true,
  "use_flash_attn": "auto",
  "use_logn_attn": true,
  "vocab_size": 152064
}

11/06/2023 13:51:50 - INFO - llmtuner.tuner.core.loader - Quantizing model to 4 bit.
[INFO|modeling_utils.py:3118] 2023-11-06 13:51:50,376 >> loading weights file /opt/qwenmodel/model.safetensors.index.json
[INFO|modeling_utils.py:1222] 2023-11-06 13:51:50,377 >> Instantiating QWenLMHeadModel model under default dtype torch.float16.
[INFO|configuration_utils.py:791] 2023-11-06 13:51:50,377 >> Generate config GenerationConfig {}

Warning: please make sure that you are using the latest codes and checkpoints, especially if you used Qwen-7B before 09.25.2023.请使用最新模型和代码，尤其如果你在9月25日前已经开始使用Qwen-7B，千万注意不要使用错误代码和模型。
Try importing flash-attention for faster inference...
Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention
[INFO|modeling_utils.py:3257] 2023-11-06 13:51:50,919 >> Detected 4-bit loading: activating 4-bit loading for this model
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1ef520900a94b5a4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
11/06/2023 13:51:51 - INFO - llmtuner.tuner.core.loader - Quantizing model to 4 bit.
Warning: please make sure that you are using the latest codes and checkpoints, especially if you used Qwen-7B before 09.25.2023.请使用最新模型和代码，尤其如果你在9月25日前已经开始使用Qwen-7B，千万注意不要使用错误代码和模型。
Try importing flash-attention for faster inference...
Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:02<00:32,  2.34s/it]Loading checkpoint shards:   7%|▋         | 1/15 [00:04<00:58,  4.17s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:04<00:26,  2.06s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:05<00:35,  2.73s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:07<00:26,  2.22s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:06<00:23,  1.94s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:09<00:22,  2.02s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:07<00:20,  1.83s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:09<00:18,  1.82s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:11<00:19,  1.96s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:11<00:16,  1.85s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:13<00:17,  1.96s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:14<00:14,  1.87s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:13<00:15,  1.90s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:16<00:12,  1.84s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:15<00:13,  1.88s/it]Loading checkpoint shards:  60%|██████    | 9/15 [00:18<00:10,  1.78s/it]Loading checkpoint shards:  60%|██████    | 9/15 [00:16<00:10,  1.75s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [00:19<00:08,  1.73s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [00:18<00:08,  1.72s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [00:21<00:06,  1.70s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [00:19<00:06,  1.68s/it]Loading checkpoint shards:  80%|████████  | 12/15 [00:22<00:04,  1.66s/it]Loading checkpoint shards:  80%|████████  | 12/15 [00:21<00:04,  1.65s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [00:24<00:03,  1.69s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [00:23<00:03,  1.65s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [00:24<00:01,  1.63s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [00:26<00:01,  1.67s/it]Loading checkpoint shards: 100%|██████████| 15/15 [00:28<00:00,  1.75s/it]Loading checkpoint shards: 100%|██████████| 15/15 [00:28<00:00,  1.89s/it]
[INFO|modeling_utils.py:3950] 2023-11-06 13:52:19,435 >> All model checkpoint weights were used when initializing QWenLMHeadModel.

[INFO|modeling_utils.py:3958] 2023-11-06 13:52:19,435 >> All the weights of QWenLMHeadModel were initialized from the model checkpoint at /opt/qwenmodel.
If your task is similar to the task the model of the checkpoint was trained on, you can already use QWenLMHeadModel for predictions without further training.
[INFO|configuration_utils.py:749] 2023-11-06 13:52:19,438 >> loading configuration file /opt/qwenmodel/generation_config.json
[INFO|configuration_utils.py:791] 2023-11-06 13:52:19,438 >> Generate config GenerationConfig {
  "chat_format": "chatml",
  "do_sample": true,
  "eos_token_id": 151643,
  "max_new_tokens": 512,
  "max_window_size": 6144,
  "pad_token_id": 151643,
  "repetition_penalty": 1.1,
  "top_k": 0,
  "top_p": 0.8
}

11/06/2023 13:52:19 - INFO - llmtuner.tuner.core.utils - Upcasting weights in layernorm in float32.
11/06/2023 13:52:19 - INFO - llmtuner.tuner.core.utils - Gradient checkpointing enabled.
11/06/2023 13:52:19 - INFO - llmtuner.tuner.core.adapter - Fine-tuning method: LoRA
11/06/2023 13:52:19 - INFO - llmtuner.tuner.core.loader - trainable params: 6553600 || all params: 14173844480 || trainable%: 0.0462
11/06/2023 13:52:19 - INFO - llmtuner.extras.template - Add eos token: <|endoftext|>
11/06/2023 13:52:19 - INFO - llmtuner.extras.template - Add pad token: <|endoftext|>
Loading checkpoint shards: 100%|██████████| 15/15 [00:27<00:00,  1.84s/it]Loading checkpoint shards: 100%|██████████| 15/15 [00:27<00:00,  1.80s/it]
11/06/2023 13:52:19 - INFO - llmtuner.tuner.core.utils - Upcasting weights in layernorm in float32.
11/06/2023 13:52:19 - INFO - llmtuner.tuner.core.utils - Gradient checkpointing enabled.
11/06/2023 13:52:19 - INFO - llmtuner.tuner.core.adapter - Fine-tuning method: LoRA
11/06/2023 13:52:20 - INFO - llmtuner.tuner.core.loader - trainable params: 6553600 || all params: 14173844480 || trainable%: 0.0462
11/06/2023 13:52:20 - INFO - llmtuner.extras.template - Add eos token: <|endoftext|>
11/06/2023 13:52:20 - INFO - llmtuner.extras.template - Add pad token: <|endoftext|>
Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1ef520900a94b5a4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-fca31e58034c4d9b.arrow
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 106559, 105588, 101121, 33108, 100176, 100142, 151645, 198, 151644, 77091, 198, 100398, 88970, 26940, 99655, 107216, 105588, 101121, 33108, 100176, 100142, 99812, 25067, 1773, 99655, 107216, 105588, 101121, 33108, 100176, 100142, 99812, 23671, 33108, 107576, 100355, 26940, 105492, 104773, 105588, 101693, 24339, 25067, 100241, 1773, 151643]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
强制招标范围和规模标准<|im_end|>
<|im_start|>assistant
具体见《工程建设项目招标范围和规模标准规定》。工程建设项目招标范围和规模标准规定.doc和安徽省实施《中华人民共和国招标投标法》办法。<|endoftext|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 100398, 88970, 26940, 99655, 107216, 105588, 101121, 33108, 100176, 100142, 99812, 25067, 1773, 99655, 107216, 105588, 101121, 33108, 100176, 100142, 99812, 23671, 33108, 107576, 100355, 26940, 105492, 104773, 105588, 101693, 24339, 25067, 100241, 1773, 151643]
labels:
具体见《工程建设项目招标范围和规模标准规定》。工程建设项目招标范围和规模标准规定.doc和安徽省实施《中华人民共和国招标投标法》办法。<|endoftext|>
[INFO|training_args.py:1784] 2023-11-06 13:52:22,432 >> PyTorch: setting up devices
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:593] 2023-11-06 13:52:22,446 >> Using auto half precision backend
[2023-11-06 13:52:22,693] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.2, git-hash=unknown, git-branch=unknown
Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1ef520900a94b5a4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-fca31e58034c4d9b.arrow
[2023-11-06 13:52:24,327] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-11-06 13:52:24,329] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-11-06 13:52:24,329] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2023-11-06 13:52:24,333] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-11-06 13:52:24,334] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-11-06 13:52:24,334] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-11-06 13:52:24,334] [INFO] [stage_1_and_2.py:147:__init__] Reduce bucket size 500000000
[2023-11-06 13:52:24,334] [INFO] [stage_1_and_2.py:148:__init__] Allgather bucket size 500000000
[2023-11-06 13:52:24,334] [INFO] [stage_1_and_2.py:149:__init__] CPU Offload: False
[2023-11-06 13:52:24,334] [INFO] [stage_1_and_2.py:150:__init__] Round robin gradient partitioning: False
[2023-11-06 13:52:25,565] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
[2023-11-06 13:52:25,566] [INFO] [utils.py:803:see_memory_usage] MA 9.13 GB         Max_MA 9.14 GB         CA 9.16 GB         Max_CA 9 GB 
[2023-11-06 13:52:25,566] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 22.33 GB, percent = 3.5%
[2023-11-06 13:52:25,725] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
[2023-11-06 13:52:25,726] [INFO] [utils.py:803:see_memory_usage] MA 9.16 GB         Max_MA 9.18 GB         CA 9.21 GB         Max_CA 9 GB 
[2023-11-06 13:52:25,726] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 22.46 GB, percent = 3.6%
[2023-11-06 13:52:25,726] [INFO] [stage_1_and_2.py:514:__init__] optimizer state initialized
[2023-11-06 13:52:25,880] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
[2023-11-06 13:52:25,881] [INFO] [utils.py:803:see_memory_usage] MA 9.16 GB         Max_MA 9.16 GB         CA 9.21 GB         Max_CA 9 GB 
[2023-11-06 13:52:25,881] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 22.57 GB, percent = 3.6%
[2023-11-06 13:52:25,883] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-11-06 13:52:25,883] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-11-06 13:52:25,883] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-11-06 13:52:25,883] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]
[2023-11-06 13:52:25,885] [INFO] [config.py:972:print] DeepSpeedEngine configuration:
[2023-11-06 13:52:25,885] [INFO] [config.py:976:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-11-06 13:52:25,885] [INFO] [config.py:976:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-11-06 13:52:25,885] [INFO] [config.py:976:print]   amp_enabled .................. False
[2023-11-06 13:52:25,885] [INFO] [config.py:976:print]   amp_params ................... False
[2023-11-06 13:52:25,885] [INFO] [config.py:976:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-11-06 13:52:25,885] [INFO] [config.py:976:print]   bfloat16_enabled ............. False
[2023-11-06 13:52:25,885] [INFO] [config.py:976:print]   checkpoint_parallel_write_pipeline  False
[2023-11-06 13:52:25,885] [INFO] [config.py:976:print]   checkpoint_tag_validation_enabled  True
[2023-11-06 13:52:25,885] [INFO] [config.py:976:print]   checkpoint_tag_validation_fail  False
[2023-11-06 13:52:25,885] [INFO] [config.py:976:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fadcdde16d0>
[2023-11-06 13:52:25,885] [INFO] [config.py:976:print]   communication_data_type ...... None
[2023-11-06 13:52:25,885] [INFO] [config.py:976:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-11-06 13:52:25,885] [INFO] [config.py:976:print]   curriculum_enabled_legacy .... False
[2023-11-06 13:52:25,885] [INFO] [config.py:976:print]   curriculum_params_legacy ..... False
[2023-11-06 13:52:25,885] [INFO] [config.py:976:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-11-06 13:52:25,885] [INFO] [config.py:976:print]   data_efficiency_enabled ...... False
[2023-11-06 13:52:25,885] [INFO] [config.py:976:print]   dataloader_drop_last ......... False
[2023-11-06 13:52:25,885] [INFO] [config.py:976:print]   disable_allgather ............ False
[2023-11-06 13:52:25,885] [INFO] [config.py:976:print]   dump_state ................... False
[2023-11-06 13:52:25,885] [INFO] [config.py:976:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-11-06 13:52:25,885] [INFO] [config.py:976:print]   eigenvalue_enabled ........... False
[2023-11-06 13:52:25,885] [INFO] [config.py:976:print]   eigenvalue_gas_boundary_resolution  1
[2023-11-06 13:52:25,885] [INFO] [config.py:976:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   eigenvalue_layer_num ......... 0
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   eigenvalue_max_iter .......... 100
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   eigenvalue_stability ......... 1e-06
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   eigenvalue_tol ............... 0.01
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   eigenvalue_verbose ........... False
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   elasticity_enabled ........... False
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   fp16_auto_cast ............... False
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   fp16_enabled ................. True
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   fp16_master_weights_and_gradients  False
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   global_rank .................. 0
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   grad_accum_dtype ............. None
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   gradient_accumulation_steps .. 4
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   gradient_clipping ............ 1.0
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   gradient_predivide_factor .... 1.0
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   initial_dynamic_scale ........ 65536
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   load_universal_checkpoint .... False
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   loss_scale ................... 0
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   memory_breakdown ............. False
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   mics_hierarchial_params_gather  False
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   mics_shard_size .............. -1
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   optimizer_legacy_fusion ...... False
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   optimizer_name ............... None
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   optimizer_params ............. None
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   pld_enabled .................. False
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   pld_params ................... False
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   prescale_gradients ........... False
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   scheduler_name ............... None
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   scheduler_params ............. None
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   seq_parallel_communication_data_type  torch.float32
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   sparse_attention ............. None
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   sparse_gradients_enabled ..... False
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   steps_per_print .............. inf
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   train_batch_size ............. 16
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   train_micro_batch_size_per_gpu  2
[2023-11-06 13:52:25,886] [INFO] [config.py:976:print]   use_node_local_storage ....... False
[2023-11-06 13:52:25,887] [INFO] [config.py:976:print]   wall_clock_breakdown ......... False
[2023-11-06 13:52:25,887] [INFO] [config.py:976:print]   weight_quantization_config ... None
[2023-11-06 13:52:25,887] [INFO] [config.py:976:print]   world_size ................... 2
[2023-11-06 13:52:25,887] [INFO] [config.py:976:print]   zero_allow_untested_optimizer  True
[2023-11-06 13:52:25,887] [INFO] [config.py:976:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-11-06 13:52:25,887] [INFO] [config.py:976:print]   zero_enabled ................. True
[2023-11-06 13:52:25,887] [INFO] [config.py:976:print]   zero_force_ds_cpu_optimizer .. True
[2023-11-06 13:52:25,887] [INFO] [config.py:976:print]   zero_optimization_stage ...... 2
[2023-11-06 13:52:25,887] [INFO] [config.py:962:print_user_config]   json = {
    "train_batch_size": 16, 
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 4, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "overlap_comm": false, 
        "contiguous_gradients": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": false
    }
}
[INFO|trainer.py:1723] 2023-11-06 13:52:25,887 >> ***** Running training *****
[INFO|trainer.py:1724] 2023-11-06 13:52:25,887 >>   Num examples = 7,317
[INFO|trainer.py:1725] 2023-11-06 13:52:25,887 >>   Num Epochs = 3
[INFO|trainer.py:1726] 2023-11-06 13:52:25,887 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:1729] 2023-11-06 13:52:25,887 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1730] 2023-11-06 13:52:25,887 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1731] 2023-11-06 13:52:25,887 >>   Total optimization steps = 1,371
[INFO|trainer.py:1732] 2023-11-06 13:52:25,889 >>   Number of trainable parameters = 6,553,600
  0%|          | 0/1371 [00:00<?, ?it/s]/root/miniconda3/envs/llama/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1881: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/root/miniconda3/envs/llama/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1881: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
  0%|          | 1/1371 [00:03<1:09:09,  3.03s/it]  0%|          | 2/1371 [00:04<46:12,  2.02s/it]    0%|          | 3/1371 [00:05<37:48,  1.66s/it]  0%|          | 4/1371 [00:06<34:31,  1.52s/it]  0%|          | 5/1371 [00:08<33:27,  1.47s/it]                                                {'loss': 2.562, 'learning_rate': 4.9998359145370634e-05, 'epoch': 0.01}
  0%|          | 5/1371 [00:08<33:27,  1.47s/it]  0%|          | 6/1371 [00:09<30:14,  1.33s/it]  1%|          | 7/1371 [00:10<29:29,  1.30s/it]  1%|          | 8/1371 [00:11<29:20,  1.29s/it]  1%|          | 9/1371 [00:13<28:31,  1.26s/it]  1%|          | 10/1371 [00:14<27:29,  1.21s/it]                                                 {'loss': 2.6213, 'learning_rate': 4.999343679687485e-05, 'epoch': 0.02}
  1%|          | 10/1371 [00:14<27:29,  1.21s/it]  1%|          | 11/1371 [00:15<29:25,  1.30s/it]  1%|          | 12/1371 [00:16<27:43,  1.22s/it]  1%|          | 13/1371 [00:17<28:24,  1.26s/it]  1%|          | 14/1371 [00:19<27:05,  1.20s/it]  1%|          | 15/1371 [00:21<32:18,  1.43s/it]                                                 {'loss': 2.3874, 'learning_rate': 4.9985233600661306e-05, 'epoch': 0.03}
  1%|          | 15/1371 [00:21<32:18,  1.43s/it]  1%|          | 16/1371 [00:22<30:28,  1.35s/it]  1%|          | 17/1371 [00:23<30:26,  1.35s/it]  1%|▏         | 18/1371 [00:24<29:29,  1.31s/it]  1%|▏         | 19/1371 [00:26<30:30,  1.35s/it]  1%|▏         | 20/1371 [00:27<29:38,  1.32s/it]                                                 {'loss': 2.4113, 'learning_rate': 4.99737506335502e-05, 'epoch': 0.04}
  1%|▏         | 20/1371 [00:27<29:38,  1.32s/it]  2%|▏         | 21/1371 [00:28<29:28,  1.31s/it]  2%|▏         | 22/1371 [00:29<28:30,  1.27s/it]  2%|▏         | 23/1371 [00:31<29:11,  1.30s/it]  2%|▏         | 24/1371 [00:32<30:29,  1.36s/it]  2%|▏         | 25/1371 [00:34<31:11,  1.39s/it]                                                 {'loss': 2.1278, 'learning_rate': 4.995898940289192e-05, 'epoch': 0.05}
  2%|▏         | 25/1371 [00:34<31:11,  1.39s/it]  2%|▏         | 26/1371 [00:35<30:55,  1.38s/it]  2%|▏         | 27/1371 [00:37<34:36,  1.54s/it]  2%|▏         | 28/1371 [00:38<33:04,  1.48s/it]  2%|▏         | 29/1371 [00:40<31:15,  1.40s/it]  2%|▏         | 30/1371 [00:41<30:07,  1.35s/it]                                                 {'loss': 2.086, 'learning_rate': 4.9940951846369166e-05, 'epoch': 0.07}
  2%|▏         | 30/1371 [00:41<30:07,  1.35s/it]  2%|▏         | 31/1371 [00:42<29:01,  1.30s/it]  2%|▏         | 32/1371 [00:43<29:11,  1.31s/it]  2%|▏         | 33/1371 [00:44<28:11,  1.26s/it]  2%|▏         | 34/1371 [00:46<28:14,  1.27s/it]  3%|▎         | 35/1371 [00:47<29:04,  1.31s/it]                                                 {'loss': 1.9772, 'learning_rate': 4.9919640331742566e-05, 'epoch': 0.08}
  3%|▎         | 35/1371 [00:47<29:04,  1.31s/it]  3%|▎         | 36/1371 [00:48<29:24,  1.32s/it]  3%|▎         | 37/1371 [00:50<28:47,  1.29s/it]  3%|▎         | 38/1371 [00:51<27:54,  1.26s/it]  3%|▎         | 39/1371 [00:52<28:57,  1.30s/it]  3%|▎         | 40/1371 [00:54<29:38,  1.34s/it]                                                 {'loss': 1.9168, 'learning_rate': 4.989505765653993e-05, 'epoch': 0.09}
  3%|▎         | 40/1371 [00:54<29:38,  1.34s/it]  3%|▎         | 41/1371 [00:55<28:44,  1.30s/it]  3%|▎         | 42/1371 [00:56<27:56,  1.26s/it]  3%|▎         | 43/1371 [00:57<28:04,  1.27s/it]  3%|▎         | 44/1371 [00:59<27:48,  1.26s/it]  3%|▎         | 45/1371 [01:00<28:09,  1.27s/it]                                                 {'loss': 1.9013, 'learning_rate': 4.9867207047688966e-05, 'epoch': 0.1}
  3%|▎         | 45/1371 [01:00<28:09,  1.27s/it]  3%|▎         | 46/1371 [01:01<27:16,  1.23s/it]  3%|▎         | 47/1371 [01:02<26:37,  1.21s/it]  4%|▎         | 48/1371 [01:03<25:50,  1.17s/it]  4%|▎         | 49/1371 [01:04<25:49,  1.17s/it]  4%|▎         | 50/1371 [01:06<26:33,  1.21s/it]                                                 {'loss': 1.7891, 'learning_rate': 4.9836092161093713e-05, 'epoch': 0.11}
  4%|▎         | 50/1371 [01:06<26:33,  1.21s/it]  4%|▎         | 51/1371 [01:07<27:21,  1.24s/it]  4%|▍         | 52/1371 [01:08<26:49,  1.22s/it]  4%|▍         | 53/1371 [01:10<29:02,  1.32s/it]  4%|▍         | 54/1371 [01:11<30:32,  1.39s/it]  4%|▍         | 55/1371 [01:13<29:28,  1.34s/it]                                                 {'loss': 1.8781, 'learning_rate': 4.980171708115463e-05, 'epoch': 0.12}
  4%|▍         | 55/1371 [01:13<29:28,  1.34s/it]  4%|▍         | 56/1371 [01:14<28:44,  1.31s/it]  4%|▍         | 57/1371 [01:15<26:50,  1.23s/it]  4%|▍         | 58/1371 [01:16<27:05,  1.24s/it]  4%|▍         | 59/1371 [01:17<27:32,  1.26s/it]  4%|▍         | 60/1371 [01:19<26:40,  1.22s/it]                                                 {'loss': 1.9274, 'learning_rate': 4.976408632023243e-05, 'epoch': 0.13}
  4%|▍         | 60/1371 [01:19<26:40,  1.22s/it]  4%|▍         | 61/1371 [01:20<26:28,  1.21s/it]  5%|▍         | 62/1371 [01:21<27:00,  1.24s/it]  5%|▍         | 63/1371 [01:23<32:31,  1.49s/it]  5%|▍         | 64/1371 [01:25<32:24,  1.49s/it]  5%|▍         | 65/1371 [01:26<30:50,  1.42s/it]                                                 {'loss': 1.7246, 'learning_rate': 4.972320481805578e-05, 'epoch': 0.14}
  5%|▍         | 65/1371 [01:26<30:50,  1.42s/it]  5%|▍         | 66/1371 [01:28<32:32,  1.50s/it]  5%|▍         | 67/1371 [01:29<34:01,  1.57s/it]  5%|▍         | 68/1371 [01:31<33:13,  1.53s/it]  5%|▌         | 69/1371 [01:32<30:56,  1.43s/it]  5%|▌         | 70/1371 [01:34<32:28,  1.50s/it]                                                 {'loss': 1.7483, 'learning_rate': 4.967907794107286e-05, 'epoch': 0.15}
  5%|▌         | 70/1371 [01:34<32:28,  1.50s/it]  5%|▌         | 71/1371 [01:35<32:30,  1.50s/it]  5%|▌         | 72/1371 [01:36<30:39,  1.42s/it]  5%|▌         | 73/1371 [01:38<31:13,  1.44s/it]  5%|▌         | 74/1371 [01:39<29:46,  1.38s/it]  5%|▌         | 75/1371 [01:40<29:15,  1.35s/it]                                                 {'loss': 1.9065, 'learning_rate': 4.963171148174689e-05, 'epoch': 0.16}
  5%|▌         | 75/1371 [01:40<29:15,  1.35s/it]  6%|▌         | 76/1371 [01:42<29:22,  1.36s/it]  6%|▌         | 77/1371 [01:43<31:21,  1.45s/it]  6%|▌         | 78/1371 [01:44<28:58,  1.34s/it]  6%|▌         | 79/1371 [01:46<28:32,  1.33s/it]  6%|▌         | 80/1371 [01:47<28:17,  1.31s/it]                                                 {'loss': 1.8016, 'learning_rate': 4.958111165779579e-05, 'epoch': 0.17}
  6%|▌         | 80/1371 [01:47<28:17,  1.31s/it]  6%|▌         | 81/1371 [01:48<27:02,  1.26s/it]  6%|▌         | 82/1371 [01:50<28:17,  1.32s/it]  6%|▌         | 83/1371 [01:51<30:38,  1.43s/it]  6%|▌         | 84/1371 [01:52<28:52,  1.35s/it]  6%|▌         | 85/1371 [01:54<27:32,  1.29s/it]                                                 {'loss': 1.7773, 'learning_rate': 4.9527285111376e-05, 'epoch': 0.19}
  6%|▌         | 85/1371 [01:54<27:32,  1.29s/it]  6%|▋         | 86/1371 [01:55<27:24,  1.28s/it]  6%|▋         | 87/1371 [01:56<28:47,  1.35s/it]  6%|▋         | 88/1371 [01:58<27:51,  1.30s/it]Traceback (most recent call last):
  File "/opt/LLaMA-Factory/src/train_bash.py", line 14, in <module>
    main()
  File "/opt/LLaMA-Factory/src/train_bash.py", line 5, in main
    run_exp()
  File "/opt/LLaMA-Factory/src/llmtuner/tuner/tune.py", line 26, in run_exp
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/opt/LLaMA-Factory/src/llmtuner/tuner/sft/workflow.py", line 67, in run_sft
    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/trainer.py", line 1555, in train
    return inner_training_loop(
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/trainer.py", line 1860, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/trainer.py", line 2734, in training_step
    self.accelerator.backward(loss)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/accelerate/accelerator.py", line 1983, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/accelerate/utils/deepspeed.py", line 167, in backward
    self.engine.backward(loss, **kwargs)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1936, in backward
    self.optimizer.backward(loss, retain_graph=retain_graph)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1953, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1012.00 MiB. GPU 1 has a total capacty of 39.42 GiB of which 867.00 MiB is free. Process 76627 has 38.57 GiB memory in use. Of the allocated memory 35.51 GiB is allocated by PyTorch, and 1.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2023-11-06 13:54:27,852] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 38547
[2023-11-06 13:54:28,469] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 38548
[2023-11-06 13:54:28,470] [ERROR] [launch.py:321:sigkill_handler] ['/root/miniconda3/envs/llama/bin/python', '-u', 'src/train_bash.py', '--local_rank=1', '--deepspeed', 'ds_config.json', '--stage', 'sft', '--model_name_or_path', '/opt/qwenmodel', '--do_train', 'True', '--finetuning_type', 'lora', '--quantization_bit', '4', '--template', 'chatml', '--flash_attn', 'False', '--shift_attn', 'False', '--dataset_dir', 'data', '--dataset', 'alpaca_en', '--cutoff_len', '1024', '--learning_rate', '5e-05', '--num_train_epochs', '3.0', '--max_samples', '100000', '--per_device_train_batch_size', '2', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'cosine', '--max_grad_norm', '1.0', '--logging_steps', '5', '--save_steps', '100', '--warmup_steps', '0', '--neft_alpha', '0', '--train_on_prompt', 'False', '--upcast_layernorm', 'True', '--lora_rank', '8', '--lora_dropout', '0.1', '--lora_target', 'c_attn', '--resume_lora_training', 'True', '--output_dir', 'saves/Qwen-14B-Chat/lora/2023-11-06-11-11-295', '--fp16', 'True', '--plot_loss', 'True'] exits with return code = 1
[2023-11-06 13:56:37,685] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-06 13:56:40,069] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-11-06 13:56:40,069] [INFO] [runner.py:570:main] cmd = /root/miniconda3/envs/llama/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=9901 --enable_each_rank_log=None src/train_bash.py --deepspeed ds_config.json --stage sft --model_name_or_path /opt/qwenmodel --do_train True --finetuning_type lora --quantization_bit 4 --template chatml --flash_attn False --shift_attn False --dataset_dir data --dataset alpaca_en --cutoff_len 4048 --learning_rate 5e-05 --num_train_epochs 3.0 --max_samples 100000 --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --max_grad_norm 1.0 --logging_steps 5 --save_steps 100 --warmup_steps 0 --neft_alpha 0 --train_on_prompt False --upcast_layernorm True --lora_rank 8 --lora_dropout 0.1 --lora_target c_attn --resume_lora_training True --output_dir saves/Qwen-14B-Chat/lora/2023-11-06-11-11-295 --fp16 True --plot_loss True
[2023-11-06 13:56:42,429] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-06 13:56:44,608] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2023-11-06 13:56:44,608] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2023-11-06 13:56:44,608] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2023-11-06 13:56:44,608] [INFO] [launch.py:163:main] dist_world_size=2
[2023-11-06 13:56:44,608] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2023-11-06 13:56:50,702] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-06 13:56:50,738] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
FlashAttention-2 is not installed, ignore this if you are not using FlashAttention.
FlashAttention-2 is not installed, ignore this if you are not using FlashAttention.
[2023-11-06 13:56:52,022] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-11-06 13:56:52,049] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-11-06 13:56:52,049] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
11/06/2023 13:56:53 - WARNING - llmtuner.tuner.core.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|training_args.py:1784] 2023-11-06 13:56:53,034 >> PyTorch: setting up devices
/root/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/training_args.py:1697: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
Traceback (most recent call last):
  File "/opt/LLaMA-Factory/src/train_bash.py", line 14, in <module>
    main()
  File "/opt/LLaMA-Factory/src/train_bash.py", line 5, in main
    run_exp()
  File "/opt/LLaMA-Factory/src/llmtuner/tuner/tune.py", line 20, in run_exp
    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)
  File "/opt/LLaMA-Factory/src/llmtuner/tuner/core/parser.py", line 174, in get_train_args
    raise ValueError("Output directory already exists and is not empty. Please set `overwrite_output_dir`.")
ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.
11/06/2023 13:56:54 - WARNING - llmtuner.tuner.core.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
/root/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/training_args.py:1697: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
Traceback (most recent call last):
  File "/opt/LLaMA-Factory/src/train_bash.py", line 14, in <module>
    main()
  File "/opt/LLaMA-Factory/src/train_bash.py", line 5, in main
    run_exp()
  File "/opt/LLaMA-Factory/src/llmtuner/tuner/tune.py", line 20, in run_exp
    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)
  File "/opt/LLaMA-Factory/src/llmtuner/tuner/core/parser.py", line 174, in get_train_args
    raise ValueError("Output directory already exists and is not empty. Please set `overwrite_output_dir`.")
ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.
[2023-11-06 13:56:54,699] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 45679
[2023-11-06 13:56:54,700] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 45680
[2023-11-06 13:56:55,115] [ERROR] [launch.py:321:sigkill_handler] ['/root/miniconda3/envs/llama/bin/python', '-u', 'src/train_bash.py', '--local_rank=1', '--deepspeed', 'ds_config.json', '--stage', 'sft', '--model_name_or_path', '/opt/qwenmodel', '--do_train', 'True', '--finetuning_type', 'lora', '--quantization_bit', '4', '--template', 'chatml', '--flash_attn', 'False', '--shift_attn', 'False', '--dataset_dir', 'data', '--dataset', 'alpaca_en', '--cutoff_len', '4048', '--learning_rate', '5e-05', '--num_train_epochs', '3.0', '--max_samples', '100000', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'cosine', '--max_grad_norm', '1.0', '--logging_steps', '5', '--save_steps', '100', '--warmup_steps', '0', '--neft_alpha', '0', '--train_on_prompt', 'False', '--upcast_layernorm', 'True', '--lora_rank', '8', '--lora_dropout', '0.1', '--lora_target', 'c_attn', '--resume_lora_training', 'True', '--output_dir', 'saves/Qwen-14B-Chat/lora/2023-11-06-11-11-295', '--fp16', 'True', '--plot_loss', 'True'] exits with return code = 1
[2023-11-06 13:57:17,284] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-06 13:57:19,614] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-11-06 13:57:19,614] [INFO] [runner.py:570:main] cmd = /root/miniconda3/envs/llama/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=9901 --enable_each_rank_log=None src/train_bash.py --deepspeed ds_config.json --stage sft --model_name_or_path /opt/qwenmodel --do_train True --finetuning_type lora --quantization_bit 4 --template chatml --flash_attn False --shift_attn False --dataset_dir data --dataset alpaca_en --cutoff_len 4048 --learning_rate 5e-05 --num_train_epochs 3.0 --max_samples 100000 --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --max_grad_norm 1.0 --logging_steps 5 --save_steps 100 --warmup_steps 0 --neft_alpha 0 --train_on_prompt False --upcast_layernorm True --lora_rank 8 --lora_dropout 0.1 --lora_target c_attn --resume_lora_training True --output_dir saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296 --fp16 True --plot_loss True
[2023-11-06 13:57:21,993] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-06 13:57:24,054] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2023-11-06 13:57:24,054] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2023-11-06 13:57:24,054] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2023-11-06 13:57:24,054] [INFO] [launch.py:163:main] dist_world_size=2
[2023-11-06 13:57:24,054] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2023-11-06 13:57:29,724] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-06 13:57:29,744] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
FlashAttention-2 is not installed, ignore this if you are not using FlashAttention.
FlashAttention-2 is not installed, ignore this if you are not using FlashAttention.
[2023-11-06 13:57:30,992] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-11-06 13:57:30,992] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-11-06 13:57:31,015] [INFO] [comm.py:637:init_distributed] cdb=None
11/06/2023 13:57:31 - WARNING - llmtuner.tuner.core.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|training_args.py:1784] 2023-11-06 13:57:31,029 >> PyTorch: setting up devices
/root/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/training_args.py:1697: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
11/06/2023 13:57:31 - INFO - llmtuner.tuner.core.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
11/06/2023 13:57:31 - INFO - llmtuner.tuner.core.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/runs/Nov06_13-57-30_47f2ea7d05bd,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=5,
logging_strategy=steps,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296,
save_on_each_node=False,
save_safetensors=True,
save_steps=100,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
11/06/2023 13:57:31 - INFO - llmtuner.dsets.loader - Loading dataset json_result_nocontext.json...
11/06/2023 13:57:31 - WARNING - llmtuner.dsets.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1ef520900a94b5a4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
[INFO|tokenization_utils_base.py:2020] 2023-11-06 13:57:32,112 >> loading file qwen.tiktoken
[INFO|tokenization_utils_base.py:2020] 2023-11-06 13:57:32,112 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2020] 2023-11-06 13:57:32,112 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2020] 2023-11-06 13:57:32,112 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2020] 2023-11-06 13:57:32,112 >> loading file tokenizer.json
11/06/2023 13:57:32 - WARNING - llmtuner.tuner.core.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
/root/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/training_args.py:1697: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
11/06/2023 13:57:32 - INFO - llmtuner.tuner.core.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
11/06/2023 13:57:32 - INFO - llmtuner.tuner.core.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/runs/Nov06_13-57-31_47f2ea7d05bd,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=5,
logging_strategy=steps,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296,
save_on_each_node=False,
save_safetensors=True,
save_steps=100,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
11/06/2023 13:57:32 - INFO - llmtuner.dsets.loader - Loading dataset json_result_nocontext.json...
11/06/2023 13:57:32 - WARNING - llmtuner.dsets.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
[INFO|configuration_utils.py:715] 2023-11-06 13:57:32,680 >> loading configuration file /opt/qwenmodel/config.json
[INFO|configuration_utils.py:715] 2023-11-06 13:57:32,681 >> loading configuration file /opt/qwenmodel/config.json
[INFO|configuration_utils.py:777] 2023-11-06 13:57:32,682 >> Model config QWenConfig {
  "_name_or_path": "/opt/qwenmodel",
  "architectures": [
    "QWenLMHeadModel"
  ],
  "attn_dropout_prob": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_qwen.QWenConfig",
    "AutoModelForCausalLM": "modeling_qwen.QWenLMHeadModel"
  },
  "bf16": false,
  "emb_dropout_prob": 0.0,
  "fp16": false,
  "fp32": false,
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 27392,
  "kv_channels": 128,
  "layer_norm_epsilon": 1e-06,
  "max_position_embeddings": 8192,
  "model_type": "qwen",
  "no_bias": true,
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "onnx_safe": null,
  "rotary_emb_base": 10000,
  "rotary_pct": 1.0,
  "scale_attn_weights": true,
  "seq_length": 2048,
  "softmax_in_fp32": false,
  "tie_word_embeddings": false,
  "tokenizer_class": "QWenTokenizer",
  "transformers_version": "4.35.0",
  "use_cache": true,
  "use_cache_kernel": false,
  "use_cache_quantization": false,
  "use_dynamic_ntk": true,
  "use_flash_attn": "auto",
  "use_logn_attn": true,
  "vocab_size": 152064
}

11/06/2023 13:57:32 - INFO - llmtuner.tuner.core.loader - Quantizing model to 4 bit.
[INFO|modeling_utils.py:3118] 2023-11-06 13:57:32,712 >> loading weights file /opt/qwenmodel/model.safetensors.index.json
[INFO|modeling_utils.py:1222] 2023-11-06 13:57:32,712 >> Instantiating QWenLMHeadModel model under default dtype torch.float16.
[INFO|configuration_utils.py:791] 2023-11-06 13:57:32,713 >> Generate config GenerationConfig {}

Warning: please make sure that you are using the latest codes and checkpoints, especially if you used Qwen-7B before 09.25.2023.请使用最新模型和代码，尤其如果你在9月25日前已经开始使用Qwen-7B，千万注意不要使用错误代码和模型。
Try importing flash-attention for faster inference...
Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention
[INFO|modeling_utils.py:3257] 2023-11-06 13:57:33,263 >> Detected 4-bit loading: activating 4-bit loading for this model
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1ef520900a94b5a4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
11/06/2023 13:57:34 - INFO - llmtuner.tuner.core.loader - Quantizing model to 4 bit.
Warning: please make sure that you are using the latest codes and checkpoints, especially if you used Qwen-7B before 09.25.2023.请使用最新模型和代码，尤其如果你在9月25日前已经开始使用Qwen-7B，千万注意不要使用错误代码和模型。
Try importing flash-attention for faster inference...
Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:02<00:41,  2.94s/it]Loading checkpoint shards:   7%|▋         | 1/15 [00:04<01:02,  4.49s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:05<00:32,  2.52s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:06<00:40,  3.13s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:07<00:27,  2.28s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:08<00:31,  2.66s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:08<00:22,  2.01s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:10<00:24,  2.23s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:10<00:18,  1.86s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:11<00:19,  1.99s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:11<00:16,  1.78s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:13<00:16,  1.85s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:15<00:14,  1.75s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:13<00:13,  1.71s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:16<00:11,  1.71s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:15<00:11,  1.68s/it]Loading checkpoint shards:  60%|██████    | 9/15 [00:18<00:09,  1.65s/it]Loading checkpoint shards:  60%|██████    | 9/15 [00:16<00:09,  1.63s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [00:18<00:07,  1.59s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [00:19<00:08,  1.61s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [00:19<00:06,  1.55s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [00:21<00:06,  1.61s/it]Loading checkpoint shards:  80%|████████  | 12/15 [00:21<00:04,  1.51s/it]Loading checkpoint shards:  80%|████████  | 12/15 [00:22<00:04,  1.56s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [00:22<00:02,  1.47s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [00:24<00:02,  1.49s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [00:23<00:01,  1.42s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [00:25<00:01,  1.54s/it]Loading checkpoint shards: 100%|██████████| 15/15 [00:24<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 15/15 [00:24<00:00,  1.67s/it]
11/06/2023 13:58:00 - INFO - llmtuner.tuner.core.utils - Upcasting weights in layernorm in float32.
11/06/2023 13:58:00 - INFO - llmtuner.tuner.core.utils - Gradient checkpointing enabled.
11/06/2023 13:58:00 - INFO - llmtuner.tuner.core.adapter - Fine-tuning method: LoRA
11/06/2023 13:58:00 - INFO - llmtuner.tuner.core.loader - trainable params: 6553600 || all params: 14173844480 || trainable%: 0.0462
11/06/2023 13:58:00 - INFO - llmtuner.extras.template - Add eos token: <|endoftext|>
11/06/2023 13:58:00 - INFO - llmtuner.extras.template - Add pad token: <|endoftext|>
Loading checkpoint shards: 100%|██████████| 15/15 [00:26<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 15/15 [00:26<00:00,  1.79s/it]
[INFO|modeling_utils.py:3950] 2023-11-06 13:58:00,267 >> All model checkpoint weights were used when initializing QWenLMHeadModel.

[INFO|modeling_utils.py:3958] 2023-11-06 13:58:00,267 >> All the weights of QWenLMHeadModel were initialized from the model checkpoint at /opt/qwenmodel.
If your task is similar to the task the model of the checkpoint was trained on, you can already use QWenLMHeadModel for predictions without further training.
[INFO|configuration_utils.py:749] 2023-11-06 13:58:00,270 >> loading configuration file /opt/qwenmodel/generation_config.json
[INFO|configuration_utils.py:791] 2023-11-06 13:58:00,270 >> Generate config GenerationConfig {
  "chat_format": "chatml",
  "do_sample": true,
  "eos_token_id": 151643,
  "max_new_tokens": 512,
  "max_window_size": 6144,
  "pad_token_id": 151643,
  "repetition_penalty": 1.1,
  "top_k": 0,
  "top_p": 0.8
}

11/06/2023 13:58:00 - INFO - llmtuner.tuner.core.utils - Upcasting weights in layernorm in float32.
11/06/2023 13:58:00 - INFO - llmtuner.tuner.core.utils - Gradient checkpointing enabled.
11/06/2023 13:58:00 - INFO - llmtuner.tuner.core.adapter - Fine-tuning method: LoRA
11/06/2023 13:58:00 - INFO - llmtuner.tuner.core.loader - trainable params: 6553600 || all params: 14173844480 || trainable%: 0.0462
11/06/2023 13:58:00 - INFO - llmtuner.extras.template - Add eos token: <|endoftext|>
11/06/2023 13:58:00 - INFO - llmtuner.extras.template - Add pad token: <|endoftext|>
Running tokenizer on dataset:   0%|          | 0/7317 [00:00<?, ? examples/s]Running tokenizer on dataset:  14%|█▎        | 1000/7317 [00:00<00:04, 1536.96 examples/s]Running tokenizer on dataset:  27%|██▋       | 2000/7317 [00:01<00:03, 1505.55 examples/s]Running tokenizer on dataset:  41%|████      | 3000/7317 [00:01<00:02, 1753.55 examples/s]Running tokenizer on dataset:  55%|█████▍    | 4000/7317 [00:02<00:01, 1893.43 examples/s]Running tokenizer on dataset:  68%|██████▊   | 5000/7317 [00:02<00:01, 1972.20 examples/s]Running tokenizer on dataset:  82%|████████▏ | 6000/7317 [00:03<00:00, 1958.15 examples/s]Running tokenizer on dataset:  96%|█████████▌| 7000/7317 [00:03<00:00, 1717.18 examples/s]Running tokenizer on dataset: 100%|██████████| 7317/7317 [00:04<00:00, 1648.02 examples/s]                                                                                          input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 106559, 105588, 101121, 33108, 100176, 100142, 151645, 198, 151644, 77091, 198, 100398, 88970, 26940, 99655, 107216, 105588, 101121, 33108, 100176, 100142, 99812, 25067, 1773, 99655, 107216, 105588, 101121, 33108, 100176, 100142, 99812, 23671, 33108, 107576, 100355, 26940, 105492, 104773, 105588, 101693, 24339, 25067, 100241, 1773, 151643]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
强制招标范围和规模标准<|im_end|>
<|im_start|>assistant
具体见《工程建设项目招标范围和规模标准规定》。工程建设项目招标范围和规模标准规定.doc和安徽省实施《中华人民共和国招标投标法》办法。<|endoftext|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 100398, 88970, 26940, 99655, 107216, 105588, 101121, 33108, 100176, 100142, 99812, 25067, 1773, 99655, 107216, 105588, 101121, 33108, 100176, 100142, 99812, 23671, 33108, 107576, 100355, 26940, 105492, 104773, 105588, 101693, 24339, 25067, 100241, 1773, 151643]
labels:
具体见《工程建设项目招标范围和规模标准规定》。工程建设项目招标范围和规模标准规定.doc和安徽省实施《中华人民共和国招标投标法》办法。<|endoftext|>
[INFO|training_args.py:1784] 2023-11-06 13:58:07,284 >> PyTorch: setting up devices
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:593] 2023-11-06 13:58:07,294 >> Using auto half precision backend
[2023-11-06 13:58:07,548] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.2, git-hash=unknown, git-branch=unknown
Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1ef520900a94b5a4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-24e9892f3d7c42f1.arrow
[2023-11-06 13:58:09,185] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-11-06 13:58:09,187] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-11-06 13:58:09,188] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2023-11-06 13:58:09,192] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-11-06 13:58:09,193] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-11-06 13:58:09,193] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-11-06 13:58:09,193] [INFO] [stage_1_and_2.py:147:__init__] Reduce bucket size 500000000
[2023-11-06 13:58:09,193] [INFO] [stage_1_and_2.py:148:__init__] Allgather bucket size 500000000
[2023-11-06 13:58:09,193] [INFO] [stage_1_and_2.py:149:__init__] CPU Offload: False
[2023-11-06 13:58:09,193] [INFO] [stage_1_and_2.py:150:__init__] Round robin gradient partitioning: False
[2023-11-06 13:58:10,380] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
[2023-11-06 13:58:10,381] [INFO] [utils.py:803:see_memory_usage] MA 9.13 GB         Max_MA 9.14 GB         CA 9.16 GB         Max_CA 9 GB 
[2023-11-06 13:58:10,381] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 22.5 GB, percent = 3.6%
[2023-11-06 13:58:10,529] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
[2023-11-06 13:58:10,530] [INFO] [utils.py:803:see_memory_usage] MA 9.16 GB         Max_MA 9.18 GB         CA 9.21 GB         Max_CA 9 GB 
[2023-11-06 13:58:10,530] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 22.62 GB, percent = 3.6%
[2023-11-06 13:58:10,530] [INFO] [stage_1_and_2.py:514:__init__] optimizer state initialized
[2023-11-06 13:58:10,673] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
[2023-11-06 13:58:10,673] [INFO] [utils.py:803:see_memory_usage] MA 9.16 GB         Max_MA 9.16 GB         CA 9.21 GB         Max_CA 9 GB 
[2023-11-06 13:58:10,674] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 22.75 GB, percent = 3.6%
[2023-11-06 13:58:10,676] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-11-06 13:58:10,676] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-11-06 13:58:10,676] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-11-06 13:58:10,676] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]
[2023-11-06 13:58:10,677] [INFO] [config.py:972:print] DeepSpeedEngine configuration:
[2023-11-06 13:58:10,677] [INFO] [config.py:976:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-11-06 13:58:10,677] [INFO] [config.py:976:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-11-06 13:58:10,677] [INFO] [config.py:976:print]   amp_enabled .................. False
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   amp_params ................... False
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   bfloat16_enabled ............. False
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   checkpoint_parallel_write_pipeline  False
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   checkpoint_tag_validation_enabled  True
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   checkpoint_tag_validation_fail  False
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc8f79f2430>
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   communication_data_type ...... None
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   curriculum_enabled_legacy .... False
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   curriculum_params_legacy ..... False
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   data_efficiency_enabled ...... False
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   dataloader_drop_last ......... False
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   disable_allgather ............ False
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   dump_state ................... False
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   eigenvalue_enabled ........... False
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   eigenvalue_gas_boundary_resolution  1
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   eigenvalue_layer_num ......... 0
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   eigenvalue_max_iter .......... 100
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   eigenvalue_stability ......... 1e-06
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   eigenvalue_tol ............... 0.01
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   eigenvalue_verbose ........... False
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   elasticity_enabled ........... False
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   fp16_auto_cast ............... False
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   fp16_enabled ................. True
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   fp16_master_weights_and_gradients  False
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   global_rank .................. 0
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   grad_accum_dtype ............. None
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   gradient_accumulation_steps .. 4
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   gradient_clipping ............ 1.0
[2023-11-06 13:58:10,678] [INFO] [config.py:976:print]   gradient_predivide_factor .... 1.0
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   initial_dynamic_scale ........ 65536
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   load_universal_checkpoint .... False
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   loss_scale ................... 0
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   memory_breakdown ............. False
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   mics_hierarchial_params_gather  False
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   mics_shard_size .............. -1
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   optimizer_legacy_fusion ...... False
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   optimizer_name ............... None
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   optimizer_params ............. None
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   pld_enabled .................. False
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   pld_params ................... False
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   prescale_gradients ........... False
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   scheduler_name ............... None
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   scheduler_params ............. None
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   seq_parallel_communication_data_type  torch.float32
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   sparse_attention ............. None
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   sparse_gradients_enabled ..... False
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   steps_per_print .............. inf
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   train_batch_size ............. 8
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   train_micro_batch_size_per_gpu  1
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   use_node_local_storage ....... False
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   wall_clock_breakdown ......... False
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   weight_quantization_config ... None
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   world_size ................... 2
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   zero_allow_untested_optimizer  True
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   zero_enabled ................. True
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   zero_force_ds_cpu_optimizer .. True
[2023-11-06 13:58:10,679] [INFO] [config.py:976:print]   zero_optimization_stage ...... 2
[2023-11-06 13:58:10,679] [INFO] [config.py:962:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 4, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "overlap_comm": false, 
        "contiguous_gradients": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": false
    }
}
[INFO|trainer.py:1723] 2023-11-06 13:58:10,680 >> ***** Running training *****
[INFO|trainer.py:1724] 2023-11-06 13:58:10,680 >>   Num examples = 7,317
[INFO|trainer.py:1725] 2023-11-06 13:58:10,680 >>   Num Epochs = 3
[INFO|trainer.py:1726] 2023-11-06 13:58:10,680 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1729] 2023-11-06 13:58:10,680 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1730] 2023-11-06 13:58:10,680 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1731] 2023-11-06 13:58:10,680 >>   Total optimization steps = 2,742
[INFO|trainer.py:1732] 2023-11-06 13:58:10,681 >>   Number of trainable parameters = 6,553,600
  0%|          | 0/2742 [00:00<?, ?it/s]/root/miniconda3/envs/llama/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1881: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/root/miniconda3/envs/llama/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1881: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
  0%|          | 1/2742 [00:02<2:12:37,  2.90s/it]  0%|          | 2/2742 [00:03<1:20:41,  1.77s/it]  0%|          | 3/2742 [00:07<2:04:46,  2.73s/it]  0%|          | 4/2742 [00:08<1:35:07,  2.08s/it]  0%|          | 5/2742 [00:09<1:16:13,  1.67s/it]                                                  {'loss': 2.765, 'learning_rate': 4.9999589782977095e-05, 'epoch': 0.01}
  0%|          | 5/2742 [00:09<1:16:13,  1.67s/it]  0%|          | 6/2742 [00:10<1:05:15,  1.43s/it]  0%|          | 7/2742 [00:11<57:30,  1.26s/it]    0%|          | 8/2742 [00:12<53:49,  1.18s/it]  0%|          | 9/2742 [00:13<51:59,  1.14s/it]  0%|          | 10/2742 [00:14<48:43,  1.07s/it]                                                 {'loss': 2.5239, 'learning_rate': 4.9998359145370634e-05, 'epoch': 0.01}
  0%|          | 10/2742 [00:14<48:43,  1.07s/it]  0%|          | 11/2742 [00:15<46:28,  1.02s/it]  0%|          | 12/2742 [00:16<44:50,  1.01it/s]  0%|          | 13/2742 [00:17<45:08,  1.01it/s]  1%|          | 14/2742 [00:18<43:56,  1.03it/s]  1%|          | 15/2742 [00:19<43:31,  1.04it/s]                                                 {'loss': 2.8549, 'learning_rate': 4.9996308127566885e-05, 'epoch': 0.02}
  1%|          | 15/2742 [00:19<43:31,  1.04it/s]  1%|          | 16/2742 [00:20<42:59,  1.06it/s]  1%|          | 17/2742 [00:21<42:45,  1.06it/s]  1%|          | 18/2742 [00:22<42:10,  1.08it/s]  1%|          | 19/2742 [00:22<42:16,  1.07it/s]  1%|          | 20/2742 [00:23<42:11,  1.08it/s]                                                 {'loss': 2.4265, 'learning_rate': 4.999343679687485e-05, 'epoch': 0.02}
  1%|          | 20/2742 [00:23<42:11,  1.08it/s]  1%|          | 21/2742 [00:24<42:45,  1.06it/s]  1%|          | 22/2742 [00:26<45:12,  1.00it/s]  1%|          | 23/2742 [00:26<43:39,  1.04it/s]  1%|          | 24/2742 [00:27<42:48,  1.06it/s]  1%|          | 25/2742 [00:28<42:14,  1.07it/s]                                                 {'loss': 2.1576, 'learning_rate': 4.9989745247524017e-05, 'epoch': 0.03}
  1%|          | 25/2742 [00:28<42:14,  1.07it/s]  1%|          | 26/2742 [00:29<44:06,  1.03it/s]  1%|          | 27/2742 [00:30<43:09,  1.05it/s]  1%|          | 28/2742 [00:31<42:16,  1.07it/s]  1%|          | 29/2742 [00:32<42:00,  1.08it/s]  1%|          | 30/2742 [00:33<45:27,  1.01s/it]                                                 {'loss': 2.2498, 'learning_rate': 4.9985233600661306e-05, 'epoch': 0.03}
  1%|          | 30/2742 [00:33<45:27,  1.01s/it]  1%|          | 31/2742 [00:34<44:01,  1.03it/s]  1%|          | 32/2742 [00:35<43:39,  1.03it/s]  1%|          | 33/2742 [00:36<43:17,  1.04it/s]  1%|          | 34/2742 [00:37<43:26,  1.04it/s]  1%|▏         | 35/2742 [00:38<43:23,  1.04it/s]                                                 {'loss': 2.0751, 'learning_rate': 4.997990200434706e-05, 'epoch': 0.04}
  1%|▏         | 35/2742 [00:38<43:23,  1.04it/s]  1%|▏         | 36/2742 [00:39<43:08,  1.05it/s]  1%|▏         | 37/2742 [00:40<43:22,  1.04it/s]  1%|▏         | 38/2742 [00:41<44:56,  1.00it/s]  1%|▏         | 39/2742 [00:42<44:18,  1.02it/s]  1%|▏         | 40/2742 [00:43<43:26,  1.04it/s]                                                 {'loss': 2.1249, 'learning_rate': 4.99737506335502e-05, 'epoch': 0.04}
  1%|▏         | 40/2742 [00:43<43:26,  1.04it/s]  1%|▏         | 41/2742 [00:44<43:48,  1.03it/s]  2%|▏         | 42/2742 [00:45<43:22,  1.04it/s]  2%|▏         | 43/2742 [00:46<43:08,  1.04it/s]  2%|▏         | 44/2742 [00:47<42:33,  1.06it/s]  2%|▏         | 45/2742 [00:48<42:24,  1.06it/s]                                                 {'loss': 1.8989, 'learning_rate': 4.996677969014251e-05, 'epoch': 0.05}
  2%|▏         | 45/2742 [00:48<42:24,  1.06it/s]  2%|▏         | 46/2742 [00:49<43:44,  1.03it/s]  2%|▏         | 47/2742 [00:50<43:50,  1.02it/s]  2%|▏         | 48/2742 [00:51<44:52,  1.00it/s]  2%|▏         | 49/2742 [00:52<44:41,  1.00it/s]  2%|▏         | 50/2742 [00:53<45:01,  1.00s/it]                                                 {'loss': 1.8847, 'learning_rate': 4.995898940289192e-05, 'epoch': 0.05}
  2%|▏         | 50/2742 [00:53<45:01,  1.00s/it]  2%|▏         | 51/2742 [00:54<46:28,  1.04s/it]  2%|▏         | 52/2742 [00:55<44:28,  1.01it/s]  2%|▏         | 53/2742 [00:56<47:14,  1.05s/it]  2%|▏         | 54/2742 [00:57<48:20,  1.08s/it]  2%|▏         | 55/2742 [00:58<47:46,  1.07s/it]                                                 {'loss': 1.7067, 'learning_rate': 4.995038002745515e-05, 'epoch': 0.06}
  2%|▏         | 55/2742 [00:58<47:46,  1.07s/it]  2%|▏         | 56/2742 [00:59<45:17,  1.01s/it]  2%|▏         | 57/2742 [01:00<44:51,  1.00s/it]  2%|▏         | 58/2742 [01:01<43:40,  1.02it/s]  2%|▏         | 59/2742 [01:02<44:18,  1.01it/s]  2%|▏         | 60/2742 [01:03<43:23,  1.03it/s]                                                 {'loss': 1.9867, 'learning_rate': 4.9940951846369166e-05, 'epoch': 0.07}
  2%|▏         | 60/2742 [01:03<43:23,  1.03it/s]  2%|▏         | 61/2742 [01:04<43:46,  1.02it/s]  2%|▏         | 62/2742 [01:05<52:25,  1.17s/it]  2%|▏         | 63/2742 [01:06<48:56,  1.10s/it]  2%|▏         | 64/2742 [01:07<48:20,  1.08s/it]  2%|▏         | 65/2742 [01:08<46:58,  1.05s/it]                                                 {'loss': 1.8116, 'learning_rate': 4.9930705169042e-05, 'epoch': 0.07}
  2%|▏         | 65/2742 [01:08<46:58,  1.05s/it]  2%|▏         | 66/2742 [01:09<45:54,  1.03s/it]  2%|▏         | 67/2742 [01:10<45:05,  1.01s/it]  2%|▏         | 68/2742 [01:11<44:50,  1.01s/it]  3%|▎         | 69/2742 [01:12<44:33,  1.00s/it]  3%|▎         | 70/2742 [01:13<45:12,  1.02s/it]                                                 {'loss': 1.7734, 'learning_rate': 4.9919640331742566e-05, 'epoch': 0.08}
  3%|▎         | 70/2742 [01:13<45:12,  1.02s/it]  3%|▎         | 71/2742 [01:14<46:23,  1.04s/it]  3%|▎         | 72/2742 [01:15<45:09,  1.01s/it]  3%|▎         | 73/2742 [01:16<43:54,  1.01it/s]  3%|▎         | 74/2742 [01:17<43:28,  1.02it/s]  3%|▎         | 75/2742 [01:18<42:42,  1.04it/s]                                                 {'loss': 1.5662, 'learning_rate': 4.990775769758964e-05, 'epoch': 0.08}
  3%|▎         | 75/2742 [01:18<42:42,  1.04it/s]  3%|▎         | 76/2742 [01:19<43:09,  1.03it/s]  3%|▎         | 77/2742 [01:20<44:15,  1.00it/s]  3%|▎         | 78/2742 [01:21<43:30,  1.02it/s]  3%|▎         | 79/2742 [01:22<43:41,  1.02it/s]  3%|▎         | 80/2742 [01:23<43:35,  1.02it/s]                                                 {'loss': 1.8894, 'learning_rate': 4.989505765653993e-05, 'epoch': 0.09}
  3%|▎         | 80/2742 [01:23<43:35,  1.02it/s]  3%|▎         | 81/2742 [01:24<42:59,  1.03it/s]  3%|▎         | 82/2742 [01:25<42:33,  1.04it/s]  3%|▎         | 83/2742 [01:26<42:02,  1.05it/s]  3%|▎         | 84/2742 [01:27<41:53,  1.06it/s]  3%|▎         | 85/2742 [01:28<41:23,  1.07it/s]                                                 {'loss': 1.7264, 'learning_rate': 4.988154062537527e-05, 'epoch': 0.09}
  3%|▎         | 85/2742 [01:28<41:23,  1.07it/s]  3%|▎         | 86/2742 [01:29<42:50,  1.03it/s]  3%|▎         | 87/2742 [01:30<43:09,  1.03it/s]  3%|▎         | 88/2742 [01:31<42:15,  1.05it/s]  3%|▎         | 89/2742 [01:32<42:28,  1.04it/s]  3%|▎         | 90/2742 [01:33<42:49,  1.03it/s]                                                 {'loss': 1.6909, 'learning_rate': 4.9867207047688966e-05, 'epoch': 0.1}
  3%|▎         | 90/2742 [01:33<42:49,  1.03it/s]  3%|▎         | 91/2742 [01:34<42:21,  1.04it/s]  3%|▎         | 92/2742 [01:34<41:46,  1.06it/s]  3%|▎         | 93/2742 [01:35<41:22,  1.07it/s]  3%|▎         | 94/2742 [01:36<41:22,  1.07it/s]  3%|▎         | 95/2742 [01:37<41:56,  1.05it/s]                                                 {'loss': 1.5717, 'learning_rate': 4.985205739387122e-05, 'epoch': 0.1}
  3%|▎         | 95/2742 [01:37<41:56,  1.05it/s]  4%|▎         | 96/2742 [01:38<41:06,  1.07it/s]  4%|▎         | 97/2742 [01:39<40:54,  1.08it/s]  4%|▎         | 98/2742 [01:40<41:19,  1.07it/s]  4%|▎         | 99/2742 [01:41<42:18,  1.04it/s]  4%|▎         | 100/2742 [01:42<42:09,  1.04it/s]                                                  {'loss': 1.7002, 'learning_rate': 4.9836092161093713e-05, 'epoch': 0.11}
  4%|▎         | 100/2742 [01:42<42:09,  1.04it/s][INFO|trainer.py:2881] 2023-11-06 14:00:03,164 >> Saving model checkpoint to saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-100
[INFO|tokenization_utils_base.py:2428] 2023-11-06 14:00:03,245 >> tokenizer config file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-06 14:00:03,245 >> Special tokens file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-100/special_tokens_map.json
[2023-11-06 14:00:03,844] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step100 is about to be saved!
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-11-06 14:00:10,914] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-100/global_step100/mp_rank_00_model_states.pt
[2023-11-06 14:00:10,914] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-100/global_step100/mp_rank_00_model_states.pt...
[2023-11-06 14:00:41,441] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-100/global_step100/mp_rank_00_model_states.pt.
[2023-11-06 14:00:41,924] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 14:00:41,992] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 14:00:41,999] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-11-06 14:00:41,999] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
  4%|▎         | 101/2742 [02:32<11:28:34, 15.64s/it]  4%|▎         | 102/2742 [02:33<8:14:33, 11.24s/it]   4%|▍         | 103/2742 [02:34<5:58:51,  8.16s/it]  4%|▍         | 104/2742 [02:35<4:22:46,  5.98s/it]  4%|▍         | 105/2742 [02:36<3:16:17,  4.47s/it]                                                    {'loss': 1.914, 'learning_rate': 4.981931187329325e-05, 'epoch': 0.11}
  4%|▍         | 105/2742 [02:36<3:16:17,  4.47s/it]  4%|▍         | 106/2742 [02:37<2:32:10,  3.46s/it]  4%|▍         | 107/2742 [02:38<1:58:59,  2.71s/it]  4%|▍         | 108/2742 [02:39<1:37:41,  2.23s/it]  4%|▍         | 109/2742 [02:40<1:20:56,  1.84s/it]  4%|▍         | 110/2742 [02:41<1:08:47,  1.57s/it]                                                    {'loss': 1.6608, 'learning_rate': 4.980171708115463e-05, 'epoch': 0.12}
  4%|▍         | 110/2742 [02:41<1:08:47,  1.57s/it]  4%|▍         | 111/2742 [02:42<1:00:18,  1.38s/it]  4%|▍         | 112/2742 [02:43<54:32,  1.24s/it]    4%|▍         | 113/2742 [02:44<49:51,  1.14s/it]  4%|▍         | 114/2742 [02:44<46:34,  1.06s/it]  4%|▍         | 115/2742 [02:45<45:10,  1.03s/it]                                                  {'loss': 1.7773, 'learning_rate': 4.978330836209249e-05, 'epoch': 0.13}
  4%|▍         | 115/2742 [02:45<45:10,  1.03s/it]  4%|▍         | 116/2742 [02:46<44:21,  1.01s/it]  4%|▍         | 117/2742 [02:47<43:51,  1.00s/it]  4%|▍         | 118/2742 [02:49<49:48,  1.14s/it]  4%|▍         | 119/2742 [02:51<1:04:13,  1.47s/it]  4%|▍         | 120/2742 [02:52<57:31,  1.32s/it]                                                    {'loss': 1.7781, 'learning_rate': 4.976408632023243e-05, 'epoch': 0.13}
  4%|▍         | 120/2742 [02:52<57:31,  1.32s/it]  4%|▍         | 121/2742 [02:53<52:45,  1.21s/it]  4%|▍         | 122/2742 [02:54<49:33,  1.13s/it]  4%|▍         | 123/2742 [02:55<46:54,  1.07s/it]  5%|▍         | 124/2742 [02:56<45:38,  1.05s/it]  5%|▍         | 125/2742 [02:57<46:17,  1.06s/it]                                                  {'loss': 1.65, 'learning_rate': 4.974405158639116e-05, 'epoch': 0.14}
  5%|▍         | 125/2742 [02:57<46:17,  1.06s/it]  5%|▍         | 126/2742 [02:58<48:02,  1.10s/it]  5%|▍         | 127/2742 [02:59<45:09,  1.04s/it]  5%|▍         | 128/2742 [03:00<46:39,  1.07s/it]  5%|▍         | 129/2742 [03:01<45:12,  1.04s/it]  5%|▍         | 130/2742 [03:02<43:43,  1.00s/it]                                                  {'loss': 1.6619, 'learning_rate': 4.972320481805578e-05, 'epoch': 0.14}
  5%|▍         | 130/2742 [03:02<43:43,  1.00s/it]  5%|▍         | 131/2742 [03:03<43:22,  1.00it/s]  5%|▍         | 132/2742 [03:05<1:02:43,  1.44s/it]  5%|▍         | 133/2742 [03:07<1:07:35,  1.55s/it]  5%|▍         | 134/2742 [03:08<59:21,  1.37s/it]    5%|▍         | 135/2742 [03:09<55:58,  1.29s/it]                                                  {'loss': 1.5463, 'learning_rate': 4.9701546699362244e-05, 'epoch': 0.15}
  5%|▍         | 135/2742 [03:09<55:58,  1.29s/it]  5%|▍         | 136/2742 [03:10<51:18,  1.18s/it]  5%|▍         | 137/2742 [03:11<47:59,  1.11s/it]  5%|▌         | 138/2742 [03:12<45:31,  1.05s/it]  5%|▌         | 139/2742 [03:13<47:16,  1.09s/it]  5%|▌         | 140/2742 [03:14<45:16,  1.04s/it]                                                  {'loss': 1.6912, 'learning_rate': 4.967907794107286e-05, 'epoch': 0.15}
  5%|▌         | 140/2742 [03:14<45:16,  1.04s/it]  5%|▌         | 141/2742 [03:15<44:45,  1.03s/it]  5%|▌         | 142/2742 [03:16<45:24,  1.05s/it]  5%|▌         | 143/2742 [03:17<43:42,  1.01s/it]  5%|▌         | 144/2742 [03:18<43:39,  1.01s/it]  5%|▌         | 145/2742 [03:19<45:16,  1.05s/it]                                                  {'loss': 1.6862, 'learning_rate': 4.9655799280552996e-05, 'epoch': 0.16}
  5%|▌         | 145/2742 [03:19<45:16,  1.05s/it]  5%|▌         | 146/2742 [03:20<44:09,  1.02s/it]  5%|▌         | 147/2742 [03:21<42:51,  1.01it/s]  5%|▌         | 148/2742 [03:22<42:37,  1.01it/s]  5%|▌         | 149/2742 [03:23<42:46,  1.01it/s]  5%|▌         | 150/2742 [03:24<42:41,  1.01it/s]                                                  {'loss': 1.9073, 'learning_rate': 4.963171148174689e-05, 'epoch': 0.16}
  5%|▌         | 150/2742 [03:24<42:41,  1.01it/s]  6%|▌         | 151/2742 [03:25<42:43,  1.01it/s]  6%|▌         | 152/2742 [03:26<43:03,  1.00it/s]  6%|▌         | 153/2742 [03:27<42:58,  1.00it/s]  6%|▌         | 154/2742 [03:28<44:56,  1.04s/it]  6%|▌         | 155/2742 [03:29<43:04,  1.00it/s]                                                  {'loss': 1.7124, 'learning_rate': 4.9606815335152535e-05, 'epoch': 0.17}
  6%|▌         | 155/2742 [03:29<43:04,  1.00it/s]  6%|▌         | 156/2742 [03:30<42:02,  1.03it/s]  6%|▌         | 157/2742 [03:31<41:32,  1.04it/s]  6%|▌         | 158/2742 [03:32<41:26,  1.04it/s]  6%|▌         | 159/2742 [03:33<40:42,  1.06it/s]  6%|▌         | 160/2742 [03:34<42:09,  1.02it/s]                                                  {'loss': 1.6053, 'learning_rate': 4.958111165779579e-05, 'epoch': 0.17}
  6%|▌         | 160/2742 [03:34<42:09,  1.02it/s]  6%|▌         | 161/2742 [03:35<41:29,  1.04it/s]  6%|▌         | 162/2742 [03:36<40:47,  1.05it/s]  6%|▌         | 163/2742 [03:37<43:05,  1.00s/it]  6%|▌         | 164/2742 [03:38<42:28,  1.01it/s]  6%|▌         | 165/2742 [03:39<43:50,  1.02s/it]                                                  {'loss': 1.7768, 'learning_rate': 4.9554601293203535e-05, 'epoch': 0.18}
  6%|▌         | 165/2742 [03:39<43:50,  1.02s/it]  6%|▌         | 166/2742 [03:40<43:52,  1.02s/it]  6%|▌         | 167/2742 [03:41<43:09,  1.01s/it]  6%|▌         | 168/2742 [03:42<42:02,  1.02it/s]  6%|▌         | 169/2742 [03:43<41:37,  1.03it/s]  6%|▌         | 170/2742 [03:44<41:00,  1.05it/s]                                                  {'loss': 1.5656, 'learning_rate': 4.9527285111376e-05, 'epoch': 0.19}
  6%|▌         | 170/2742 [03:44<41:00,  1.05it/s]  6%|▌         | 171/2742 [03:45<41:04,  1.04it/s]  6%|▋         | 172/2742 [03:46<41:07,  1.04it/s]  6%|▋         | 173/2742 [03:47<42:42,  1.00it/s]  6%|▋         | 174/2742 [03:48<42:48,  1.00s/it]  6%|▋         | 175/2742 [03:49<42:09,  1.01it/s]                                                  {'loss': 1.6003, 'learning_rate': 4.9499164008758196e-05, 'epoch': 0.19}
  6%|▋         | 175/2742 [03:49<42:09,  1.01it/s]  6%|▋         | 176/2742 [03:50<41:28,  1.03it/s]  6%|▋         | 177/2742 [03:51<45:01,  1.05s/it]  6%|▋         | 178/2742 [03:52<42:55,  1.00s/it]  7%|▋         | 179/2742 [03:53<43:25,  1.02s/it]  7%|▋         | 180/2742 [03:54<41:52,  1.02it/s]                                                  {'loss': 1.5188, 'learning_rate': 4.947023890821054e-05, 'epoch': 0.2}
  7%|▋         | 180/2742 [03:54<41:52,  1.02it/s]  7%|▋         | 181/2742 [03:55<41:50,  1.02it/s]  7%|▋         | 182/2742 [03:56<42:46,  1.00s/it]  7%|▋         | 183/2742 [03:57<42:16,  1.01it/s]  7%|▋         | 184/2742 [03:58<42:17,  1.01it/s]  7%|▋         | 185/2742 [03:59<41:44,  1.02it/s]                                                  {'loss': 1.7297, 'learning_rate': 4.944051075897851e-05, 'epoch': 0.2}
  7%|▋         | 185/2742 [03:59<41:44,  1.02it/s]  7%|▋         | 186/2742 [04:00<41:13,  1.03it/s]  7%|▋         | 187/2742 [04:01<40:41,  1.05it/s]  7%|▋         | 188/2742 [04:02<40:36,  1.05it/s]  7%|▋         | 189/2742 [04:03<43:01,  1.01s/it]  7%|▋         | 190/2742 [04:04<43:35,  1.02s/it]                                                  {'loss': 1.6379, 'learning_rate': 4.940998053666154e-05, 'epoch': 0.21}
  7%|▋         | 190/2742 [04:04<43:35,  1.02s/it]  7%|▋         | 191/2742 [04:06<56:53,  1.34s/it]  7%|▋         | 192/2742 [04:07<52:33,  1.24s/it]  7%|▋         | 193/2742 [04:08<48:52,  1.15s/it]  7%|▋         | 194/2742 [04:09<46:11,  1.09s/it]  7%|▋         | 195/2742 [04:10<44:02,  1.04s/it]                                                  {'loss': 1.3812, 'learning_rate': 4.9378649243180987e-05, 'epoch': 0.21}
  7%|▋         | 195/2742 [04:10<44:02,  1.04s/it]  7%|▋         | 196/2742 [04:11<43:52,  1.03s/it]  7%|▋         | 197/2742 [04:12<42:51,  1.01s/it]  7%|▋         | 198/2742 [04:13<42:38,  1.01s/it]  7%|▋         | 199/2742 [04:14<42:20,  1.00it/s]  7%|▋         | 200/2742 [04:15<44:14,  1.04s/it]                                                  {'loss': 1.5404, 'learning_rate': 4.934651790674724e-05, 'epoch': 0.22}
  7%|▋         | 200/2742 [04:15<44:14,  1.04s/it][INFO|trainer.py:2881] 2023-11-06 14:02:45,791 >> Saving model checkpoint to saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-200
[INFO|tokenization_utils_base.py:2428] 2023-11-06 14:02:45,859 >> tokenizer config file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-06 14:02:45,860 >> Special tokens file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-200/special_tokens_map.json
[2023-11-06 14:02:46,500] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-11-06 14:02:54,471] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-200/global_step200/mp_rank_00_model_states.pt
[2023-11-06 14:02:54,472] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-200/global_step200/mp_rank_00_model_states.pt...
[2023-11-06 14:03:27,719] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-200/global_step200/mp_rank_00_model_states.pt.
[2023-11-06 14:03:28,203] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 14:03:28,314] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 14:03:28,317] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-11-06 14:03:28,317] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
  7%|▋         | 201/2742 [05:18<13:57:11, 19.77s/it]  7%|▋         | 202/2742 [05:19<9:59:39, 14.17s/it]   7%|▋         | 203/2742 [05:20<7:12:28, 10.22s/it]  7%|▋         | 204/2742 [05:21<5:14:32,  7.44s/it]  7%|▋         | 205/2742 [05:22<3:51:59,  5.49s/it]                                                    {'loss': 1.5195, 'learning_rate': 4.9313587581825996e-05, 'epoch': 0.22}
  7%|▋         | 205/2742 [05:22<3:51:59,  5.49s/it]  8%|▊         | 206/2742 [05:23<2:54:13,  4.12s/it]  8%|▊         | 207/2742 [05:24<2:15:36,  3.21s/it]  8%|▊         | 208/2742 [05:25<1:46:54,  2.53s/it]  8%|▊         | 209/2742 [05:26<1:27:20,  2.07s/it]  8%|▊         | 210/2742 [05:27<1:13:15,  1.74s/it]                                                    {'loss': 1.6466, 'learning_rate': 4.927985934910363e-05, 'epoch': 0.23}
  8%|▊         | 210/2742 [05:27<1:13:15,  1.74s/it]  8%|▊         | 211/2742 [05:28<1:03:13,  1.50s/it]  8%|▊         | 212/2742 [05:29<59:32,  1.41s/it]    8%|▊         | 213/2742 [05:30<53:39,  1.27s/it]  8%|▊         | 214/2742 [05:31<50:55,  1.21s/it]  8%|▊         | 215/2742 [05:32<48:53,  1.16s/it]                                                  {'loss': 1.5667, 'learning_rate': 4.924533431545179e-05, 'epoch': 0.24}
  8%|▊         | 215/2742 [05:32<48:53,  1.16s/it]  8%|▊         | 216/2742 [05:33<47:47,  1.14s/it]  8%|▊         | 217/2742 [05:34<46:10,  1.10s/it]  8%|▊         | 218/2742 [05:35<44:36,  1.06s/it]  8%|▊         | 219/2742 [05:36<44:15,  1.05s/it]  8%|▊         | 220/2742 [05:37<42:22,  1.01s/it]                                                  {'loss': 1.6966, 'learning_rate': 4.921001361389096e-05, 'epoch': 0.24}
  8%|▊         | 220/2742 [05:37<42:22,  1.01s/it]  8%|▊         | 221/2742 [05:38<40:56,  1.03it/s]  8%|▊         | 222/2742 [05:39<40:45,  1.03it/s]  8%|▊         | 223/2742 [05:40<40:56,  1.03it/s]  8%|▊         | 224/2742 [05:41<40:03,  1.05it/s]  8%|▊         | 225/2742 [05:42<39:14,  1.07it/s]                                                  {'loss': 1.5419, 'learning_rate': 4.9173898403553406e-05, 'epoch': 0.25}
  8%|▊         | 225/2742 [05:42<39:14,  1.07it/s]  8%|▊         | 226/2742 [05:43<39:16,  1.07it/s]  8%|▊         | 227/2742 [05:44<39:46,  1.05it/s]  8%|▊         | 228/2742 [05:45<38:48,  1.08it/s]  8%|▊         | 229/2742 [05:46<39:14,  1.07it/s]  8%|▊         | 230/2742 [05:47<38:52,  1.08it/s]                                                  {'loss': 1.5569, 'learning_rate': 4.913698986964506e-05, 'epoch': 0.25}
  8%|▊         | 230/2742 [05:47<38:52,  1.08it/s]  8%|▊         | 231/2742 [05:48<40:10,  1.04it/s]  8%|▊         | 232/2742 [05:49<40:19,  1.04it/s]  8%|▊         | 233/2742 [05:50<43:11,  1.03s/it]  9%|▊         | 234/2742 [05:51<41:48,  1.00s/it]  9%|▊         | 235/2742 [05:52<41:21,  1.01it/s]                                                  {'loss': 1.686, 'learning_rate': 4.909928922340661e-05, 'epoch': 0.26}
  9%|▊         | 235/2742 [05:52<41:21,  1.01it/s]  9%|▊         | 236/2742 [05:53<40:13,  1.04it/s]  9%|▊         | 237/2742 [05:54<40:01,  1.04it/s]  9%|▊         | 238/2742 [05:55<40:22,  1.03it/s]  9%|▊         | 239/2742 [05:56<40:03,  1.04it/s]  9%|▉         | 240/2742 [05:56<39:48,  1.05it/s]                                                  {'loss': 1.6753, 'learning_rate': 4.9060797702073824e-05, 'epoch': 0.26}
  9%|▉         | 240/2742 [05:56<39:48,  1.05it/s]  9%|▉         | 241/2742 [05:57<38:43,  1.08it/s]  9%|▉         | 242/2742 [05:58<38:30,  1.08it/s]  9%|▉         | 243/2742 [05:59<38:46,  1.07it/s]  9%|▉         | 244/2742 [06:00<38:33,  1.08it/s]  9%|▉         | 245/2742 [06:01<38:26,  1.08it/s]                                                  {'loss': 1.7361, 'learning_rate': 4.902151656883688e-05, 'epoch': 0.27}
  9%|▉         | 245/2742 [06:01<38:26,  1.08it/s]  9%|▉         | 246/2742 [06:02<41:26,  1.00it/s]  9%|▉         | 247/2742 [06:03<40:09,  1.04it/s]  9%|▉         | 248/2742 [06:04<39:35,  1.05it/s]  9%|▉         | 249/2742 [06:05<39:53,  1.04it/s]  9%|▉         | 250/2742 [06:06<40:27,  1.03it/s]                                                  {'loss': 1.3858, 'learning_rate': 4.898144711279894e-05, 'epoch': 0.27}
  9%|▉         | 250/2742 [06:06<40:27,  1.03it/s]  9%|▉         | 251/2742 [06:07<39:49,  1.04it/s]  9%|▉         | 252/2742 [06:08<39:28,  1.05it/s]  9%|▉         | 253/2742 [06:09<41:13,  1.01it/s]  9%|▉         | 254/2742 [06:10<40:20,  1.03it/s]  9%|▉         | 255/2742 [06:11<39:34,  1.05it/s]                                                  {'loss': 1.6192, 'learning_rate': 4.894059064893384e-05, 'epoch': 0.28}
  9%|▉         | 255/2742 [06:11<39:34,  1.05it/s]  9%|▉         | 256/2742 [06:12<41:17,  1.00it/s]  9%|▉         | 257/2742 [06:13<41:10,  1.01it/s]  9%|▉         | 258/2742 [06:14<43:24,  1.05s/it]  9%|▉         | 259/2742 [06:15<41:36,  1.01s/it]  9%|▉         | 260/2742 [06:16<41:46,  1.01s/it]                                                  {'loss': 1.3868, 'learning_rate': 4.889894851804293e-05, 'epoch': 0.28}
  9%|▉         | 260/2742 [06:16<41:46,  1.01s/it] 10%|▉         | 261/2742 [06:17<42:54,  1.04s/it] 10%|▉         | 262/2742 [06:18<42:01,  1.02s/it] 10%|▉         | 263/2742 [06:19<41:10,  1.00it/s] 10%|▉         | 264/2742 [06:20<41:01,  1.01it/s] 10%|▉         | 265/2742 [06:21<40:32,  1.02it/s]                                                  {'loss': 1.5556, 'learning_rate': 4.8856522086711104e-05, 'epoch': 0.29}
 10%|▉         | 265/2742 [06:21<40:32,  1.02it/s] 10%|▉         | 266/2742 [06:22<39:57,  1.03it/s] 10%|▉         | 267/2742 [06:23<39:51,  1.04it/s] 10%|▉         | 268/2742 [06:24<39:23,  1.05it/s] 10%|▉         | 269/2742 [06:25<39:40,  1.04it/s] 10%|▉         | 270/2742 [06:26<40:33,  1.02it/s]                                                  {'loss': 1.5009, 'learning_rate': 4.8813312747261894e-05, 'epoch': 0.3}
 10%|▉         | 270/2742 [06:26<40:33,  1.02it/s] 10%|▉         | 271/2742 [06:27<39:43,  1.04it/s] 10%|▉         | 272/2742 [06:28<43:39,  1.06s/it] 10%|▉         | 273/2742 [06:29<42:17,  1.03s/it] 10%|▉         | 274/2742 [06:30<40:54,  1.01it/s] 10%|█         | 275/2742 [06:31<41:36,  1.01s/it]                                                  {'loss': 1.6125, 'learning_rate': 4.876932191771185e-05, 'epoch': 0.3}
 10%|█         | 275/2742 [06:31<41:36,  1.01s/it] 10%|█         | 276/2742 [06:32<41:11,  1.00s/it] 10%|█         | 277/2742 [06:33<40:50,  1.01it/s] 10%|█         | 278/2742 [06:35<53:11,  1.30s/it] 10%|█         | 279/2742 [06:36<48:22,  1.18s/it] 10%|█         | 280/2742 [06:37<46:05,  1.12s/it]                                                  {'loss': 1.6118, 'learning_rate': 4.872455104172391e-05, 'epoch': 0.31}
 10%|█         | 280/2742 [06:37<46:05,  1.12s/it] 10%|█         | 281/2742 [06:38<44:49,  1.09s/it] 10%|█         | 282/2742 [06:39<43:27,  1.06s/it] 10%|█         | 283/2742 [06:40<41:56,  1.02s/it] 10%|█         | 284/2742 [06:41<41:18,  1.01s/it] 10%|█         | 285/2742 [06:42<40:07,  1.02it/s]                                                  {'loss': 1.4617, 'learning_rate': 4.867900158856014e-05, 'epoch': 0.31}
 10%|█         | 285/2742 [06:42<40:07,  1.02it/s] 10%|█         | 286/2742 [06:43<40:50,  1.00it/s] 10%|█         | 287/2742 [06:44<41:37,  1.02s/it] 11%|█         | 288/2742 [06:45<40:33,  1.01it/s] 11%|█         | 289/2742 [06:46<39:28,  1.04it/s] 11%|█         | 290/2742 [06:47<41:06,  1.01s/it]                                                  {'loss': 1.5788, 'learning_rate': 4.8632675053033415e-05, 'epoch': 0.32}
 11%|█         | 290/2742 [06:47<41:06,  1.01s/it] 11%|█         | 291/2742 [06:48<40:56,  1.00s/it] 11%|█         | 292/2742 [06:49<43:45,  1.07s/it] 11%|█         | 293/2742 [06:50<41:29,  1.02s/it] 11%|█         | 294/2742 [06:51<42:02,  1.03s/it] 11%|█         | 295/2742 [06:52<41:10,  1.01s/it]                                                  {'loss': 1.5915, 'learning_rate': 4.858557295545841e-05, 'epoch': 0.32}
 11%|█         | 295/2742 [06:52<41:10,  1.01s/it] 11%|█         | 296/2742 [06:53<41:48,  1.03s/it] 11%|█         | 297/2742 [06:54<40:52,  1.00s/it] 11%|█         | 298/2742 [06:55<39:59,  1.02it/s] 11%|█         | 299/2742 [06:56<39:40,  1.03it/s] 11%|█         | 300/2742 [06:57<39:19,  1.03it/s]                                                  {'loss': 1.6219, 'learning_rate': 4.85376968416017e-05, 'epoch': 0.33}
 11%|█         | 300/2742 [06:57<39:19,  1.03it/s][INFO|trainer.py:2881] 2023-11-06 14:05:16,638 >> Saving model checkpoint to saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-300
[INFO|tokenization_utils_base.py:2428] 2023-11-06 14:05:16,698 >> tokenizer config file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-06 14:05:16,699 >> Special tokens file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-300/special_tokens_map.json
[2023-11-06 14:05:18,009] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step300 is about to be saved!
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-11-06 14:05:25,387] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-300/global_step300/mp_rank_00_model_states.pt
[2023-11-06 14:05:25,388] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-300/global_step300/mp_rank_00_model_states.pt...
[2023-11-06 14:05:58,163] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-300/global_step300/mp_rank_00_model_states.pt.
[2023-11-06 14:05:58,635] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 14:05:58,688] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 14:05:58,688] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-11-06 14:05:58,688] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step300 is ready now!
 11%|█         | 301/2742 [07:48<11:00:46, 16.24s/it] 11%|█         | 302/2742 [07:49<7:53:23, 11.64s/it]  11%|█         | 303/2742 [07:50<5:42:19,  8.42s/it] 11%|█         | 304/2742 [07:51<4:11:10,  6.18s/it] 11%|█         | 305/2742 [07:52<3:06:51,  4.60s/it]                                                    {'loss': 1.6595, 'learning_rate': 4.8489048282631056e-05, 'epoch': 0.33}
 11%|█         | 305/2742 [07:52<3:06:51,  4.60s/it] 11%|█         | 306/2742 [07:53<2:22:38,  3.51s/it] 11%|█         | 307/2742 [07:54<1:50:45,  2.73s/it] 11%|█         | 308/2742 [07:55<1:30:44,  2.24s/it] 11%|█▏        | 309/2742 [07:56<1:15:42,  1.87s/it] 11%|█▏        | 310/2742 [07:57<1:05:16,  1.61s/it]                                                    {'loss': 1.5143, 'learning_rate': 4.843962887506382e-05, 'epoch': 0.34}
 11%|█▏        | 310/2742 [07:57<1:05:16,  1.61s/it] 11%|█▏        | 311/2742 [07:58<59:07,  1.46s/it]   11%|█▏        | 312/2742 [07:59<53:26,  1.32s/it] 11%|█▏        | 313/2742 [08:00<49:49,  1.23s/it] 11%|█▏        | 314/2742 [08:01<46:35,  1.15s/it] 11%|█▏        | 315/2742 [08:02<43:56,  1.09s/it]                                                  {'loss': 1.8286, 'learning_rate': 4.838944024071458e-05, 'epoch': 0.34}
 11%|█▏        | 315/2742 [08:02<43:56,  1.09s/it] 12%|█▏        | 316/2742 [08:03<42:24,  1.05s/it] 12%|█▏        | 317/2742 [08:04<40:41,  1.01s/it] 12%|█▏        | 318/2742 [08:05<42:00,  1.04s/it] 12%|█▏        | 319/2742 [08:06<41:15,  1.02s/it] 12%|█▏        | 320/2742 [08:07<40:01,  1.01it/s]                                                  {'loss': 1.3418, 'learning_rate': 4.833848402664192e-05, 'epoch': 0.35}
 12%|█▏        | 320/2742 [08:07<40:01,  1.01it/s] 12%|█▏        | 321/2742 [08:08<39:01,  1.03it/s] 12%|█▏        | 322/2742 [08:09<39:00,  1.03it/s] 12%|█▏        | 323/2742 [08:10<38:24,  1.05it/s] 12%|█▏        | 324/2742 [08:11<40:14,  1.00it/s] 12%|█▏        | 325/2742 [08:12<39:37,  1.02it/s]                                                  {'loss': 1.4772, 'learning_rate': 4.8286761905094325e-05, 'epoch': 0.36}
 12%|█▏        | 325/2742 [08:12<39:37,  1.02it/s] 12%|█▏        | 326/2742 [08:13<39:39,  1.02it/s] 12%|█▏        | 327/2742 [08:14<39:21,  1.02it/s] 12%|█▏        | 328/2742 [08:15<38:20,  1.05it/s] 12%|█▏        | 329/2742 [08:16<41:23,  1.03s/it] 12%|█▏        | 330/2742 [08:17<40:14,  1.00s/it]                                                  {'loss': 1.5225, 'learning_rate': 4.82342755734554e-05, 'epoch': 0.36}
 12%|█▏        | 330/2742 [08:17<40:14,  1.00s/it] 12%|█▏        | 331/2742 [08:18<38:49,  1.03it/s] 12%|█▏        | 332/2742 [08:19<38:05,  1.05it/s] 12%|█▏        | 333/2742 [08:20<37:38,  1.07it/s] 12%|█▏        | 334/2742 [08:21<41:17,  1.03s/it] 12%|█▏        | 335/2742 [08:22<39:58,  1.00it/s]                                                  {'loss': 1.4445, 'learning_rate': 4.818102675418808e-05, 'epoch': 0.37}
 12%|█▏        | 335/2742 [08:22<39:58,  1.00it/s] 12%|█▏        | 336/2742 [08:23<39:49,  1.01it/s] 12%|█▏        | 337/2742 [08:24<39:27,  1.02it/s] 12%|█▏        | 338/2742 [08:25<39:26,  1.02it/s] 12%|█▏        | 339/2742 [08:26<39:32,  1.01it/s] 12%|█▏        | 340/2742 [08:27<38:32,  1.04it/s]                                                  {'loss': 1.4207, 'learning_rate': 4.812701719477812e-05, 'epoch': 0.37}
 12%|█▏        | 340/2742 [08:27<38:32,  1.04it/s] 12%|█▏        | 341/2742 [08:28<38:50,  1.03it/s] 12%|█▏        | 342/2742 [08:29<38:23,  1.04it/s] 13%|█▎        | 343/2742 [08:29<37:44,  1.06it/s] 13%|█▎        | 344/2742 [08:30<37:45,  1.06it/s] 13%|█▎        | 345/2742 [08:31<37:42,  1.06it/s]                                                  {'loss': 1.6286, 'learning_rate': 4.8072248667676785e-05, 'epoch': 0.38}
 13%|█▎        | 345/2742 [08:31<37:42,  1.06it/s] 13%|█▎        | 346/2742 [08:32<37:26,  1.07it/s] 13%|█▎        | 347/2742 [08:33<37:51,  1.05it/s] 13%|█▎        | 348/2742 [08:34<39:06,  1.02it/s] 13%|█▎        | 349/2742 [08:35<38:47,  1.03it/s] 13%|█▎        | 350/2742 [08:36<38:01,  1.05it/s]                                                  {'loss': 1.778, 'learning_rate': 4.8016722970242635e-05, 'epoch': 0.38}
 13%|█▎        | 350/2742 [08:36<38:01,  1.05it/s] 13%|█▎        | 351/2742 [08:37<37:43,  1.06it/s] 13%|█▎        | 352/2742 [08:38<37:13,  1.07it/s] 13%|█▎        | 353/2742 [08:39<40:12,  1.01s/it] 13%|█▎        | 354/2742 [08:40<40:07,  1.01s/it] 13%|█▎        | 355/2742 [08:41<39:59,  1.01s/it]                                                  {'loss': 1.5351, 'learning_rate': 4.7960441924682587e-05, 'epoch': 0.39}
 13%|█▎        | 355/2742 [08:41<39:59,  1.01s/it] 13%|█▎        | 356/2742 [08:42<39:51,  1.00s/it] 13%|█▎        | 357/2742 [08:43<39:07,  1.02it/s] 13%|█▎        | 358/2742 [08:44<39:17,  1.01it/s] 13%|█▎        | 359/2742 [08:45<39:06,  1.02it/s] 13%|█▎        | 360/2742 [08:46<39:17,  1.01it/s]                                                  {'loss': 1.5678, 'learning_rate': 4.790340737799206e-05, 'epoch': 0.39}
 13%|█▎        | 360/2742 [08:46<39:17,  1.01it/s] 13%|█▎        | 361/2742 [08:47<38:30,  1.03it/s] 13%|█▎        | 362/2742 [08:48<37:46,  1.05it/s] 13%|█▎        | 363/2742 [08:49<38:51,  1.02it/s] 13%|█▎        | 364/2742 [08:50<39:06,  1.01it/s] 13%|█▎        | 365/2742 [08:51<38:16,  1.04it/s]                                                  {'loss': 1.5714, 'learning_rate': 4.784562120189443e-05, 'epoch': 0.4}
 13%|█▎        | 365/2742 [08:51<38:16,  1.04it/s] 13%|█▎        | 366/2742 [08:52<38:10,  1.04it/s] 13%|█▎        | 367/2742 [08:53<38:31,  1.03it/s] 13%|█▎        | 368/2742 [08:54<37:53,  1.04it/s] 13%|█▎        | 369/2742 [08:55<37:04,  1.07it/s] 13%|█▎        | 370/2742 [08:56<37:53,  1.04it/s]                                                  {'loss': 1.3447, 'learning_rate': 4.7787085292779534e-05, 'epoch': 0.4}
 13%|█▎        | 370/2742 [08:56<37:53,  1.04it/s] 14%|█▎        | 371/2742 [08:57<38:10,  1.03it/s] 14%|█▎        | 372/2742 [08:58<38:12,  1.03it/s] 14%|█▎        | 373/2742 [08:59<38:56,  1.01it/s] 14%|█▎        | 374/2742 [09:00<38:34,  1.02it/s] 14%|█▎        | 375/2742 [09:01<39:34,  1.00s/it]                                                  {'loss': 1.4373, 'learning_rate': 4.772780157164149e-05, 'epoch': 0.41}
 14%|█▎        | 375/2742 [09:01<39:34,  1.00s/it] 14%|█▎        | 376/2742 [09:02<38:44,  1.02it/s] 14%|█▎        | 377/2742 [09:03<39:34,  1.00s/it] 14%|█▍        | 378/2742 [09:04<38:26,  1.02it/s] 14%|█▍        | 379/2742 [09:05<38:28,  1.02it/s] 14%|█▍        | 380/2742 [09:05<37:50,  1.04it/s]                                                  {'loss': 1.3182, 'learning_rate': 4.766777198401562e-05, 'epoch': 0.42}
 14%|█▍        | 380/2742 [09:05<37:50,  1.04it/s] 14%|█▍        | 381/2742 [09:06<38:49,  1.01it/s] 14%|█▍        | 382/2742 [09:07<38:12,  1.03it/s] 14%|█▍        | 383/2742 [09:08<38:19,  1.03it/s] 14%|█▍        | 384/2742 [09:09<38:51,  1.01it/s] 14%|█▍        | 385/2742 [09:10<39:01,  1.01it/s]                                                  {'loss': 1.4338, 'learning_rate': 4.760699849991461e-05, 'epoch': 0.42}
 14%|█▍        | 385/2742 [09:10<39:01,  1.01it/s] 14%|█▍        | 386/2742 [09:11<37:50,  1.04it/s] 14%|█▍        | 387/2742 [09:13<41:00,  1.04s/it] 14%|█▍        | 388/2742 [09:14<40:10,  1.02s/it] 14%|█▍        | 389/2742 [09:15<39:43,  1.01s/it] 14%|█▍        | 390/2742 [09:15<38:48,  1.01it/s]                                                  {'loss': 1.4241, 'learning_rate': 4.75454831137639e-05, 'epoch': 0.43}
 14%|█▍        | 390/2742 [09:15<38:48,  1.01it/s] 14%|█▍        | 391/2742 [09:16<38:59,  1.01it/s] 14%|█▍        | 392/2742 [09:17<38:07,  1.03it/s] 14%|█▍        | 393/2742 [09:18<39:06,  1.00it/s] 14%|█▍        | 394/2742 [09:19<38:17,  1.02it/s] 14%|█▍        | 395/2742 [09:20<37:34,  1.04it/s]                                                  {'loss': 1.4051, 'learning_rate': 4.7483227844336164e-05, 'epoch': 0.43}
 14%|█▍        | 395/2742 [09:20<37:34,  1.04it/s] 14%|█▍        | 396/2742 [09:21<38:14,  1.02it/s] 14%|█▍        | 397/2742 [09:22<38:14,  1.02it/s] 15%|█▍        | 398/2742 [09:23<38:22,  1.02it/s] 15%|█▍        | 399/2742 [09:24<38:01,  1.03it/s] 15%|█▍        | 400/2742 [09:25<37:29,  1.04it/s]                                                  {'loss': 1.5728, 'learning_rate': 4.74202347346851e-05, 'epoch': 0.44}
 15%|█▍        | 400/2742 [09:25<37:29,  1.04it/s][INFO|trainer.py:2881] 2023-11-06 14:07:44,962 >> Saving model checkpoint to saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-400
[INFO|tokenization_utils_base.py:2428] 2023-11-06 14:07:45,008 >> tokenizer config file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-06 14:07:45,009 >> Special tokens file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-400/special_tokens_map.json
[2023-11-06 14:07:45,690] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is about to be saved!
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-11-06 14:07:52,780] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-400/global_step400/mp_rank_00_model_states.pt
[2023-11-06 14:07:52,780] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-400/global_step400/mp_rank_00_model_states.pt...
[2023-11-06 14:08:23,198] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-400/global_step400/mp_rank_00_model_states.pt.
[2023-11-06 14:08:23,774] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 14:08:23,848] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 14:08:23,849] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-11-06 14:08:23,849] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
 15%|█▍        | 401/2742 [10:14<9:53:38, 15.21s/it] 15%|█▍        | 402/2742 [10:15<7:06:57, 10.95s/it] 15%|█▍        | 403/2742 [10:16<5:09:32,  7.94s/it] 15%|█▍        | 404/2742 [10:16<3:47:04,  5.83s/it] 15%|█▍        | 405/2742 [10:18<2:53:03,  4.44s/it]                                                    {'loss': 1.6799, 'learning_rate': 4.735650585207839e-05, 'epoch': 0.44}
 15%|█▍        | 405/2742 [10:18<2:53:03,  4.44s/it] 15%|█▍        | 406/2742 [10:19<2:14:05,  3.44s/it] 15%|█▍        | 407/2742 [10:20<1:44:31,  2.69s/it] 15%|█▍        | 408/2742 [10:21<1:24:03,  2.16s/it] 15%|█▍        | 409/2742 [10:22<1:09:31,  1.79s/it] 15%|█▍        | 410/2742 [10:22<59:20,  1.53s/it]                                                    {'loss': 1.488, 'learning_rate': 4.7292043287929834e-05, 'epoch': 0.45}
 15%|█▍        | 410/2742 [10:22<59:20,  1.53s/it] 15%|█▍        | 411/2742 [10:23<53:10,  1.37s/it] 15%|█▌        | 412/2742 [10:25<49:33,  1.28s/it] 15%|█▌        | 413/2742 [10:26<47:44,  1.23s/it] 15%|█▌        | 414/2742 [10:27<45:49,  1.18s/it] 15%|█▌        | 415/2742 [10:28<46:06,  1.19s/it]                                                  {'loss': 1.4504, 'learning_rate': 4.7226849157730715e-05, 'epoch': 0.45}
 15%|█▌        | 415/2742 [10:28<46:06,  1.19s/it] 15%|█▌        | 416/2742 [10:29<43:57,  1.13s/it] 15%|█▌        | 417/2742 [10:30<42:24,  1.09s/it] 15%|█▌        | 418/2742 [10:31<40:49,  1.05s/it] 15%|█▌        | 419/2742 [10:32<39:06,  1.01s/it] 15%|█▌        | 420/2742 [10:33<37:55,  1.02it/s]                                                  {'loss': 1.5511, 'learning_rate': 4.71609256009804e-05, 'epoch': 0.46}
 15%|█▌        | 420/2742 [10:33<37:55,  1.02it/s] 15%|█▌        | 421/2742 [10:34<38:11,  1.01it/s] 15%|█▌        | 422/2742 [10:35<36:57,  1.05it/s] 15%|█▌        | 423/2742 [10:36<36:30,  1.06it/s] 15%|█▌        | 424/2742 [10:36<36:12,  1.07it/s] 15%|█▌        | 425/2742 [10:37<36:36,  1.05it/s]                                                  {'loss': 1.549, 'learning_rate': 4.70942747811161e-05, 'epoch': 0.46}
 15%|█▌        | 425/2742 [10:37<36:36,  1.05it/s] 16%|█▌        | 426/2742 [10:38<36:17,  1.06it/s] 16%|█▌        | 427/2742 [10:39<36:10,  1.07it/s] 16%|█▌        | 428/2742 [10:40<35:50,  1.08it/s] 16%|█▌        | 429/2742 [10:41<35:59,  1.07it/s] 16%|█▌        | 430/2742 [10:42<36:30,  1.06it/s]                                                  {'loss': 1.4613, 'learning_rate': 4.70268988854419e-05, 'epoch': 0.47}
 16%|█▌        | 430/2742 [10:42<36:30,  1.06it/s] 16%|█▌        | 431/2742 [10:43<36:42,  1.05it/s] 16%|█▌        | 432/2742 [10:44<36:33,  1.05it/s] 16%|█▌        | 433/2742 [10:45<37:41,  1.02it/s] 16%|█▌        | 434/2742 [10:46<39:56,  1.04s/it] 16%|█▌        | 435/2742 [10:47<40:29,  1.05s/it]                                                  {'loss': 1.556, 'learning_rate': 4.695880012505692e-05, 'epoch': 0.48}
 16%|█▌        | 435/2742 [10:47<40:29,  1.05s/it] 16%|█▌        | 436/2742 [10:48<39:06,  1.02s/it] 16%|█▌        | 437/2742 [10:49<37:52,  1.01it/s] 16%|█▌        | 438/2742 [10:50<36:47,  1.04it/s] 16%|█▌        | 439/2742 [10:51<36:11,  1.06it/s] 16%|█▌        | 440/2742 [10:52<37:41,  1.02it/s]                                                  {'loss': 1.3262, 'learning_rate': 4.688998073478284e-05, 'epoch': 0.48}
 16%|█▌        | 440/2742 [10:52<37:41,  1.02it/s] 16%|█▌        | 441/2742 [10:53<37:06,  1.03it/s] 16%|█▌        | 442/2742 [10:54<36:55,  1.04it/s] 16%|█▌        | 443/2742 [10:55<36:09,  1.06it/s] 16%|█▌        | 444/2742 [10:56<36:09,  1.06it/s] 16%|█▌        | 445/2742 [10:57<35:20,  1.08it/s]                                                  {'loss': 1.5117, 'learning_rate': 4.6820442973090496e-05, 'epoch': 0.49}
 16%|█▌        | 445/2742 [10:57<35:20,  1.08it/s] 16%|█▋        | 446/2742 [10:58<34:58,  1.09it/s] 16%|█▋        | 447/2742 [10:59<37:05,  1.03it/s] 16%|█▋        | 448/2742 [11:00<38:54,  1.02s/it] 16%|█▋        | 449/2742 [11:01<38:14,  1.00s/it] 16%|█▋        | 450/2742 [11:02<37:36,  1.02it/s]                                                  {'loss': 1.4855, 'learning_rate': 4.675018912202576e-05, 'epoch': 0.49}
 16%|█▋        | 450/2742 [11:02<37:36,  1.02it/s] 16%|█▋        | 451/2742 [11:03<37:15,  1.02it/s] 16%|█▋        | 452/2742 [11:04<37:26,  1.02it/s] 17%|█▋        | 453/2742 [11:05<38:59,  1.02s/it] 17%|█▋        | 454/2742 [11:06<37:56,  1.01it/s] 17%|█▋        | 455/2742 [11:07<36:50,  1.03it/s]                                                  {'loss': 1.3653, 'learning_rate': 4.667922148713469e-05, 'epoch': 0.5}
 17%|█▋        | 455/2742 [11:07<36:50,  1.03it/s] 17%|█▋        | 456/2742 [11:08<36:53,  1.03it/s] 17%|█▋        | 457/2742 [11:09<37:01,  1.03it/s] 17%|█▋        | 458/2742 [11:09<36:09,  1.05it/s] 17%|█▋        | 459/2742 [11:10<36:00,  1.06it/s] 17%|█▋        | 460/2742 [11:11<36:42,  1.04it/s]                                                  {'loss': 1.6645, 'learning_rate': 4.660754239738784e-05, 'epoch': 0.5}
 17%|█▋        | 460/2742 [11:11<36:42,  1.04it/s] 17%|█▋        | 461/2742 [11:12<35:58,  1.06it/s] 17%|█▋        | 462/2742 [11:13<36:08,  1.05it/s] 17%|█▋        | 463/2742 [11:14<36:00,  1.05it/s] 17%|█▋        | 464/2742 [11:15<37:13,  1.02it/s] 17%|█▋        | 465/2742 [11:16<37:01,  1.03it/s]                                                  {'loss': 1.6482, 'learning_rate': 4.6535154205103826e-05, 'epoch': 0.51}
 17%|█▋        | 465/2742 [11:16<37:01,  1.03it/s] 17%|█▋        | 466/2742 [11:17<36:53,  1.03it/s] 17%|█▋        | 467/2742 [11:18<36:09,  1.05it/s] 17%|█▋        | 468/2742 [11:19<36:36,  1.04it/s] 17%|█▋        | 469/2742 [11:20<35:54,  1.06it/s] 17%|█▋        | 470/2742 [11:21<36:55,  1.03it/s]                                                  {'loss': 1.4905, 'learning_rate': 4.646205928587215e-05, 'epoch': 0.51}
 17%|█▋        | 470/2742 [11:21<36:55,  1.03it/s] 17%|█▋        | 471/2742 [11:22<36:18,  1.04it/s] 17%|█▋        | 472/2742 [11:23<37:28,  1.01it/s] 17%|█▋        | 473/2742 [11:24<39:55,  1.06s/it] 17%|█▋        | 474/2742 [11:25<40:03,  1.06s/it] 17%|█▋        | 475/2742 [11:26<39:48,  1.05s/it]                                                  {'loss': 1.5621, 'learning_rate': 4.638826003847523e-05, 'epoch': 0.52}
 17%|█▋        | 475/2742 [11:26<39:48,  1.05s/it] 17%|█▋        | 476/2742 [11:27<38:32,  1.02s/it] 17%|█▋        | 477/2742 [11:28<38:09,  1.01s/it] 17%|█▋        | 478/2742 [11:29<37:53,  1.00s/it] 17%|█▋        | 479/2742 [11:30<37:22,  1.01it/s] 18%|█▊        | 480/2742 [11:31<36:49,  1.02it/s]                                                  {'loss': 1.4798, 'learning_rate': 4.631375888480968e-05, 'epoch': 0.52}
 18%|█▊        | 480/2742 [11:31<36:49,  1.02it/s] 18%|█▊        | 481/2742 [11:32<37:13,  1.01it/s] 18%|█▊        | 482/2742 [11:33<37:38,  1.00it/s] 18%|█▊        | 483/2742 [11:34<40:00,  1.06s/it] 18%|█▊        | 484/2742 [11:35<38:21,  1.02s/it] 18%|█▊        | 485/2742 [11:36<37:29,  1.00it/s]                                                  {'loss': 1.327, 'learning_rate': 4.623855826980679e-05, 'epoch': 0.53}
 18%|█▊        | 485/2742 [11:36<37:29,  1.00it/s] 18%|█▊        | 486/2742 [11:37<36:55,  1.02it/s] 18%|█▊        | 487/2742 [11:38<39:13,  1.04s/it] 18%|█▊        | 488/2742 [11:39<37:55,  1.01s/it] 18%|█▊        | 489/2742 [11:40<37:33,  1.00s/it] 18%|█▊        | 490/2742 [11:41<36:35,  1.03it/s]                                                  {'loss': 1.4011, 'learning_rate': 4.616266066135236e-05, 'epoch': 0.54}
 18%|█▊        | 490/2742 [11:41<36:35,  1.03it/s] 18%|█▊        | 491/2742 [11:42<36:28,  1.03it/s] 18%|█▊        | 492/2742 [11:43<37:00,  1.01it/s] 18%|█▊        | 493/2742 [11:44<38:04,  1.02s/it] 18%|█▊        | 494/2742 [11:45<36:48,  1.02it/s] 18%|█▊        | 495/2742 [11:46<37:06,  1.01it/s]                                                  {'loss': 1.3178, 'learning_rate': 4.608606855020569e-05, 'epoch': 0.54}
 18%|█▊        | 495/2742 [11:46<37:06,  1.01it/s] 18%|█▊        | 496/2742 [11:47<36:53,  1.01it/s] 18%|█▊        | 497/2742 [11:48<37:15,  1.00it/s] 18%|█▊        | 498/2742 [11:49<36:48,  1.02it/s] 18%|█▊        | 499/2742 [11:50<36:17,  1.03it/s] 18%|█▊        | 500/2742 [11:51<35:51,  1.04it/s]                                                  {'loss': 1.3868, 'learning_rate': 4.600878444991779e-05, 'epoch': 0.55}
 18%|█▊        | 500/2742 [11:51<35:51,  1.04it/s][INFO|trainer.py:2881] 2023-11-06 14:10:10,497 >> Saving model checkpoint to saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-500
[INFO|tokenization_utils_base.py:2428] 2023-11-06 14:10:10,541 >> tokenizer config file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-06 14:10:10,542 >> Special tokens file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-500/special_tokens_map.json
[2023-11-06 14:10:11,577] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step500 is about to be saved!
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-11-06 14:10:18,682] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-500/global_step500/mp_rank_00_model_states.pt
[2023-11-06 14:10:18,682] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-500/global_step500/mp_rank_00_model_states.pt...
[2023-11-06 14:10:50,072] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-500/global_step500/mp_rank_00_model_states.pt.
[2023-11-06 14:10:50,614] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 14:10:50,662] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 14:10:50,665] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-11-06 14:10:50,665] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step500 is ready now!
 18%|█▊        | 501/2742 [12:41<9:39:57, 15.53s/it] 18%|█▊        | 502/2742 [12:41<6:56:10, 11.15s/it] 18%|█▊        | 503/2742 [12:42<5:01:09,  8.07s/it] 18%|█▊        | 504/2742 [12:43<3:42:22,  5.96s/it] 18%|█▊        | 505/2742 [12:44<2:45:44,  4.45s/it]                                                    {'loss': 1.2832, 'learning_rate': 4.5930810896748934e-05, 'epoch': 0.55}
 18%|█▊        | 505/2742 [12:44<2:45:44,  4.45s/it] 18%|█▊        | 506/2742 [12:45<2:06:21,  3.39s/it] 18%|█▊        | 507/2742 [12:46<1:39:11,  2.66s/it] 19%|█▊        | 508/2742 [12:47<1:20:27,  2.16s/it] 19%|█▊        | 509/2742 [12:48<1:06:52,  1.80s/it] 19%|█▊        | 510/2742 [12:49<57:26,  1.54s/it]                                                    {'loss': 1.2945, 'learning_rate': 4.585215044958544e-05, 'epoch': 0.56}
 19%|█▊        | 510/2742 [12:49<57:26,  1.54s/it] 19%|█▊        | 511/2742 [12:50<51:01,  1.37s/it] 19%|█▊        | 512/2742 [12:51<46:12,  1.24s/it] 19%|█▊        | 513/2742 [12:52<43:00,  1.16s/it] 19%|█▊        | 514/2742 [12:53<40:10,  1.08s/it] 19%|█▉        | 515/2742 [12:54<41:34,  1.12s/it]                                                  {'loss': 1.5683, 'learning_rate': 4.577280568985567e-05, 'epoch': 0.56}
 19%|█▉        | 515/2742 [12:54<41:34,  1.12s/it] 19%|█▉        | 516/2742 [12:55<39:41,  1.07s/it] 19%|█▉        | 517/2742 [12:56<39:22,  1.06s/it] 19%|█▉        | 518/2742 [12:57<39:19,  1.06s/it] 19%|█▉        | 519/2742 [12:58<39:39,  1.07s/it][2023-11-06 14:11:10,444] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
 19%|█▉        | 520/2742 [12:59<39:18,  1.06s/it]                                                  {'loss': 1.6411, 'learning_rate': 4.570883892608292e-05, 'epoch': 0.57}
 19%|█▉        | 520/2742 [12:59<39:18,  1.06s/it] 19%|█▉        | 521/2742 [13:00<39:06,  1.06s/it] 19%|█▉        | 522/2742 [13:01<38:26,  1.04s/it] 19%|█▉        | 523/2742 [13:02<38:29,  1.04s/it] 19%|█▉        | 524/2742 [13:03<37:47,  1.02s/it] 19%|█▉        | 525/2742 [13:04<38:49,  1.05s/it]                                                  {'loss': 1.5526, 'learning_rate': 4.562826898056257e-05, 'epoch': 0.57}
 19%|█▉        | 525/2742 [13:04<38:49,  1.05s/it] 19%|█▉        | 526/2742 [13:06<39:00,  1.06s/it] 19%|█▉        | 527/2742 [13:07<38:13,  1.04s/it] 19%|█▉        | 528/2742 [13:08<38:37,  1.05s/it] 19%|█▉        | 529/2742 [13:09<38:00,  1.03s/it] 19%|█▉        | 530/2742 [13:10<37:08,  1.01s/it]                                                  {'loss': 1.5358, 'learning_rate': 4.554702206967512e-05, 'epoch': 0.58}
 19%|█▉        | 530/2742 [13:10<37:08,  1.01s/it] 19%|█▉        | 531/2742 [13:11<36:44,  1.00it/s] 19%|█▉        | 532/2742 [13:11<36:23,  1.01it/s] 19%|█▉        | 533/2742 [13:12<36:01,  1.02it/s] 19%|█▉        | 534/2742 [13:13<35:14,  1.04it/s] 20%|█▉        | 535/2742 [13:14<35:00,  1.05it/s]                                                  {'loss': 1.3752, 'learning_rate': 4.546510085972983e-05, 'epoch': 0.58}
 20%|█▉        | 535/2742 [13:14<35:00,  1.05it/s] 20%|█▉        | 536/2742 [13:15<35:39,  1.03it/s] 20%|█▉        | 537/2742 [13:16<34:54,  1.05it/s] 20%|█▉        | 538/2742 [13:17<34:22,  1.07it/s] 20%|█▉        | 539/2742 [13:18<34:57,  1.05it/s] 20%|█▉        | 540/2742 [13:19<34:49,  1.05it/s]                                                  {'loss': 1.3604, 'learning_rate': 4.538250803916469e-05, 'epoch': 0.59}
 20%|█▉        | 540/2742 [13:19<34:49,  1.05it/s] 20%|█▉        | 541/2742 [13:20<36:47,  1.00s/it] 20%|█▉        | 542/2742 [13:21<35:52,  1.02it/s] 20%|█▉        | 543/2742 [13:22<35:24,  1.04it/s] 20%|█▉        | 544/2742 [13:23<35:40,  1.03it/s] 20%|█▉        | 545/2742 [13:24<36:11,  1.01it/s]                                                  {'loss': 1.5185, 'learning_rate': 4.5299246318458185e-05, 'epoch': 0.6}
 20%|█▉        | 545/2742 [13:24<36:11,  1.01it/s] 20%|█▉        | 546/2742 [13:25<36:11,  1.01it/s] 20%|█▉        | 547/2742 [13:26<35:05,  1.04it/s] 20%|█▉        | 548/2742 [13:27<34:57,  1.05it/s] 20%|██        | 549/2742 [13:28<34:19,  1.07it/s] 20%|██        | 550/2742 [13:29<34:16,  1.07it/s]                                                  {'loss': 1.3688, 'learning_rate': 4.521531843004033e-05, 'epoch': 0.6}
 20%|██        | 550/2742 [13:29<34:16,  1.07it/s] 20%|██        | 551/2742 [13:30<34:05,  1.07it/s] 20%|██        | 552/2742 [13:31<34:12,  1.07it/s] 20%|██        | 553/2742 [13:31<33:59,  1.07it/s] 20%|██        | 554/2742 [13:33<35:50,  1.02it/s] 20%|██        | 555/2742 [13:34<35:40,  1.02it/s]                                                  {'loss': 1.6795, 'learning_rate': 4.513072712820301e-05, 'epoch': 0.61}
 20%|██        | 555/2742 [13:34<35:40,  1.02it/s] 20%|██        | 556/2742 [13:35<35:48,  1.02it/s] 20%|██        | 557/2742 [13:35<35:22,  1.03it/s] 20%|██        | 558/2742 [13:36<35:27,  1.03it/s] 20%|██        | 559/2742 [13:37<35:35,  1.02it/s] 20%|██        | 560/2742 [13:38<35:14,  1.03it/s]                                                  {'loss': 1.4429, 'learning_rate': 4.504547518900957e-05, 'epoch': 0.61}
 20%|██        | 560/2742 [13:38<35:14,  1.03it/s] 20%|██        | 561/2742 [13:39<36:35,  1.01s/it] 20%|██        | 562/2742 [13:40<36:14,  1.00it/s] 21%|██        | 563/2742 [13:41<35:41,  1.02it/s] 21%|██        | 564/2742 [13:42<35:55,  1.01it/s] 21%|██        | 565/2742 [13:44<42:22,  1.17s/it]                                                  {'loss': 1.4867, 'learning_rate': 4.495956541020376e-05, 'epoch': 0.62}
 21%|██        | 565/2742 [13:44<42:22,  1.17s/it] 21%|██        | 566/2742 [13:45<39:21,  1.09s/it] 21%|██        | 567/2742 [13:46<37:38,  1.04s/it] 21%|██        | 568/2742 [13:47<36:42,  1.01s/it] 21%|██        | 569/2742 [13:48<36:16,  1.00s/it] 21%|██        | 570/2742 [13:49<36:52,  1.02s/it]                                                  {'loss': 1.2782, 'learning_rate': 4.487300061111787e-05, 'epoch': 0.62}
 21%|██        | 570/2742 [13:49<36:52,  1.02s/it] 21%|██        | 571/2742 [13:50<36:17,  1.00s/it] 21%|██        | 572/2742 [13:51<40:07,  1.11s/it] 21%|██        | 573/2742 [13:52<38:29,  1.06s/it] 21%|██        | 574/2742 [13:53<37:47,  1.05s/it] 21%|██        | 575/2742 [13:54<36:43,  1.02s/it]                                                  {'loss': 1.29, 'learning_rate': 4.478578363258023e-05, 'epoch': 0.63}
 21%|██        | 575/2742 [13:54<36:43,  1.02s/it] 21%|██        | 576/2742 [13:55<38:11,  1.06s/it] 21%|██        | 577/2742 [13:56<37:15,  1.03s/it] 21%|██        | 578/2742 [13:57<36:03,  1.00it/s] 21%|██        | 579/2742 [13:58<34:53,  1.03it/s] 21%|██        | 580/2742 [13:59<34:58,  1.03it/s]                                                  {'loss': 1.4787, 'learning_rate': 4.469791733682199e-05, 'epoch': 0.63}
 21%|██        | 580/2742 [13:59<34:58,  1.03it/s] 21%|██        | 581/2742 [14:00<34:29,  1.04it/s] 21%|██        | 582/2742 [14:01<35:01,  1.03it/s] 21%|██▏       | 583/2742 [14:02<36:36,  1.02s/it] 21%|██▏       | 584/2742 [14:03<35:25,  1.02it/s] 21%|██▏       | 585/2742 [14:04<36:29,  1.02s/it]                                                  {'loss': 1.2848, 'learning_rate': 4.4609404607383174e-05, 'epoch': 0.64}
 21%|██▏       | 585/2742 [14:04<36:29,  1.02s/it] 21%|██▏       | 586/2742 [14:08<1:03:21,  1.76s/it] 21%|██▏       | 587/2742 [14:08<54:38,  1.52s/it]   21%|██▏       | 588/2742 [14:09<48:06,  1.34s/it] 21%|██▏       | 589/2742 [14:10<43:39,  1.22s/it] 22%|██▏       | 590/2742 [14:11<40:25,  1.13s/it]                                                  {'loss': 1.3627, 'learning_rate': 4.4520248349018034e-05, 'epoch': 0.64}
 22%|██▏       | 590/2742 [14:11<40:25,  1.13s/it] 22%|██▏       | 591/2742 [14:12<38:14,  1.07s/it] 22%|██▏       | 592/2742 [14:13<37:12,  1.04s/it] 22%|██▏       | 593/2742 [14:14<36:07,  1.01s/it] 22%|██▏       | 594/2742 [14:15<37:14,  1.04s/it] 22%|██▏       | 595/2742 [14:16<38:02,  1.06s/it]                                                  {'loss': 1.3766, 'learning_rate': 4.4430451487599775e-05, 'epoch': 0.65}
 22%|██▏       | 595/2742 [14:16<38:02,  1.06s/it] 22%|██▏       | 596/2742 [14:17<36:56,  1.03s/it] 22%|██▏       | 597/2742 [14:18<36:48,  1.03s/it] 22%|██▏       | 598/2742 [14:19<37:18,  1.04s/it] 22%|██▏       | 599/2742 [14:20<36:35,  1.02s/it] 22%|██▏       | 600/2742 [14:21<36:26,  1.02s/it]                                                  {'loss': 1.3227, 'learning_rate': 4.434001697002449e-05, 'epoch': 0.66}
 22%|██▏       | 600/2742 [14:21<36:26,  1.02s/it][INFO|trainer.py:2881] 2023-11-06 14:12:45,791 >> Saving model checkpoint to saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-600
[INFO|tokenization_utils_base.py:2428] 2023-11-06 14:12:45,845 >> tokenizer config file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-06 14:12:45,845 >> Special tokens file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-600/special_tokens_map.json
[2023-11-06 14:12:48,857] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step600 is about to be saved!
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-11-06 14:12:55,935] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-600/global_step600/mp_rank_00_model_states.pt
[2023-11-06 14:12:55,935] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-600/global_step600/mp_rank_00_model_states.pt...
[2023-11-06 14:13:30,080] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-600/global_step600/mp_rank_00_model_states.pt.
[2023-11-06 14:13:30,618] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-600/global_step600/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 14:13:30,780] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-600/global_step600/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 14:13:30,782] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-600/global_step600/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-11-06 14:13:30,782] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step600 is ready now!
 22%|██▏       | 601/2742 [15:21<11:01:07, 18.53s/it] 22%|██▏       | 602/2742 [15:22<7:52:14, 13.24s/it]  22%|██▏       | 603/2742 [15:23<5:40:41,  9.56s/it] 22%|██▏       | 604/2742 [15:24<4:09:18,  7.00s/it] 22%|██▏       | 605/2742 [15:25<3:04:31,  5.18s/it]                                                    {'loss': 1.538, 'learning_rate': 4.424894776411445e-05, 'epoch': 0.66}
 22%|██▏       | 605/2742 [15:25<3:04:31,  5.18s/it] 22%|██▏       | 606/2742 [15:26<2:20:09,  3.94s/it] 22%|██▏       | 607/2742 [15:27<1:48:37,  3.05s/it] 22%|██▏       | 608/2742 [15:28<1:26:34,  2.43s/it] 22%|██▏       | 609/2742 [15:29<1:11:45,  2.02s/it] 22%|██▏       | 610/2742 [15:30<1:01:06,  1.72s/it]                                                    {'loss': 1.446, 'learning_rate': 4.415724685852075e-05, 'epoch': 0.67}
 22%|██▏       | 610/2742 [15:30<1:01:06,  1.72s/it] 22%|██▏       | 611/2742 [15:31<53:10,  1.50s/it]   22%|██▏       | 612/2742 [15:32<48:46,  1.37s/it] 22%|██▏       | 613/2742 [15:33<45:05,  1.27s/it] 22%|██▏       | 614/2742 [15:34<42:34,  1.20s/it] 22%|██▏       | 615/2742 [15:35<39:42,  1.12s/it]                                                  {'loss': 1.4686, 'learning_rate': 4.406491726262519e-05, 'epoch': 0.67}
 22%|██▏       | 615/2742 [15:35<39:42,  1.12s/it] 22%|██▏       | 616/2742 [15:36<38:05,  1.07s/it] 23%|██▎       | 617/2742 [15:37<36:46,  1.04s/it] 23%|██▎       | 618/2742 [15:38<36:03,  1.02s/it] 23%|██▎       | 619/2742 [15:39<34:45,  1.02it/s] 23%|██▎       | 620/2742 [15:40<34:49,  1.02it/s]                                                  {'loss': 1.489, 'learning_rate': 4.3971962006441524e-05, 'epoch': 0.68}
 23%|██▎       | 620/2742 [15:40<34:49,  1.02it/s] 23%|██▎       | 621/2742 [15:41<34:54,  1.01it/s] 23%|██▎       | 622/2742 [15:41<33:41,  1.05it/s] 23%|██▎       | 623/2742 [15:42<33:17,  1.06it/s] 23%|██▎       | 624/2742 [15:43<33:25,  1.06it/s] 23%|██▎       | 625/2742 [15:44<35:12,  1.00it/s]                                                  {'loss': 1.3027, 'learning_rate': 4.387838414051603e-05, 'epoch': 0.68}
 23%|██▎       | 625/2742 [15:44<35:12,  1.00it/s] 23%|██▎       | 626/2742 [15:45<34:10,  1.03it/s] 23%|██▎       | 627/2742 [15:46<34:11,  1.03it/s] 23%|██▎       | 628/2742 [15:47<35:00,  1.01it/s] 23%|██▎       | 629/2742 [15:48<33:51,  1.04it/s] 23%|██▎       | 630/2742 [15:49<34:02,  1.03it/s]                                                  {'loss': 1.3423, 'learning_rate': 4.378418673582738e-05, 'epoch': 0.69}
 23%|██▎       | 630/2742 [15:49<34:02,  1.03it/s] 23%|██▎       | 631/2742 [15:50<34:50,  1.01it/s] 23%|██▎       | 632/2742 [15:51<34:35,  1.02it/s] 23%|██▎       | 633/2742 [15:52<36:47,  1.05s/it] 23%|██▎       | 634/2742 [15:53<35:34,  1.01s/it] 23%|██▎       | 635/2742 [15:54<34:50,  1.01it/s]                                                  {'loss': 1.3574, 'learning_rate': 4.36893728836859e-05, 'epoch': 0.69}
 23%|██▎       | 635/2742 [15:54<34:50,  1.01it/s] 23%|██▎       | 636/2742 [15:55<36:04,  1.03s/it] 23%|██▎       | 637/2742 [15:56<35:50,  1.02s/it] 23%|██▎       | 638/2742 [15:57<36:09,  1.03s/it] 23%|██▎       | 639/2742 [15:58<35:20,  1.01s/it] 23%|██▎       | 640/2742 [15:59<35:37,  1.02s/it]                                                  {'loss': 1.3903, 'learning_rate': 4.359394569563208e-05, 'epoch': 0.7}
 23%|██▎       | 640/2742 [15:59<35:37,  1.02s/it] 23%|██▎       | 641/2742 [16:00<34:50,  1.01it/s] 23%|██▎       | 642/2742 [16:01<34:44,  1.01it/s] 23%|██▎       | 643/2742 [16:02<33:37,  1.04it/s] 23%|██▎       | 644/2742 [16:03<34:33,  1.01it/s] 24%|██▎       | 645/2742 [16:05<37:45,  1.08s/it]                                                  {'loss': 1.3687, 'learning_rate': 4.349790830333448e-05, 'epoch': 0.71}
 24%|██▎       | 645/2742 [16:05<37:45,  1.08s/it] 24%|██▎       | 646/2742 [16:06<36:59,  1.06s/it] 24%|██▎       | 647/2742 [16:07<36:32,  1.05s/it] 24%|██▎       | 648/2742 [16:08<35:32,  1.02s/it] 24%|██▎       | 649/2742 [16:09<34:45,  1.00it/s] 24%|██▎       | 650/2742 [16:10<35:27,  1.02s/it]                                                  {'loss': 1.3413, 'learning_rate': 4.340126385848696e-05, 'epoch': 0.71}
 24%|██▎       | 650/2742 [16:10<35:27,  1.02s/it] 24%|██▎       | 651/2742 [16:10<34:22,  1.01it/s] 24%|██▍       | 652/2742 [16:11<33:43,  1.03it/s] 24%|██▍       | 653/2742 [16:12<33:08,  1.05it/s] 24%|██▍       | 654/2742 [16:13<33:23,  1.04it/s] 24%|██▍       | 655/2742 [16:14<33:42,  1.03it/s]                                                  {'loss': 1.4102, 'learning_rate': 4.330401553270522e-05, 'epoch': 0.72}
 24%|██▍       | 655/2742 [16:14<33:42,  1.03it/s] 24%|██▍       | 656/2742 [16:15<33:14,  1.05it/s] 24%|██▍       | 657/2742 [16:16<36:22,  1.05s/it] 24%|██▍       | 658/2742 [16:17<35:46,  1.03s/it] 24%|██▍       | 659/2742 [16:18<35:09,  1.01s/it] 24%|██▍       | 660/2742 [16:20<35:40,  1.03s/it]                                                  {'loss': 1.4866, 'learning_rate': 4.320616651742276e-05, 'epoch': 0.72}
 24%|██▍       | 660/2742 [16:20<35:40,  1.03s/it] 24%|██▍       | 661/2742 [16:21<35:23,  1.02s/it] 24%|██▍       | 662/2742 [16:21<34:17,  1.01it/s] 24%|██▍       | 663/2742 [16:22<34:04,  1.02it/s] 24%|██▍       | 664/2742 [16:23<33:22,  1.04it/s] 24%|██▍       | 665/2742 [16:24<33:04,  1.05it/s]                                                  {'loss': 1.4985, 'learning_rate': 4.3107720023786124e-05, 'epoch': 0.73}
 24%|██▍       | 665/2742 [16:24<33:04,  1.05it/s] 24%|██▍       | 666/2742 [16:25<34:09,  1.01it/s] 24%|██▍       | 667/2742 [16:26<35:02,  1.01s/it] 24%|██▍       | 668/2742 [16:27<34:34,  1.00s/it] 24%|██▍       | 669/2742 [16:28<34:42,  1.00s/it] 24%|██▍       | 670/2742 [16:29<35:23,  1.02s/it]                                                  {'loss': 1.501, 'learning_rate': 4.3008679282549506e-05, 'epoch': 0.73}
 24%|██▍       | 670/2742 [16:29<35:23,  1.02s/it] 24%|██▍       | 671/2742 [16:31<36:13,  1.05s/it] 25%|██▍       | 672/2742 [16:32<36:15,  1.05s/it] 25%|██▍       | 673/2742 [16:33<36:05,  1.05s/it] 25%|██▍       | 674/2742 [16:34<35:35,  1.03s/it] 25%|██▍       | 675/2742 [16:35<34:36,  1.00s/it]                                                  {'loss': 1.3412, 'learning_rate': 4.290904754396876e-05, 'epoch': 0.74}
 25%|██▍       | 675/2742 [16:35<34:36,  1.00s/it] 25%|██▍       | 676/2742 [16:36<34:17,  1.00it/s] 25%|██▍       | 677/2742 [16:37<34:05,  1.01it/s] 25%|██▍       | 678/2742 [16:38<34:17,  1.00it/s] 25%|██▍       | 679/2742 [16:39<34:32,  1.00s/it] 25%|██▍       | 680/2742 [16:39<33:40,  1.02it/s]                                                  {'loss': 1.5038, 'learning_rate': 4.28088280776947e-05, 'epoch': 0.74}
 25%|██▍       | 680/2742 [16:39<33:40,  1.02it/s] 25%|██▍       | 681/2742 [16:40<33:08,  1.04it/s] 25%|██▍       | 682/2742 [16:41<33:39,  1.02it/s] 25%|██▍       | 683/2742 [16:42<33:53,  1.01it/s] 25%|██▍       | 684/2742 [16:43<33:01,  1.04it/s] 25%|██▍       | 685/2742 [16:44<33:19,  1.03it/s]                                                  {'loss': 1.5844, 'learning_rate': 4.270802417266579e-05, 'epoch': 0.75}
 25%|██▍       | 685/2742 [16:44<33:19,  1.03it/s] 25%|██▌       | 686/2742 [16:46<35:48,  1.05s/it] 25%|██▌       | 687/2742 [16:46<34:26,  1.01s/it] 25%|██▌       | 688/2742 [16:48<36:34,  1.07s/it] 25%|██▌       | 689/2742 [16:49<36:55,  1.08s/it] 25%|██▌       | 690/2742 [16:50<36:50,  1.08s/it]                                                  {'loss': 1.5261, 'learning_rate': 4.2606639137000294e-05, 'epoch': 0.75}
 25%|██▌       | 690/2742 [16:50<36:50,  1.08s/it] 25%|██▌       | 691/2742 [16:51<35:04,  1.03s/it] 25%|██▌       | 692/2742 [16:52<34:07,  1.00it/s] 25%|██▌       | 693/2742 [16:53<34:07,  1.00it/s] 25%|██▌       | 694/2742 [16:54<33:45,  1.01it/s] 25%|██▌       | 695/2742 [16:55<32:56,  1.04it/s]                                                  {'loss': 1.3999, 'learning_rate': 4.250467629788758e-05, 'epoch': 0.76}
 25%|██▌       | 695/2742 [16:55<32:56,  1.04it/s] 25%|██▌       | 696/2742 [16:56<32:55,  1.04it/s] 25%|██▌       | 697/2742 [16:58<44:01,  1.29s/it] 25%|██▌       | 698/2742 [16:59<40:37,  1.19s/it] 25%|██▌       | 699/2742 [17:00<40:53,  1.20s/it] 26%|██▌       | 700/2742 [17:01<41:23,  1.22s/it]                                                  {'loss': 1.2794, 'learning_rate': 4.240213900147905e-05, 'epoch': 0.77}
 26%|██▌       | 700/2742 [17:01<41:23,  1.22s/it][INFO|trainer.py:2881] 2023-11-06 14:15:21,813 >> Saving model checkpoint to saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-700
[INFO|tokenization_utils_base.py:2428] 2023-11-06 14:15:21,869 >> tokenizer config file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-06 14:15:21,870 >> Special tokens file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-700/special_tokens_map.json
[2023-11-06 14:15:22,486] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step700 is about to be saved!
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-11-06 14:15:29,327] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-700/global_step700/mp_rank_00_model_states.pt
[2023-11-06 14:15:29,327] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-700/global_step700/mp_rank_00_model_states.pt...
[2023-11-06 14:15:58,423] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-700/global_step700/mp_rank_00_model_states.pt.
[2023-11-06 14:15:58,920] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-700/global_step700/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 14:15:58,974] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-700/global_step700/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 14:15:58,976] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-700/global_step700/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-11-06 14:15:58,976] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step700 is ready now!
 26%|██▌       | 701/2742 [17:49<8:36:08, 15.17s/it] 26%|██▌       | 702/2742 [17:50<6:13:45, 10.99s/it] 26%|██▌       | 703/2742 [17:51<4:34:13,  8.07s/it] 26%|██▌       | 704/2742 [17:52<3:22:27,  5.96s/it] 26%|██▌       | 705/2742 [17:53<2:30:54,  4.45s/it]                                                    {'loss': 1.2862, 'learning_rate': 4.229903061277824e-05, 'epoch': 0.77}
 26%|██▌       | 705/2742 [17:53<2:30:54,  4.45s/it] 26%|██▌       | 706/2742 [17:54<1:55:02,  3.39s/it] 26%|██▌       | 707/2742 [17:55<1:30:33,  2.67s/it] 26%|██▌       | 708/2742 [17:56<1:12:11,  2.13s/it] 26%|██▌       | 709/2742 [17:57<1:00:01,  1.77s/it] 26%|██▌       | 710/2742 [17:58<51:25,  1.52s/it]                                                    {'loss': 1.2622, 'learning_rate': 4.2195354515530476e-05, 'epoch': 0.78}
 26%|██▌       | 710/2742 [17:58<51:25,  1.52s/it] 26%|██▌       | 711/2742 [17:59<45:22,  1.34s/it] 26%|██▌       | 712/2742 [18:00<41:40,  1.23s/it] 26%|██▌       | 713/2742 [18:01<40:10,  1.19s/it] 26%|██▌       | 714/2742 [18:02<39:08,  1.16s/it] 26%|██▌       | 715/2742 [18:03<37:48,  1.12s/it]                                                  {'loss': 1.2426, 'learning_rate': 4.209111411211174e-05, 'epoch': 0.78}
 26%|██▌       | 715/2742 [18:03<37:48,  1.12s/it] 26%|██▌       | 716/2742 [18:04<37:12,  1.10s/it] 26%|██▌       | 717/2742 [18:05<36:16,  1.08s/it] 26%|██▌       | 718/2742 [18:06<34:58,  1.04s/it] 26%|██▌       | 719/2742 [18:07<35:36,  1.06s/it] 26%|██▋       | 720/2742 [18:08<35:15,  1.05s/it]                                                  {'loss': 1.3458, 'learning_rate': 4.198631282341708e-05, 'epoch': 0.79}
 26%|██▋       | 720/2742 [18:08<35:15,  1.05s/it] 26%|██▋       | 721/2742 [18:09<35:48,  1.06s/it] 26%|██▋       | 722/2742 [18:10<34:55,  1.04s/it] 26%|██▋       | 723/2742 [18:11<34:20,  1.02s/it] 26%|██▋       | 724/2742 [18:12<34:39,  1.03s/it] 26%|██▋       | 725/2742 [18:13<34:09,  1.02s/it]                                                  {'loss': 1.5601, 'learning_rate': 4.188095408874829e-05, 'epoch': 0.79}
 26%|██▋       | 725/2742 [18:13<34:09,  1.02s/it] 26%|██▋       | 726/2742 [18:14<35:09,  1.05s/it] 27%|██▋       | 727/2742 [18:15<33:46,  1.01s/it] 27%|██▋       | 728/2742 [18:16<32:37,  1.03it/s] 27%|██▋       | 729/2742 [18:17<33:02,  1.02it/s] 27%|██▋       | 730/2742 [18:18<33:11,  1.01it/s]                                                  {'loss': 1.512, 'learning_rate': 4.1775041365701106e-05, 'epoch': 0.8}
 27%|██▋       | 730/2742 [18:18<33:11,  1.01it/s] 27%|██▋       | 731/2742 [18:19<33:03,  1.01it/s] 27%|██▋       | 732/2742 [18:20<32:21,  1.04it/s] 27%|██▋       | 733/2742 [18:21<32:48,  1.02it/s] 27%|██▋       | 734/2742 [18:22<35:30,  1.06s/it] 27%|██▋       | 735/2742 [18:23<34:57,  1.05s/it]                                                  {'loss': 1.3045, 'learning_rate': 4.166857813005167e-05, 'epoch': 0.8}
 27%|██▋       | 735/2742 [18:23<34:57,  1.05s/it] 27%|██▋       | 736/2742 [18:24<33:51,  1.01s/it] 27%|██▋       | 737/2742 [18:25<32:49,  1.02it/s] 27%|██▋       | 738/2742 [18:26<31:59,  1.04it/s] 27%|██▋       | 739/2742 [18:27<32:10,  1.04it/s] 27%|██▋       | 740/2742 [18:28<31:54,  1.05it/s]                                                  {'loss': 1.4193, 'learning_rate': 4.156156787564252e-05, 'epoch': 0.81}
 27%|██▋       | 740/2742 [18:28<31:54,  1.05it/s] 27%|██▋       | 741/2742 [18:29<31:17,  1.07it/s] 27%|██▋       | 742/2742 [18:30<31:19,  1.06it/s] 27%|██▋       | 743/2742 [18:31<31:30,  1.06it/s] 27%|██▋       | 744/2742 [18:32<31:56,  1.04it/s] 27%|██▋       | 745/2742 [18:33<31:44,  1.05it/s]                                                  {'loss': 1.2498, 'learning_rate': 4.145401411426787e-05, 'epoch': 0.81}
 27%|██▋       | 745/2742 [18:33<31:44,  1.05it/s] 27%|██▋       | 746/2742 [18:34<31:12,  1.07it/s] 27%|██▋       | 747/2742 [18:35<30:50,  1.08it/s] 27%|██▋       | 748/2742 [18:35<30:42,  1.08it/s] 27%|██▋       | 749/2742 [18:36<30:28,  1.09it/s] 27%|██▋       | 750/2742 [18:37<31:46,  1.05it/s]                                                  {'loss': 1.5268, 'learning_rate': 4.134592037555846e-05, 'epoch': 0.82}
 27%|██▋       | 750/2742 [18:37<31:46,  1.05it/s] 27%|██▋       | 751/2742 [18:38<32:08,  1.03it/s] 27%|██▋       | 752/2742 [18:39<31:55,  1.04it/s] 27%|██▋       | 753/2742 [18:40<31:17,  1.06it/s] 27%|██▋       | 754/2742 [18:41<31:09,  1.06it/s] 28%|██▊       | 755/2742 [18:42<30:49,  1.07it/s]                                                  {'loss': 1.5032, 'learning_rate': 4.12372902068656e-05, 'epoch': 0.83}
 28%|██▊       | 755/2742 [18:42<30:49,  1.07it/s] 28%|██▊       | 756/2742 [18:43<32:57,  1.00it/s] 28%|██▊       | 757/2742 [18:44<32:24,  1.02it/s] 28%|██▊       | 758/2742 [18:45<33:43,  1.02s/it] 28%|██▊       | 759/2742 [18:46<32:25,  1.02it/s] 28%|██▊       | 760/2742 [18:47<32:02,  1.03it/s]                                                  {'loss': 1.2619, 'learning_rate': 4.112812717314484e-05, 'epoch': 0.83}
 28%|██▊       | 760/2742 [18:47<32:02,  1.03it/s] 28%|██▊       | 761/2742 [18:48<31:44,  1.04it/s] 28%|██▊       | 762/2742 [18:49<31:54,  1.03it/s] 28%|██▊       | 763/2742 [18:50<31:52,  1.03it/s] 28%|██▊       | 764/2742 [18:51<32:18,  1.02it/s] 28%|██▊       | 765/2742 [18:52<33:08,  1.01s/it]                                                  {'loss': 1.3255, 'learning_rate': 4.101843485683897e-05, 'epoch': 0.84}
 28%|██▊       | 765/2742 [18:52<33:08,  1.01s/it] 28%|██▊       | 766/2742 [18:53<32:53,  1.00it/s] 28%|██▊       | 767/2742 [18:54<32:35,  1.01it/s] 28%|██▊       | 768/2742 [18:55<32:14,  1.02it/s] 28%|██▊       | 769/2742 [18:56<31:40,  1.04it/s] 28%|██▊       | 770/2742 [18:57<31:36,  1.04it/s]                                                  {'loss': 1.6841, 'learning_rate': 4.0908216857760416e-05, 'epoch': 0.84}
 28%|██▊       | 770/2742 [18:57<31:36,  1.04it/s] 28%|██▊       | 771/2742 [18:58<31:55,  1.03it/s] 28%|██▊       | 772/2742 [18:59<32:04,  1.02it/s] 28%|██▊       | 773/2742 [19:00<31:19,  1.05it/s] 28%|██▊       | 774/2742 [19:01<31:27,  1.04it/s] 28%|██▊       | 775/2742 [19:02<31:04,  1.06it/s]                                                  {'loss': 1.362, 'learning_rate': 4.079747679297314e-05, 'epoch': 0.85}
 28%|██▊       | 775/2742 [19:02<31:04,  1.06it/s] 28%|██▊       | 776/2742 [19:03<31:52,  1.03it/s] 28%|██▊       | 777/2742 [19:04<32:50,  1.00s/it] 28%|██▊       | 778/2742 [19:05<31:58,  1.02it/s] 28%|██▊       | 779/2742 [19:06<31:35,  1.04it/s] 28%|██▊       | 780/2742 [19:07<31:06,  1.05it/s]                                                  {'loss': 1.3159, 'learning_rate': 4.068621829667391e-05, 'epoch': 0.85}
 28%|██▊       | 780/2742 [19:07<31:06,  1.05it/s] 28%|██▊       | 781/2742 [19:07<31:27,  1.04it/s] 29%|██▊       | 782/2742 [19:08<31:23,  1.04it/s] 29%|██▊       | 783/2742 [19:09<31:48,  1.03it/s] 29%|██▊       | 784/2742 [19:10<31:12,  1.05it/s] 29%|██▊       | 785/2742 [19:11<31:26,  1.04it/s]                                                  {'loss': 1.4937, 'learning_rate': 4.057444502007306e-05, 'epoch': 0.86}
 29%|██▊       | 785/2742 [19:11<31:26,  1.04it/s] 29%|██▊       | 786/2742 [19:12<31:27,  1.04it/s] 29%|██▊       | 787/2742 [19:13<31:21,  1.04it/s] 29%|██▊       | 788/2742 [19:14<30:54,  1.05it/s] 29%|██▉       | 789/2742 [19:15<31:19,  1.04it/s] 29%|██▉       | 790/2742 [19:16<32:11,  1.01it/s]                                                  {'loss': 1.2645, 'learning_rate': 4.046216063127466e-05, 'epoch': 0.86}
 29%|██▉       | 790/2742 [19:16<32:11,  1.01it/s] 29%|██▉       | 791/2742 [19:17<33:14,  1.02s/it] 29%|██▉       | 792/2742 [19:18<32:15,  1.01it/s] 29%|██▉       | 793/2742 [19:19<31:37,  1.03it/s] 29%|██▉       | 794/2742 [19:20<31:07,  1.04it/s] 29%|██▉       | 795/2742 [19:21<32:11,  1.01it/s]                                                  {'loss': 1.32, 'learning_rate': 4.0349368815156105e-05, 'epoch': 0.87}
 29%|██▉       | 795/2742 [19:21<32:11,  1.01it/s] 29%|██▉       | 796/2742 [19:22<32:08,  1.01it/s] 29%|██▉       | 797/2742 [19:23<32:23,  1.00it/s] 29%|██▉       | 798/2742 [19:24<31:57,  1.01it/s] 29%|██▉       | 799/2742 [19:25<31:45,  1.02it/s] 29%|██▉       | 800/2742 [19:26<31:52,  1.02it/s]                                                  {'loss': 1.6499, 'learning_rate': 4.023607327324726e-05, 'epoch': 0.87}
 29%|██▉       | 800/2742 [19:26<31:52,  1.02it/s][INFO|trainer.py:2881] 2023-11-06 14:17:45,292 >> Saving model checkpoint to saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-800
[INFO|tokenization_utils_base.py:2428] 2023-11-06 14:17:45,330 >> tokenizer config file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-06 14:17:45,330 >> Special tokens file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-800/special_tokens_map.json
[2023-11-06 14:17:46,350] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step800 is about to be saved!
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-11-06 14:17:53,115] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-800/global_step800/mp_rank_00_model_states.pt
[2023-11-06 14:17:53,116] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-800/global_step800/mp_rank_00_model_states.pt...
[2023-11-06 14:18:22,558] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-800/global_step800/mp_rank_00_model_states.pt.
[2023-11-06 14:18:23,064] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-800/global_step800/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 14:18:23,137] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-800/global_step800/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 14:18:23,138] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-800/global_step800/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-11-06 14:18:23,138] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step800 is ready now!
 29%|██▉       | 801/2742 [20:13<7:57:42, 14.77s/it] 29%|██▉       | 802/2742 [20:14<5:43:34, 10.63s/it] 29%|██▉       | 803/2742 [20:15<4:11:01,  7.77s/it] 29%|██▉       | 804/2742 [20:16<3:04:45,  5.72s/it] 29%|██▉       | 805/2742 [20:17<2:18:01,  4.28s/it]                                                    {'loss': 1.5161, 'learning_rate': 4.0122277723608894e-05, 'epoch': 0.88}
 29%|██▉       | 805/2742 [20:17<2:18:01,  4.28s/it] 29%|██▉       | 806/2742 [20:18<1:45:22,  3.27s/it] 29%|██▉       | 807/2742 [20:19<1:23:08,  2.58s/it] 29%|██▉       | 808/2742 [20:20<1:07:11,  2.08s/it] 30%|██▉       | 809/2742 [20:21<56:49,  1.76s/it]   30%|██▉       | 810/2742 [20:22<48:24,  1.50s/it]                                                  {'loss': 1.3911, 'learning_rate': 4.000798590071075e-05, 'epoch': 0.89}
 30%|██▉       | 810/2742 [20:22<48:24,  1.50s/it] 30%|██▉       | 811/2742 [20:23<43:14,  1.34s/it] 30%|██▉       | 812/2742 [20:24<39:40,  1.23s/it] 30%|██▉       | 813/2742 [20:25<37:33,  1.17s/it] 30%|██▉       | 814/2742 [20:26<37:29,  1.17s/it] 30%|██▉       | 815/2742 [20:27<36:05,  1.12s/it]                                                  {'loss': 1.3324, 'learning_rate': 3.9893201555308934e-05, 'epoch': 0.89}
 30%|██▉       | 815/2742 [20:27<36:05,  1.12s/it] 30%|██▉       | 816/2742 [20:28<35:14,  1.10s/it] 30%|██▉       | 817/2742 [20:29<34:10,  1.07s/it] 30%|██▉       | 818/2742 [20:30<32:37,  1.02s/it] 30%|██▉       | 819/2742 [20:31<32:02,  1.00it/s] 30%|██▉       | 820/2742 [20:32<31:49,  1.01it/s]                                                  {'loss': 1.4772, 'learning_rate': 3.977792845432283e-05, 'epoch': 0.9}
 30%|██▉       | 820/2742 [20:32<31:49,  1.01it/s] 30%|██▉       | 821/2742 [20:33<33:30,  1.05s/it] 30%|██▉       | 822/2742 [20:34<32:16,  1.01s/it] 30%|███       | 823/2742 [20:35<31:19,  1.02it/s][2023-11-06 14:18:46,936] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
 30%|███       | 824/2742 [20:36<32:06,  1.00s/it] 30%|███       | 825/2742 [20:37<31:30,  1.01it/s]                                                  {'loss': 1.6101, 'learning_rate': 3.9685360611126e-05, 'epoch': 0.9}
 30%|███       | 825/2742 [20:37<31:30,  1.01it/s] 30%|███       | 826/2742 [20:38<31:13,  1.02it/s] 30%|███       | 827/2742 [20:39<30:44,  1.04it/s] 30%|███       | 828/2742 [20:39<30:12,  1.06it/s] 30%|███       | 829/2742 [20:40<29:55,  1.07it/s] 30%|███       | 830/2742 [20:41<30:00,  1.06it/s]                                                  {'loss': 1.2465, 'learning_rate': 3.956921729384446e-05, 'epoch': 0.91}
 30%|███       | 830/2742 [20:41<30:00,  1.06it/s] 30%|███       | 831/2742 [20:42<30:31,  1.04it/s] 30%|███       | 832/2742 [20:43<30:25,  1.05it/s] 30%|███       | 833/2742 [20:44<30:13,  1.05it/s] 30%|███       | 834/2742 [20:46<33:28,  1.05s/it] 30%|███       | 835/2742 [20:46<32:11,  1.01s/it]                                                  {'loss': 1.3237, 'learning_rate': 3.945259585328737e-05, 'epoch': 0.91}
 30%|███       | 835/2742 [20:46<32:11,  1.01s/it] 30%|███       | 836/2742 [20:47<31:30,  1.01it/s] 31%|███       | 837/2742 [20:49<33:51,  1.07s/it] 31%|███       | 838/2742 [20:50<32:20,  1.02s/it] 31%|███       | 839/2742 [20:51<31:38,  1.00it/s] 31%|███       | 840/2742 [20:51<31:11,  1.02it/s]                                                  {'loss': 1.5802, 'learning_rate': 3.933550011666275e-05, 'epoch': 0.92}
 31%|███       | 840/2742 [20:51<31:11,  1.02it/s] 31%|███       | 841/2742 [20:52<30:16,  1.05it/s] 31%|███       | 842/2742 [20:53<29:36,  1.07it/s] 31%|███       | 843/2742 [20:54<29:23,  1.08it/s] 31%|███       | 844/2742 [20:55<30:09,  1.05it/s] 31%|███       | 845/2742 [20:56<30:03,  1.05it/s]                                                  {'loss': 1.3266, 'learning_rate': 3.921793392674376e-05, 'epoch': 0.92}
 31%|███       | 845/2742 [20:56<30:03,  1.05it/s] 31%|███       | 846/2742 [20:57<29:58,  1.05it/s] 31%|███       | 847/2742 [20:58<31:32,  1.00it/s] 31%|███       | 848/2742 [20:59<31:52,  1.01s/it] 31%|███       | 849/2742 [21:00<31:11,  1.01it/s] 31%|███       | 850/2742 [21:01<30:44,  1.03it/s]                                                  {'loss': 1.5488, 'learning_rate': 3.909990114174259e-05, 'epoch': 0.93}
 31%|███       | 850/2742 [21:01<30:44,  1.03it/s] 31%|███       | 851/2742 [21:02<30:54,  1.02it/s] 31%|███       | 852/2742 [21:03<30:42,  1.03it/s] 31%|███       | 853/2742 [21:04<29:55,  1.05it/s] 31%|███       | 854/2742 [21:05<29:41,  1.06it/s] 31%|███       | 855/2742 [21:06<29:24,  1.07it/s]                                                  {'loss': 1.2559, 'learning_rate': 3.8981405635183854e-05, 'epoch': 0.93}
 31%|███       | 855/2742 [21:06<29:24,  1.07it/s] 31%|███       | 856/2742 [21:07<30:53,  1.02it/s] 31%|███▏      | 857/2742 [21:08<30:19,  1.04it/s] 31%|███▏      | 858/2742 [21:09<29:58,  1.05it/s] 31%|███▏      | 859/2742 [21:10<29:49,  1.05it/s] 31%|███▏      | 860/2742 [21:11<29:40,  1.06it/s]                                                  {'loss': 1.4674, 'learning_rate': 3.886245129577747e-05, 'epoch': 0.94}
 31%|███▏      | 860/2742 [21:11<29:40,  1.06it/s] 31%|███▏      | 861/2742 [21:12<30:31,  1.03it/s] 31%|███▏      | 862/2742 [21:13<30:56,  1.01it/s] 31%|███▏      | 863/2742 [21:14<30:26,  1.03it/s] 32%|███▏      | 864/2742 [21:14<29:42,  1.05it/s] 32%|███▏      | 865/2742 [21:15<29:45,  1.05it/s]                                                  {'loss': 1.5379, 'learning_rate': 3.874304202729102e-05, 'epoch': 0.95}
 32%|███▏      | 865/2742 [21:15<29:45,  1.05it/s] 32%|███▏      | 866/2742 [21:16<29:31,  1.06it/s] 32%|███▏      | 867/2742 [21:17<29:44,  1.05it/s] 32%|███▏      | 868/2742 [21:18<29:21,  1.06it/s] 32%|███▏      | 869/2742 [21:19<29:45,  1.05it/s] 32%|███▏      | 870/2742 [21:20<29:19,  1.06it/s]                                                  {'loss': 1.5518, 'learning_rate': 3.86231817484217e-05, 'epoch': 0.95}
 32%|███▏      | 870/2742 [21:20<29:19,  1.06it/s] 32%|███▏      | 871/2742 [21:21<29:18,  1.06it/s] 32%|███▏      | 872/2742 [21:22<29:26,  1.06it/s] 32%|███▏      | 873/2742 [21:23<29:55,  1.04it/s] 32%|███▏      | 874/2742 [21:24<30:44,  1.01it/s] 32%|███▏      | 875/2742 [21:25<30:20,  1.03it/s]                                                  {'loss': 1.3499, 'learning_rate': 3.850287439266762e-05, 'epoch': 0.96}
 32%|███▏      | 875/2742 [21:25<30:20,  1.03it/s] 32%|███▏      | 876/2742 [21:26<30:46,  1.01it/s] 32%|███▏      | 877/2742 [21:27<31:10,  1.00s/it] 32%|███▏      | 878/2742 [21:28<30:30,  1.02it/s] 32%|███▏      | 879/2742 [21:29<30:47,  1.01it/s] 32%|███▏      | 880/2742 [21:30<30:28,  1.02it/s]                                                  {'loss': 1.4758, 'learning_rate': 3.838212390819884e-05, 'epoch': 0.96}
 32%|███▏      | 880/2742 [21:30<30:28,  1.02it/s] 32%|███▏      | 881/2742 [21:31<30:47,  1.01it/s] 32%|███▏      | 882/2742 [21:32<30:08,  1.03it/s] 32%|███▏      | 883/2742 [21:33<29:58,  1.03it/s] 32%|███▏      | 884/2742 [21:34<30:31,  1.01it/s] 32%|███▏      | 885/2742 [21:35<32:17,  1.04s/it]                                                  {'loss': 1.3252, 'learning_rate': 3.826093425772767e-05, 'epoch': 0.97}
 32%|███▏      | 885/2742 [21:35<32:17,  1.04s/it] 32%|███▏      | 886/2742 [21:36<30:49,  1.00it/s] 32%|███▏      | 887/2742 [21:37<30:45,  1.01it/s] 32%|███▏      | 888/2742 [21:38<29:42,  1.04it/s] 32%|███▏      | 889/2742 [21:39<30:11,  1.02it/s] 32%|███▏      | 890/2742 [21:40<32:26,  1.05s/it]                                                  {'loss': 1.2917, 'learning_rate': 3.813930941837874e-05, 'epoch': 0.97}
 32%|███▏      | 890/2742 [21:40<32:26,  1.05s/it] 32%|███▏      | 891/2742 [21:41<31:29,  1.02s/it] 33%|███▎      | 892/2742 [21:42<30:38,  1.01it/s] 33%|███▎      | 893/2742 [21:43<30:05,  1.02it/s] 33%|███▎      | 894/2742 [21:44<29:48,  1.03it/s] 33%|███▎      | 895/2742 [21:45<29:26,  1.05it/s]                                                  {'loss': 1.2572, 'learning_rate': 3.8017253381558396e-05, 'epoch': 0.98}
 33%|███▎      | 895/2742 [21:45<29:26,  1.05it/s] 33%|███▎      | 896/2742 [21:46<29:32,  1.04it/s] 33%|███▎      | 897/2742 [21:47<31:41,  1.03s/it] 33%|███▎      | 898/2742 [21:48<30:51,  1.00s/it] 33%|███▎      | 899/2742 [21:49<31:13,  1.02s/it] 33%|███▎      | 900/2742 [21:50<33:53,  1.10s/it]                                                  {'loss': 1.2289, 'learning_rate': 3.789477015282377e-05, 'epoch': 0.98}
 33%|███▎      | 900/2742 [21:50<33:53,  1.10s/it][INFO|trainer.py:2881] 2023-11-06 14:20:09,383 >> Saving model checkpoint to saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-900
[INFO|tokenization_utils_base.py:2428] 2023-11-06 14:20:09,424 >> tokenizer config file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-06 14:20:09,424 >> Special tokens file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-900/special_tokens_map.json
[2023-11-06 14:20:10,576] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step900 is about to be saved!
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-11-06 14:20:17,093] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-900/global_step900/mp_rank_00_model_states.pt
[2023-11-06 14:20:17,094] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-900/global_step900/mp_rank_00_model_states.pt...
[2023-11-06 14:20:47,032] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-900/global_step900/mp_rank_00_model_states.pt.
[2023-11-06 14:20:47,532] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-900/global_step900/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 14:20:47,582] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-900/global_step900/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 14:20:47,584] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-900/global_step900/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-11-06 14:20:47,584] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step900 is ready now!
 33%|███▎      | 901/2742 [22:37<7:37:40, 14.92s/it] 33%|███▎      | 902/2742 [22:38<5:28:40, 10.72s/it] 33%|███▎      | 903/2742 [22:39<3:58:30,  7.78s/it] 33%|███▎      | 904/2742 [22:40<2:56:31,  5.76s/it] 33%|███▎      | 905/2742 [22:41<2:11:57,  4.31s/it]                                                    {'loss': 1.4208, 'learning_rate': 3.77718637517513e-05, 'epoch': 0.99}
 33%|███▎      | 905/2742 [22:41<2:11:57,  4.31s/it] 33%|███▎      | 906/2742 [22:42<1:40:32,  3.29s/it] 33%|███▎      | 907/2742 [22:43<1:18:22,  2.56s/it] 33%|███▎      | 908/2742 [22:44<1:05:13,  2.13s/it] 33%|███▎      | 909/2742 [22:45<53:37,  1.76s/it]   33%|███▎      | 910/2742 [22:46<46:04,  1.51s/it]                                                  {'loss': 1.2704, 'learning_rate': 3.764853821180481e-05, 'epoch': 0.99}
 33%|███▎      | 910/2742 [22:46<46:04,  1.51s/it] 33%|███▎      | 911/2742 [22:47<40:31,  1.33s/it] 33%|███▎      | 912/2742 [22:48<37:22,  1.23s/it] 33%|███▎      | 913/2742 [22:49<34:27,  1.13s/it] 33%|███▎      | 914/2742 [22:50<33:23,  1.10s/it] 33%|███▎      | 915/2742 [22:51<33:05,  1.09s/it]                                                  {'loss': 1.4871, 'learning_rate': 3.7524797580203185e-05, 'epoch': 1.0}
 33%|███▎      | 915/2742 [22:51<33:05,  1.09s/it] 33%|███▎      | 916/2742 [22:52<32:17,  1.06s/it] 33%|███▎      | 917/2742 [22:53<32:11,  1.06s/it] 33%|███▎      | 918/2742 [22:54<31:57,  1.05s/it] 34%|███▎      | 919/2742 [22:55<31:41,  1.04s/it] 34%|███▎      | 920/2742 [22:56<31:12,  1.03s/it]                                                  {'loss': 1.5789, 'learning_rate': 3.740064591778749e-05, 'epoch': 1.01}
 34%|███▎      | 920/2742 [22:56<31:12,  1.03s/it] 34%|███▎      | 921/2742 [22:57<30:42,  1.01s/it] 34%|███▎      | 922/2742 [22:58<29:55,  1.01it/s] 34%|███▎      | 923/2742 [22:59<30:49,  1.02s/it] 34%|███▎      | 924/2742 [23:00<30:29,  1.01s/it] 34%|███▎      | 925/2742 [23:01<29:27,  1.03it/s]                                                  {'loss': 1.3313, 'learning_rate': 3.727608729888776e-05, 'epoch': 1.01}
 34%|███▎      | 925/2742 [23:01<29:27,  1.03it/s] 34%|███▍      | 926/2742 [23:02<28:58,  1.04it/s] 34%|███▍      | 927/2742 [23:03<29:08,  1.04it/s] 34%|███▍      | 928/2742 [23:04<29:03,  1.04it/s] 34%|███▍      | 929/2742 [23:05<29:27,  1.03it/s] 34%|███▍      | 930/2742 [23:06<28:51,  1.05it/s]                                                  {'loss': 1.4873, 'learning_rate': 3.7151125811189256e-05, 'epoch': 1.02}
 34%|███▍      | 930/2742 [23:06<28:51,  1.05it/s] 34%|███▍      | 931/2742 [23:07<28:55,  1.04it/s] 34%|███▍      | 932/2742 [23:07<29:02,  1.04it/s] 34%|███▍      | 933/2742 [23:08<28:39,  1.05it/s] 34%|███▍      | 934/2742 [23:09<28:37,  1.05it/s] 34%|███▍      | 935/2742 [23:10<28:03,  1.07it/s]                                                  {'loss': 1.3827, 'learning_rate': 3.702576555559834e-05, 'epoch': 1.02}
 34%|███▍      | 935/2742 [23:10<28:03,  1.07it/s] 34%|███▍      | 936/2742 [23:11<29:12,  1.03it/s] 34%|███▍      | 937/2742 [23:12<30:01,  1.00it/s] 34%|███▍      | 938/2742 [23:13<29:09,  1.03it/s] 34%|███▍      | 939/2742 [23:14<28:58,  1.04it/s] 34%|███▍      | 940/2742 [23:15<28:40,  1.05it/s]                                                  {'loss': 1.3358, 'learning_rate': 3.690001064610788e-05, 'epoch': 1.03}
 34%|███▍      | 940/2742 [23:15<28:40,  1.05it/s] 34%|███▍      | 941/2742 [23:16<29:58,  1.00it/s] 34%|███▍      | 942/2742 [23:17<29:15,  1.03it/s] 34%|███▍      | 943/2742 [23:18<28:44,  1.04it/s] 34%|███▍      | 944/2742 [23:19<28:37,  1.05it/s] 34%|███▍      | 945/2742 [23:20<31:04,  1.04s/it]                                                  {'loss': 1.5102, 'learning_rate': 3.6773865209662245e-05, 'epoch': 1.03}
 34%|███▍      | 945/2742 [23:20<31:04,  1.04s/it] 35%|███▍      | 946/2742 [23:21<30:23,  1.02s/it] 35%|███▍      | 947/2742 [23:22<30:07,  1.01s/it] 35%|███▍      | 948/2742 [23:23<30:25,  1.02s/it] 35%|███▍      | 949/2742 [23:24<29:33,  1.01it/s] 35%|███▍      | 950/2742 [23:25<29:11,  1.02it/s]                                                  {'loss': 1.1828, 'learning_rate': 3.6647333386021856e-05, 'epoch': 1.04}
 35%|███▍      | 950/2742 [23:25<29:11,  1.02it/s] 35%|███▍      | 951/2742 [23:26<29:14,  1.02it/s] 35%|███▍      | 952/2742 [23:27<28:45,  1.04it/s] 35%|███▍      | 953/2742 [23:28<29:38,  1.01it/s] 35%|███▍      | 954/2742 [23:29<29:35,  1.01it/s] 35%|███▍      | 955/2742 [23:30<28:47,  1.03it/s]                                                  {'loss': 1.3437, 'learning_rate': 3.652041932762735e-05, 'epoch': 1.04}
 35%|███▍      | 955/2742 [23:30<28:47,  1.03it/s] 35%|███▍      | 956/2742 [23:31<29:19,  1.02it/s] 35%|███▍      | 957/2742 [23:32<29:07,  1.02it/s] 35%|███▍      | 958/2742 [23:33<28:35,  1.04it/s] 35%|███▍      | 959/2742 [23:34<28:21,  1.05it/s] 35%|███▌      | 960/2742 [23:35<29:05,  1.02it/s]                                                  {'loss': 1.3055, 'learning_rate': 3.6393127199463326e-05, 'epoch': 1.05}
 35%|███▌      | 960/2742 [23:35<29:05,  1.02it/s] 35%|███▌      | 961/2742 [23:36<28:58,  1.02it/s] 35%|███▌      | 962/2742 [23:37<29:05,  1.02it/s] 35%|███▌      | 963/2742 [23:38<30:32,  1.03s/it] 35%|███▌      | 964/2742 [23:39<29:27,  1.01it/s] 35%|███▌      | 965/2742 [23:40<31:01,  1.05s/it]                                                  {'loss': 1.3818, 'learning_rate': 3.626546117892159e-05, 'epoch': 1.05}
 35%|███▌      | 965/2742 [23:40<31:01,  1.05s/it] 35%|███▌      | 966/2742 [23:41<30:24,  1.03s/it] 35%|███▌      | 967/2742 [23:42<31:38,  1.07s/it] 35%|███▌      | 968/2742 [23:43<30:49,  1.04s/it] 35%|███▌      | 969/2742 [23:44<31:20,  1.06s/it] 35%|███▌      | 970/2742 [23:45<31:12,  1.06s/it]                                                  {'loss': 1.2668, 'learning_rate': 3.613742545566414e-05, 'epoch': 1.06}
 35%|███▌      | 970/2742 [23:45<31:12,  1.06s/it] 35%|███▌      | 971/2742 [23:46<29:49,  1.01s/it] 35%|███▌      | 972/2742 [23:47<29:33,  1.00s/it] 35%|███▌      | 973/2742 [23:48<29:04,  1.01it/s] 36%|███▌      | 974/2742 [23:49<29:12,  1.01it/s] 36%|███▌      | 975/2742 [23:50<28:42,  1.03it/s]                                                  {'loss': 1.4529, 'learning_rate': 3.6009024231485633e-05, 'epoch': 1.07}
 36%|███▌      | 975/2742 [23:50<28:42,  1.03it/s] 36%|███▌      | 976/2742 [23:51<29:02,  1.01it/s] 36%|███▌      | 977/2742 [23:52<28:50,  1.02it/s] 36%|███▌      | 978/2742 [23:53<28:25,  1.03it/s] 36%|███▌      | 979/2742 [23:54<29:15,  1.00it/s] 36%|███▌      | 980/2742 [23:55<29:20,  1.00it/s]                                                  {'loss': 1.3721, 'learning_rate': 3.58802617201755e-05, 'epoch': 1.07}
 36%|███▌      | 980/2742 [23:55<29:20,  1.00it/s] 36%|███▌      | 981/2742 [23:56<28:53,  1.02it/s] 36%|███▌      | 982/2742 [23:57<29:31,  1.01s/it] 36%|███▌      | 983/2742 [23:58<29:08,  1.01it/s] 36%|███▌      | 984/2742 [23:59<30:02,  1.03s/it] 36%|███▌      | 985/2742 [24:00<30:56,  1.06s/it]                                                  {'loss': 1.2891, 'learning_rate': 3.575114214737967e-05, 'epoch': 1.08}
 36%|███▌      | 985/2742 [24:00<30:56,  1.06s/it] 36%|███▌      | 986/2742 [24:01<30:13,  1.03s/it] 36%|███▌      | 987/2742 [24:02<29:18,  1.00s/it] 36%|███▌      | 988/2742 [24:03<29:24,  1.01s/it] 36%|███▌      | 989/2742 [24:04<28:40,  1.02it/s] 36%|███▌      | 990/2742 [24:05<28:46,  1.02it/s]                                                  {'loss': 1.0946, 'learning_rate': 3.562166975046188e-05, 'epoch': 1.08}
 36%|███▌      | 990/2742 [24:05<28:46,  1.02it/s] 36%|███▌      | 991/2742 [24:06<29:43,  1.02s/it] 36%|███▌      | 992/2742 [24:07<29:13,  1.00s/it] 36%|███▌      | 993/2742 [24:08<29:28,  1.01s/it] 36%|███▋      | 994/2742 [24:09<28:56,  1.01it/s] 36%|███▋      | 995/2742 [24:10<28:32,  1.02it/s]                                                  {'loss': 1.4971, 'learning_rate': 3.5491848778364626e-05, 'epoch': 1.09}
 36%|███▋      | 995/2742 [24:10<28:32,  1.02it/s] 36%|███▋      | 996/2742 [24:11<28:00,  1.04it/s] 36%|███▋      | 997/2742 [24:12<27:53,  1.04it/s] 36%|███▋      | 998/2742 [24:13<27:50,  1.04it/s] 36%|███▋      | 999/2742 [24:14<27:42,  1.05it/s] 36%|███▋      | 1000/2742 [24:15<28:09,  1.03it/s]                                                   {'loss': 1.2483, 'learning_rate': 3.536168349146973e-05, 'epoch': 1.09}
 36%|███▋      | 1000/2742 [24:15<28:09,  1.03it/s][INFO|trainer.py:2881] 2023-11-06 14:22:34,371 >> Saving model checkpoint to saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1000
[INFO|tokenization_utils_base.py:2428] 2023-11-06 14:22:34,421 >> tokenizer config file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-06 14:22:34,421 >> Special tokens file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1000/special_tokens_map.json
[2023-11-06 14:22:35,231] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1000 is about to be saved!
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-11-06 14:22:41,815] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt
[2023-11-06 14:22:41,815] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt...
[2023-11-06 14:23:13,960] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt.
[2023-11-06 14:23:14,467] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 14:23:14,543] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 14:23:14,546] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-11-06 14:23:14,546] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
 37%|███▋      | 1001/2742 [25:04<7:30:03, 15.51s/it] 37%|███▋      | 1002/2742 [25:05<5:23:22, 11.15s/it] 37%|███▋      | 1003/2742 [25:06<3:54:06,  8.08s/it] 37%|███▋      | 1004/2742 [25:07<2:52:10,  5.94s/it] 37%|███▋      | 1005/2742 [25:08<2:08:57,  4.45s/it]                                                     {'loss': 1.1742, 'learning_rate': 3.52311781614585e-05, 'epoch': 1.1}
 37%|███▋      | 1005/2742 [25:08<2:08:57,  4.45s/it] 37%|███▋      | 1006/2742 [25:09<1:38:22,  3.40s/it] 37%|███▋      | 1007/2742 [25:10<1:16:25,  2.64s/it] 37%|███▋      | 1008/2742 [25:11<1:01:19,  2.12s/it] 37%|███▋      | 1009/2742 [25:12<51:09,  1.77s/it]   37%|███▋      | 1010/2742 [25:13<44:03,  1.53s/it]                                                   {'loss': 1.2603, 'learning_rate': 3.510033707117157e-05, 'epoch': 1.1}
 37%|███▋      | 1010/2742 [25:13<44:03,  1.53s/it] 37%|███▋      | 1011/2742 [25:14<38:26,  1.33s/it] 37%|███▋      | 1012/2742 [25:15<34:55,  1.21s/it] 37%|███▋      | 1013/2742 [25:16<32:28,  1.13s/it] 37%|███▋      | 1014/2742 [25:17<31:42,  1.10s/it] 37%|███▋      | 1015/2742 [25:18<30:43,  1.07s/it]                                                   {'loss': 1.293, 'learning_rate': 3.496916451446836e-05, 'epoch': 1.11}
 37%|███▋      | 1015/2742 [25:18<30:43,  1.07s/it] 37%|███▋      | 1016/2742 [25:19<30:37,  1.06s/it] 37%|███▋      | 1017/2742 [25:20<30:14,  1.05s/it] 37%|███▋      | 1018/2742 [25:21<29:45,  1.04s/it] 37%|███▋      | 1019/2742 [25:22<28:27,  1.01it/s] 37%|███▋      | 1020/2742 [25:23<28:20,  1.01it/s]                                                   {'loss': 1.4161, 'learning_rate': 3.483766479608611e-05, 'epoch': 1.12}
 37%|███▋      | 1020/2742 [25:23<28:20,  1.01it/s] 37%|███▋      | 1021/2742 [25:24<29:40,  1.03s/it] 37%|███▋      | 1022/2742 [25:25<28:50,  1.01s/it] 37%|███▋      | 1023/2742 [25:26<29:41,  1.04s/it] 37%|███▋      | 1024/2742 [25:27<28:43,  1.00s/it] 37%|███▋      | 1025/2742 [25:28<28:09,  1.02it/s]                                                   {'loss': 1.3977, 'learning_rate': 3.470584223149867e-05, 'epoch': 1.12}
 37%|███▋      | 1025/2742 [25:28<28:09,  1.02it/s] 37%|███▋      | 1026/2742 [25:29<27:51,  1.03it/s] 37%|███▋      | 1027/2742 [25:29<27:11,  1.05it/s] 37%|███▋      | 1028/2742 [25:30<26:34,  1.08it/s] 38%|███▊      | 1029/2742 [25:31<26:22,  1.08it/s] 38%|███▊      | 1030/2742 [25:32<26:29,  1.08it/s]                                                   {'loss': 1.4242, 'learning_rate': 3.457370114677482e-05, 'epoch': 1.13}
 38%|███▊      | 1030/2742 [25:32<26:29,  1.08it/s] 38%|███▊      | 1031/2742 [25:33<26:53,  1.06it/s] 38%|███▊      | 1032/2742 [25:34<27:06,  1.05it/s] 38%|███▊      | 1033/2742 [25:35<27:19,  1.04it/s] 38%|███▊      | 1034/2742 [25:36<27:06,  1.05it/s] 38%|███▊      | 1035/2742 [25:37<26:24,  1.08it/s]                                                   {'loss': 1.3181, 'learning_rate': 3.444124587843637e-05, 'epoch': 1.13}
 38%|███▊      | 1035/2742 [25:37<26:24,  1.08it/s] 38%|███▊      | 1036/2742 [25:38<26:14,  1.08it/s] 38%|███▊      | 1037/2742 [25:39<26:54,  1.06it/s] 38%|███▊      | 1038/2742 [25:40<27:09,  1.05it/s] 38%|███▊      | 1039/2742 [25:41<26:33,  1.07it/s] 38%|███▊      | 1040/2742 [25:42<27:58,  1.01it/s]                                                   {'loss': 1.2942, 'learning_rate': 3.430848077331579e-05, 'epoch': 1.14}
 38%|███▊      | 1040/2742 [25:42<27:58,  1.01it/s] 38%|███▊      | 1041/2742 [25:43<29:49,  1.05s/it] 38%|███▊      | 1042/2742 [25:44<28:35,  1.01s/it] 38%|███▊      | 1043/2742 [25:45<29:50,  1.05s/it] 38%|███▊      | 1044/2742 [25:46<28:53,  1.02s/it] 38%|███▊      | 1045/2742 [25:47<28:04,  1.01it/s]                                                   {'loss': 1.3036, 'learning_rate': 3.417541018841355e-05, 'epoch': 1.14}
 38%|███▊      | 1045/2742 [25:47<28:04,  1.01it/s] 38%|███▊      | 1046/2742 [25:48<27:59,  1.01it/s] 38%|███▊      | 1047/2742 [25:49<29:23,  1.04s/it] 38%|███▊      | 1048/2742 [25:51<33:02,  1.17s/it] 38%|███▊      | 1049/2742 [25:52<31:02,  1.10s/it] 38%|███▊      | 1050/2742 [25:53<31:14,  1.11s/it]                                                   {'loss': 1.3141, 'learning_rate': 3.4042038490755204e-05, 'epoch': 1.15}
 38%|███▊      | 1050/2742 [25:53<31:14,  1.11s/it] 38%|███▊      | 1051/2742 [25:54<29:32,  1.05s/it] 38%|███▊      | 1052/2742 [25:54<28:22,  1.01s/it] 38%|███▊      | 1053/2742 [25:56<29:43,  1.06s/it] 38%|███▊      | 1054/2742 [25:57<30:36,  1.09s/it] 38%|███▊      | 1055/2742 [25:58<29:16,  1.04s/it]                                                   {'loss': 1.3337, 'learning_rate': 3.390837005724801e-05, 'epoch': 1.15}
 38%|███▊      | 1055/2742 [25:58<29:16,  1.04s/it] 39%|███▊      | 1056/2742 [25:59<29:11,  1.04s/it] 39%|███▊      | 1057/2742 [26:00<29:14,  1.04s/it] 39%|███▊      | 1058/2742 [26:01<28:05,  1.00s/it] 39%|███▊      | 1059/2742 [26:02<28:33,  1.02s/it] 39%|███▊      | 1060/2742 [26:03<29:13,  1.04s/it]                                                   {'loss': 1.3772, 'learning_rate': 3.377440927453731e-05, 'epoch': 1.16}
 39%|███▊      | 1060/2742 [26:03<29:13,  1.04s/it] 39%|███▊      | 1061/2742 [26:04<28:34,  1.02s/it] 39%|███▊      | 1062/2742 [26:05<27:39,  1.01it/s] 39%|███▉      | 1063/2742 [26:06<27:41,  1.01it/s] 39%|███▉      | 1064/2742 [26:07<27:38,  1.01it/s] 39%|███▉      | 1065/2742 [26:08<27:25,  1.02it/s]                                                   {'loss': 1.4494, 'learning_rate': 3.3640160538862575e-05, 'epoch': 1.16}
 39%|███▉      | 1065/2742 [26:08<27:25,  1.02it/s] 39%|███▉      | 1066/2742 [26:09<28:16,  1.01s/it] 39%|███▉      | 1067/2742 [26:10<27:45,  1.01it/s] 39%|███▉      | 1068/2742 [26:11<27:40,  1.01it/s] 39%|███▉      | 1069/2742 [26:12<28:53,  1.04s/it] 39%|███▉      | 1070/2742 [26:13<27:38,  1.01it/s]                                                   {'loss': 1.2885, 'learning_rate': 3.350562825591316e-05, 'epoch': 1.17}
 39%|███▉      | 1070/2742 [26:13<27:38,  1.01it/s] 39%|███▉      | 1071/2742 [26:14<27:09,  1.03it/s] 39%|███▉      | 1072/2742 [26:15<26:59,  1.03it/s] 39%|███▉      | 1073/2742 [26:16<26:42,  1.04it/s] 39%|███▉      | 1074/2742 [26:16<26:14,  1.06it/s] 39%|███▉      | 1075/2742 [26:18<27:08,  1.02it/s]                                                   {'loss': 1.3372, 'learning_rate': 3.3370816840683656e-05, 'epoch': 1.18}
 39%|███▉      | 1075/2742 [26:18<27:08,  1.02it/s] 39%|███▉      | 1076/2742 [26:18<26:50,  1.03it/s] 39%|███▉      | 1077/2742 [26:19<26:29,  1.05it/s] 39%|███▉      | 1078/2742 [26:21<27:49,  1.00s/it] 39%|███▉      | 1079/2742 [26:22<27:54,  1.01s/it] 39%|███▉      | 1080/2742 [26:23<27:57,  1.01s/it]                                                   {'loss': 1.4114, 'learning_rate': 3.323573071732907e-05, 'epoch': 1.18}
 39%|███▉      | 1080/2742 [26:23<27:57,  1.01s/it] 39%|███▉      | 1081/2742 [26:24<28:02,  1.01s/it] 39%|███▉      | 1082/2742 [26:25<27:57,  1.01s/it] 39%|███▉      | 1083/2742 [26:25<26:55,  1.03it/s] 40%|███▉      | 1084/2742 [26:26<26:45,  1.03it/s] 40%|███▉      | 1085/2742 [26:27<26:29,  1.04it/s]                                                   {'loss': 1.2639, 'learning_rate': 3.3100374319019565e-05, 'epoch': 1.19}
 40%|███▉      | 1085/2742 [26:27<26:29,  1.04it/s] 40%|███▉      | 1086/2742 [26:28<26:14,  1.05it/s] 40%|███▉      | 1087/2742 [26:29<26:33,  1.04it/s] 40%|███▉      | 1088/2742 [26:30<27:19,  1.01it/s] 40%|███▉      | 1089/2742 [26:31<27:43,  1.01s/it] 40%|███▉      | 1090/2742 [26:32<26:59,  1.02it/s]                                                   {'loss': 1.2928, 'learning_rate': 3.296475208779506e-05, 'epoch': 1.19}
 40%|███▉      | 1090/2742 [26:32<26:59,  1.02it/s] 40%|███▉      | 1091/2742 [26:33<26:30,  1.04it/s] 40%|███▉      | 1092/2742 [26:34<28:45,  1.05s/it] 40%|███▉      | 1093/2742 [26:35<27:28,  1.00it/s] 40%|███▉      | 1094/2742 [26:36<27:56,  1.02s/it] 40%|███▉      | 1095/2742 [26:37<26:51,  1.02it/s]                                                   {'loss': 1.3206, 'learning_rate': 3.282886847441939e-05, 'epoch': 1.2}
 40%|███▉      | 1095/2742 [26:37<26:51,  1.02it/s] 40%|███▉      | 1096/2742 [26:38<27:37,  1.01s/it] 40%|████      | 1097/2742 [26:39<27:29,  1.00s/it] 40%|████      | 1098/2742 [26:40<27:01,  1.01it/s] 40%|████      | 1099/2742 [26:41<27:16,  1.00it/s] 40%|████      | 1100/2742 [26:42<27:10,  1.01it/s]                                                   {'loss': 1.3978, 'learning_rate': 3.2692727938234265e-05, 'epoch': 1.2}
 40%|████      | 1100/2742 [26:42<27:10,  1.01it/s][INFO|trainer.py:2881] 2023-11-06 14:25:01,618 >> Saving model checkpoint to saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1100
[INFO|tokenization_utils_base.py:2428] 2023-11-06 14:25:01,665 >> tokenizer config file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-06 14:25:01,665 >> Special tokens file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1100/special_tokens_map.json
[2023-11-06 14:25:02,998] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1100 is about to be saved!
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-11-06 14:25:10,270] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1100/global_step1100/mp_rank_00_model_states.pt
[2023-11-06 14:25:10,270] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1100/global_step1100/mp_rank_00_model_states.pt...
[2023-11-06 14:25:39,280] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1100/global_step1100/mp_rank_00_model_states.pt.
[2023-11-06 14:25:39,841] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1100/global_step1100/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 14:25:39,933] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1100/global_step1100/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 14:25:39,933] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1100/global_step1100/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-11-06 14:25:39,934] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1100 is ready now!
 40%|████      | 1101/2742 [27:30<6:48:28, 14.93s/it] 40%|████      | 1102/2742 [27:31<4:53:08, 10.72s/it] 40%|████      | 1103/2742 [27:32<3:34:13,  7.84s/it] 40%|████      | 1104/2742 [27:33<2:38:47,  5.82s/it] 40%|████      | 1105/2742 [27:34<1:59:29,  4.38s/it]                                                     {'loss': 1.4308, 'learning_rate': 3.255633494701291e-05, 'epoch': 1.21}
 40%|████      | 1105/2742 [27:34<1:59:29,  4.38s/it] 40%|████      | 1106/2742 [27:35<1:31:50,  3.37s/it] 40%|████      | 1107/2742 [27:36<1:12:51,  2.67s/it] 40%|████      | 1108/2742 [27:37<58:38,  2.15s/it]   40%|████      | 1109/2742 [27:38<48:55,  1.80s/it] 40%|████      | 1110/2742 [27:39<42:17,  1.55s/it]                                                   {'loss': 1.0926, 'learning_rate': 3.241969397681348e-05, 'epoch': 1.21}
 40%|████      | 1110/2742 [27:39<42:17,  1.55s/it] 41%|████      | 1111/2742 [27:40<38:09,  1.40s/it] 41%|████      | 1112/2742 [27:41<34:25,  1.27s/it] 41%|████      | 1113/2742 [27:42<32:02,  1.18s/it] 41%|████      | 1114/2742 [27:43<31:08,  1.15s/it] 41%|████      | 1115/2742 [27:44<29:23,  1.08s/it]                                                   {'loss': 1.3175, 'learning_rate': 3.228280951183212e-05, 'epoch': 1.22}
 41%|████      | 1115/2742 [27:44<29:23,  1.08s/it] 41%|████      | 1116/2742 [27:45<28:13,  1.04s/it] 41%|████      | 1117/2742 [27:46<28:33,  1.05s/it] 41%|████      | 1118/2742 [27:47<28:06,  1.04s/it] 41%|████      | 1119/2742 [27:48<27:11,  1.01s/it] 41%|████      | 1120/2742 [27:49<26:38,  1.01it/s]                                                   {'loss': 1.2379, 'learning_rate': 3.214568604425586e-05, 'epoch': 1.22}
 41%|████      | 1120/2742 [27:49<26:38,  1.01it/s] 41%|████      | 1121/2742 [27:50<26:06,  1.03it/s] 41%|████      | 1122/2742 [27:51<27:04,  1.00s/it] 41%|████      | 1123/2742 [27:52<26:36,  1.01it/s] 41%|████      | 1124/2742 [27:53<26:30,  1.02it/s] 41%|████      | 1125/2742 [27:54<25:55,  1.04it/s]                                                   {'loss': 1.3447, 'learning_rate': 3.2008328074115145e-05, 'epoch': 1.23}
 41%|████      | 1125/2742 [27:54<25:55,  1.04it/s] 41%|████      | 1126/2742 [27:55<25:37,  1.05it/s] 41%|████      | 1127/2742 [27:56<27:33,  1.02s/it] 41%|████      | 1128/2742 [27:57<26:55,  1.00s/it] 41%|████      | 1129/2742 [27:58<27:04,  1.01s/it] 41%|████      | 1130/2742 [27:59<26:54,  1.00s/it]                                                   {'loss': 1.3242, 'learning_rate': 3.187074010913618e-05, 'epoch': 1.24}
 41%|████      | 1130/2742 [27:59<26:54,  1.00s/it] 41%|████      | 1131/2742 [28:00<27:13,  1.01s/it] 41%|████▏     | 1132/2742 [28:01<26:24,  1.02it/s] 41%|████▏     | 1133/2742 [28:02<25:50,  1.04it/s] 41%|████▏     | 1134/2742 [28:03<25:48,  1.04it/s] 41%|████▏     | 1135/2742 [28:03<25:20,  1.06it/s]                                                   {'loss': 1.3897, 'learning_rate': 3.173292666459301e-05, 'epoch': 1.24}
 41%|████▏     | 1135/2742 [28:03<25:20,  1.06it/s] 41%|████▏     | 1136/2742 [28:04<25:37,  1.04it/s] 41%|████▏     | 1137/2742 [28:05<25:47,  1.04it/s] 42%|████▏     | 1138/2742 [28:06<25:59,  1.03it/s] 42%|████▏     | 1139/2742 [28:07<25:32,  1.05it/s] 42%|████▏     | 1140/2742 [28:08<25:17,  1.06it/s]                                                   {'loss': 1.2238, 'learning_rate': 3.1594892263159296e-05, 'epoch': 1.25}
 42%|████▏     | 1140/2742 [28:08<25:17,  1.06it/s] 42%|████▏     | 1141/2742 [28:09<25:27,  1.05it/s] 42%|████▏     | 1142/2742 [28:10<25:22,  1.05it/s] 42%|████▏     | 1143/2742 [28:11<25:02,  1.06it/s] 42%|████▏     | 1144/2742 [28:12<25:16,  1.05it/s] 42%|████▏     | 1145/2742 [28:13<25:05,  1.06it/s]                                                   {'loss': 1.2694, 'learning_rate': 3.1456641434759926e-05, 'epoch': 1.25}
 42%|████▏     | 1145/2742 [28:13<25:05,  1.06it/s] 42%|████▏     | 1146/2742 [28:14<25:51,  1.03it/s] 42%|████▏     | 1147/2742 [28:15<27:48,  1.05s/it] 42%|████▏     | 1148/2742 [28:16<27:18,  1.03s/it] 42%|████▏     | 1149/2742 [28:17<26:17,  1.01it/s] 42%|████▏     | 1150/2742 [28:18<26:01,  1.02it/s]                                                   {'loss': 1.5587, 'learning_rate': 3.131817871642237e-05, 'epoch': 1.26}
 42%|████▏     | 1150/2742 [28:18<26:01,  1.02it/s] 42%|████▏     | 1151/2742 [28:19<25:28,  1.04it/s] 42%|████▏     | 1152/2742 [28:20<25:34,  1.04it/s] 42%|████▏     | 1153/2742 [28:21<25:36,  1.03it/s] 42%|████▏     | 1154/2742 [28:22<25:31,  1.04it/s] 42%|████▏     | 1155/2742 [28:23<25:06,  1.05it/s]                                                   {'loss': 1.2395, 'learning_rate': 3.1179508652127776e-05, 'epoch': 1.26}
 42%|████▏     | 1155/2742 [28:23<25:06,  1.05it/s] 42%|████▏     | 1156/2742 [28:24<24:38,  1.07it/s] 42%|████▏     | 1157/2742 [28:25<24:41,  1.07it/s] 42%|████▏     | 1158/2742 [28:26<24:33,  1.08it/s] 42%|████▏     | 1159/2742 [28:26<24:30,  1.08it/s] 42%|████▏     | 1160/2742 [28:28<26:30,  1.01s/it]                                                   {'loss': 1.4016, 'learning_rate': 3.1040635792661785e-05, 'epoch': 1.27}
 42%|████▏     | 1160/2742 [28:28<26:30,  1.01s/it] 42%|████▏     | 1161/2742 [28:29<25:47,  1.02it/s] 42%|████▏     | 1162/2742 [28:29<25:03,  1.05it/s] 42%|████▏     | 1163/2742 [28:30<24:56,  1.06it/s] 42%|████▏     | 1164/2742 [28:31<25:05,  1.05it/s] 42%|████▏     | 1165/2742 [28:32<24:44,  1.06it/s]                                                   {'loss': 1.2687, 'learning_rate': 3.0901564695465296e-05, 'epoch': 1.27}
 42%|████▏     | 1165/2742 [28:32<24:44,  1.06it/s] 43%|████▎     | 1166/2742 [28:33<24:42,  1.06it/s] 43%|████▎     | 1167/2742 [28:34<24:32,  1.07it/s] 43%|████▎     | 1168/2742 [28:35<25:35,  1.02it/s] 43%|████▎     | 1169/2742 [28:36<25:22,  1.03it/s] 43%|████▎     | 1170/2742 [28:37<26:11,  1.00it/s]                                                   {'loss': 1.2807, 'learning_rate': 3.076229992448482e-05, 'epoch': 1.28}
 43%|████▎     | 1170/2742 [28:37<26:11,  1.00it/s] 43%|████▎     | 1171/2742 [28:38<25:43,  1.02it/s] 43%|████▎     | 1172/2742 [28:39<25:22,  1.03it/s] 43%|████▎     | 1173/2742 [28:40<26:54,  1.03s/it] 43%|████▎     | 1174/2742 [28:42<28:34,  1.09s/it] 43%|████▎     | 1175/2742 [28:42<27:13,  1.04s/it]                                                   {'loss': 1.1231, 'learning_rate': 3.062284605002274e-05, 'epoch': 1.28}
 43%|████▎     | 1175/2742 [28:42<27:13,  1.04s/it] 43%|████▎     | 1176/2742 [28:44<27:58,  1.07s/it] 43%|████▎     | 1177/2742 [28:45<27:05,  1.04s/it] 43%|████▎     | 1178/2742 [28:45<26:02,  1.00it/s] 43%|████▎     | 1179/2742 [28:46<25:47,  1.01it/s] 43%|████▎     | 1180/2742 [28:47<25:26,  1.02it/s]                                                   {'loss': 1.3058, 'learning_rate': 3.0483207648587296e-05, 'epoch': 1.29}
 43%|████▎     | 1180/2742 [28:47<25:26,  1.02it/s] 43%|████▎     | 1181/2742 [28:48<24:55,  1.04it/s] 43%|████▎     | 1182/2742 [28:49<24:36,  1.06it/s] 43%|████▎     | 1183/2742 [28:50<24:32,  1.06it/s] 43%|████▎     | 1184/2742 [28:51<25:28,  1.02it/s] 43%|████▎     | 1185/2742 [28:52<25:04,  1.04it/s]                                                   {'loss': 1.1867, 'learning_rate': 3.0343389302742452e-05, 'epoch': 1.3}
 43%|████▎     | 1185/2742 [28:52<25:04,  1.04it/s] 43%|████▎     | 1186/2742 [28:53<24:38,  1.05it/s] 43%|████▎     | 1187/2742 [28:54<27:16,  1.05s/it] 43%|████▎     | 1188/2742 [28:55<26:37,  1.03s/it] 43%|████▎     | 1189/2742 [28:56<25:44,  1.01it/s] 43%|████▎     | 1190/2742 [28:57<25:55,  1.00s/it]                                                   {'loss': 1.3325, 'learning_rate': 3.0203395600957458e-05, 'epoch': 1.3}
 43%|████▎     | 1190/2742 [28:57<25:55,  1.00s/it] 43%|████▎     | 1191/2742 [28:58<25:54,  1.00s/it] 43%|████▎     | 1192/2742 [28:59<27:03,  1.05s/it] 44%|████▎     | 1193/2742 [29:00<26:08,  1.01s/it] 44%|████▎     | 1194/2742 [29:01<25:35,  1.01it/s] 44%|████▎     | 1195/2742 [29:02<25:24,  1.01it/s]                                                   {'loss': 1.3101, 'learning_rate': 3.006323113745627e-05, 'epoch': 1.31}
 44%|████▎     | 1195/2742 [29:02<25:24,  1.01it/s] 44%|████▎     | 1196/2742 [29:03<25:54,  1.01s/it] 44%|████▎     | 1197/2742 [29:04<26:02,  1.01s/it] 44%|████▎     | 1198/2742 [29:05<25:54,  1.01s/it] 44%|████▎     | 1199/2742 [29:06<25:16,  1.02it/s] 44%|████▍     | 1200/2742 [29:07<25:36,  1.00it/s]                                                   {'loss': 1.2206, 'learning_rate': 2.9922900512066803e-05, 'epoch': 1.31}
 44%|████▍     | 1200/2742 [29:07<25:36,  1.00it/s][INFO|trainer.py:2881] 2023-11-06 14:27:26,787 >> Saving model checkpoint to saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1200
[INFO|tokenization_utils_base.py:2428] 2023-11-06 14:27:26,828 >> tokenizer config file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-06 14:27:26,829 >> Special tokens file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1200/special_tokens_map.json
[2023-11-06 14:27:28,071] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1200 is about to be saved!
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-11-06 14:27:35,283] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1200/global_step1200/mp_rank_00_model_states.pt
[2023-11-06 14:27:35,283] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1200/global_step1200/mp_rank_00_model_states.pt...
[2023-11-06 14:28:07,517] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1200/global_step1200/mp_rank_00_model_states.pt.
[2023-11-06 14:28:08,052] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1200/global_step1200/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 14:28:08,124] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1200/global_step1200/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 14:28:08,125] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1200/global_step1200/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-11-06 14:28:08,125] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1200 is ready now!
 44%|████▍     | 1201/2742 [29:58<6:48:18, 15.90s/it] 44%|████▍     | 1202/2742 [29:59<4:53:22, 11.43s/it] 44%|████▍     | 1203/2742 [30:00<3:32:17,  8.28s/it] 44%|████▍     | 1204/2742 [30:01<2:35:44,  6.08s/it] 44%|████▍     | 1205/2742 [30:02<1:57:31,  4.59s/it]                                                     {'loss': 1.3258, 'learning_rate': 2.9782408330069966e-05, 'epoch': 1.32}
 44%|████▍     | 1205/2742 [30:02<1:57:31,  4.59s/it] 44%|████▍     | 1206/2742 [30:03<1:30:29,  3.53s/it] 44%|████▍     | 1207/2742 [30:04<1:11:14,  2.78s/it] 44%|████▍     | 1208/2742 [30:05<58:54,  2.30s/it]   44%|████▍     | 1209/2742 [30:06<49:02,  1.92s/it] 44%|████▍     | 1210/2742 [30:07<42:05,  1.65s/it]                                                   {'loss': 1.3097, 'learning_rate': 2.9641759202048535e-05, 'epoch': 1.32}
 44%|████▍     | 1210/2742 [30:07<42:05,  1.65s/it] 44%|████▍     | 1211/2742 [30:08<38:00,  1.49s/it] 44%|████▍     | 1212/2742 [30:09<34:16,  1.34s/it] 44%|████▍     | 1213/2742 [30:10<31:47,  1.25s/it] 44%|████▍     | 1214/2742 [30:11<29:11,  1.15s/it] 44%|████▍     | 1215/2742 [30:12<27:29,  1.08s/it]                                                   {'loss': 1.3397, 'learning_rate': 2.950095774373583e-05, 'epoch': 1.33}
 44%|████▍     | 1215/2742 [30:12<27:29,  1.08s/it] 44%|████▍     | 1216/2742 [30:13<26:18,  1.03s/it] 44%|████▍     | 1217/2742 [30:14<25:28,  1.00s/it] 44%|████▍     | 1218/2742 [30:15<24:35,  1.03it/s] 44%|████▍     | 1219/2742 [30:16<24:30,  1.04it/s] 44%|████▍     | 1220/2742 [30:17<24:02,  1.06it/s]                                                   {'loss': 1.4512, 'learning_rate': 2.9360008575864262e-05, 'epoch': 1.33}
 44%|████▍     | 1220/2742 [30:17<24:02,  1.06it/s] 45%|████▍     | 1221/2742 [30:18<24:12,  1.05it/s] 45%|████▍     | 1222/2742 [30:19<26:02,  1.03s/it] 45%|████▍     | 1223/2742 [30:20<26:19,  1.04s/it] 45%|████▍     | 1224/2742 [30:21<25:25,  1.00s/it] 45%|████▍     | 1225/2742 [30:22<24:54,  1.02it/s]                                                   {'loss': 1.2442, 'learning_rate': 2.9218916324013663e-05, 'epoch': 1.34}
 45%|████▍     | 1225/2742 [30:22<24:54,  1.02it/s] 45%|████▍     | 1226/2742 [30:23<25:18,  1.00s/it] 45%|████▍     | 1227/2742 [30:24<24:47,  1.02it/s] 45%|████▍     | 1228/2742 [30:25<24:43,  1.02it/s] 45%|████▍     | 1229/2742 [30:26<24:30,  1.03it/s] 45%|████▍     | 1230/2742 [30:27<24:19,  1.04it/s]                                                   {'loss': 1.4878, 'learning_rate': 2.907768561845952e-05, 'epoch': 1.34}
 45%|████▍     | 1230/2742 [30:27<24:19,  1.04it/s] 45%|████▍     | 1231/2742 [30:28<24:00,  1.05it/s] 45%|████▍     | 1232/2742 [30:29<25:06,  1.00it/s] 45%|████▍     | 1233/2742 [30:30<24:28,  1.03it/s] 45%|████▌     | 1234/2742 [30:31<24:30,  1.03it/s] 45%|████▌     | 1235/2742 [30:32<24:14,  1.04it/s]                                                   {'loss': 1.1903, 'learning_rate': 2.8936321094021006e-05, 'epoch': 1.35}
 45%|████▌     | 1235/2742 [30:32<24:14,  1.04it/s] 45%|████▌     | 1236/2742 [30:33<23:37,  1.06it/s] 45%|████▌     | 1237/2742 [30:34<23:53,  1.05it/s] 45%|████▌     | 1238/2742 [30:34<23:33,  1.06it/s] 45%|████▌     | 1239/2742 [30:36<24:49,  1.01it/s] 45%|████▌     | 1240/2742 [30:37<25:20,  1.01s/it]                                                   {'loss': 1.1801, 'learning_rate': 2.879482738990885e-05, 'epoch': 1.36}
 45%|████▌     | 1240/2742 [30:37<25:20,  1.01s/it] 45%|████▌     | 1241/2742 [30:38<24:27,  1.02it/s] 45%|████▌     | 1242/2742 [30:38<24:12,  1.03it/s] 45%|████▌     | 1243/2742 [30:39<23:46,  1.05it/s] 45%|████▌     | 1244/2742 [30:41<25:41,  1.03s/it] 45%|████▌     | 1245/2742 [30:41<24:43,  1.01it/s]                                                   {'loss': 1.1831, 'learning_rate': 2.865320914957315e-05, 'epoch': 1.36}
 45%|████▌     | 1245/2742 [30:41<24:43,  1.01it/s] 45%|████▌     | 1246/2742 [30:42<23:56,  1.04it/s] 45%|████▌     | 1247/2742 [30:43<23:35,  1.06it/s] 46%|████▌     | 1248/2742 [30:44<23:52,  1.04it/s] 46%|████▌     | 1249/2742 [30:45<25:27,  1.02s/it] 46%|████▌     | 1250/2742 [30:46<24:41,  1.01it/s]                                                   {'loss': 1.1421, 'learning_rate': 2.851147102055095e-05, 'epoch': 1.37}
 46%|████▌     | 1250/2742 [30:46<24:41,  1.01it/s] 46%|████▌     | 1251/2742 [30:47<24:32,  1.01it/s] 46%|████▌     | 1252/2742 [30:48<24:39,  1.01it/s] 46%|████▌     | 1253/2742 [30:49<25:01,  1.01s/it] 46%|████▌     | 1254/2742 [30:50<24:18,  1.02it/s] 46%|████▌     | 1255/2742 [30:51<23:59,  1.03it/s]                                                   {'loss': 1.2234, 'learning_rate': 2.83696176543137e-05, 'epoch': 1.37}
 46%|████▌     | 1255/2742 [30:51<23:59,  1.03it/s] 46%|████▌     | 1256/2742 [30:52<24:09,  1.02it/s] 46%|████▌     | 1257/2742 [30:53<23:44,  1.04it/s] 46%|████▌     | 1258/2742 [30:54<23:18,  1.06it/s] 46%|████▌     | 1259/2742 [30:55<23:29,  1.05it/s] 46%|████▌     | 1260/2742 [30:56<23:30,  1.05it/s]                                                   {'loss': 1.3159, 'learning_rate': 2.8227653706114653e-05, 'epoch': 1.38}
 46%|████▌     | 1260/2742 [30:56<23:30,  1.05it/s] 46%|████▌     | 1261/2742 [30:57<23:07,  1.07it/s] 46%|████▌     | 1262/2742 [30:58<24:18,  1.02it/s] 46%|████▌     | 1263/2742 [30:59<23:47,  1.04it/s] 46%|████▌     | 1264/2742 [31:00<23:48,  1.03it/s] 46%|████▌     | 1265/2742 [31:01<23:40,  1.04it/s]                                                   {'loss': 1.5593, 'learning_rate': 2.808558383483606e-05, 'epoch': 1.38}
 46%|████▌     | 1265/2742 [31:01<23:40,  1.04it/s] 46%|████▌     | 1266/2742 [31:02<23:07,  1.06it/s] 46%|████▌     | 1267/2742 [31:03<23:02,  1.07it/s] 46%|████▌     | 1268/2742 [31:04<22:51,  1.07it/s] 46%|████▋     | 1269/2742 [31:05<23:25,  1.05it/s] 46%|████▋     | 1270/2742 [31:06<23:34,  1.04it/s]                                                   {'loss': 1.2238, 'learning_rate': 2.79434127028363e-05, 'epoch': 1.39}
 46%|████▋     | 1270/2742 [31:06<23:34,  1.04it/s] 46%|████▋     | 1271/2742 [31:07<23:49,  1.03it/s] 46%|████▋     | 1272/2742 [31:08<23:49,  1.03it/s] 46%|████▋     | 1273/2742 [31:08<23:32,  1.04it/s] 46%|████▋     | 1274/2742 [31:09<23:40,  1.03it/s] 46%|████▋     | 1275/2742 [31:10<24:13,  1.01it/s]                                                   {'loss': 1.3676, 'learning_rate': 2.780114497579685e-05, 'epoch': 1.39}
 46%|████▋     | 1275/2742 [31:10<24:13,  1.01it/s] 47%|████▋     | 1276/2742 [31:11<23:28,  1.04it/s] 47%|████▋     | 1277/2742 [31:12<23:43,  1.03it/s] 47%|████▋     | 1278/2742 [31:13<23:40,  1.03it/s] 47%|████▋     | 1279/2742 [31:14<24:06,  1.01it/s] 47%|████▋     | 1280/2742 [31:15<23:35,  1.03it/s]                                                   {'loss': 1.3256, 'learning_rate': 2.765878532256918e-05, 'epoch': 1.4}
 47%|████▋     | 1280/2742 [31:15<23:35,  1.03it/s] 47%|████▋     | 1281/2742 [31:16<23:27,  1.04it/s] 47%|████▋     | 1282/2742 [31:17<23:41,  1.03it/s] 47%|████▋     | 1283/2742 [31:18<23:09,  1.05it/s] 47%|████▋     | 1284/2742 [31:19<23:00,  1.06it/s] 47%|████▋     | 1285/2742 [31:20<23:12,  1.05it/s]                                                   {'loss': 1.1191, 'learning_rate': 2.7516338415021546e-05, 'epoch': 1.4}
 47%|████▋     | 1285/2742 [31:20<23:12,  1.05it/s] 47%|████▋     | 1286/2742 [31:21<23:22,  1.04it/s] 47%|████▋     | 1287/2742 [31:22<23:22,  1.04it/s] 47%|████▋     | 1288/2742 [31:23<24:04,  1.01it/s] 47%|████▋     | 1289/2742 [31:24<23:49,  1.02it/s] 47%|████▋     | 1290/2742 [31:25<24:21,  1.01s/it]                                                   {'loss': 1.2417, 'learning_rate': 2.7373808927885665e-05, 'epoch': 1.41}
 47%|████▋     | 1290/2742 [31:25<24:21,  1.01s/it] 47%|████▋     | 1291/2742 [31:26<23:51,  1.01it/s] 47%|████▋     | 1292/2742 [31:27<24:12,  1.00s/it] 47%|████▋     | 1293/2742 [31:28<23:28,  1.03it/s] 47%|████▋     | 1294/2742 [31:29<23:23,  1.03it/s] 47%|████▋     | 1295/2742 [31:30<23:00,  1.05it/s]                                                   {'loss': 1.0753, 'learning_rate': 2.7231201538603268e-05, 'epoch': 1.42}
 47%|████▋     | 1295/2742 [31:30<23:00,  1.05it/s] 47%|████▋     | 1296/2742 [31:31<23:27,  1.03it/s] 47%|████▋     | 1297/2742 [31:32<23:23,  1.03it/s] 47%|████▋     | 1298/2742 [31:33<23:16,  1.03it/s] 47%|████▋     | 1299/2742 [31:34<23:40,  1.02it/s] 47%|████▋     | 1300/2742 [31:35<23:48,  1.01it/s]                                                   {'loss': 1.1558, 'learning_rate': 2.7088520927172657e-05, 'epoch': 1.42}
 47%|████▋     | 1300/2742 [31:35<23:48,  1.01it/s][INFO|trainer.py:2881] 2023-11-06 14:29:54,334 >> Saving model checkpoint to saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1300
[INFO|tokenization_utils_base.py:2428] 2023-11-06 14:29:54,373 >> tokenizer config file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-06 14:29:54,373 >> Special tokens file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1300/special_tokens_map.json
[2023-11-06 14:29:55,134] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1300 is about to be saved!
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-11-06 14:30:02,257] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1300/global_step1300/mp_rank_00_model_states.pt
[2023-11-06 14:30:02,257] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1300/global_step1300/mp_rank_00_model_states.pt...
[2023-11-06 14:30:31,603] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1300/global_step1300/mp_rank_00_model_states.pt.
[2023-11-06 14:30:32,181] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1300/global_step1300/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 14:30:32,233] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1300/global_step1300/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 14:30:32,233] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1300/global_step1300/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-11-06 14:30:32,234] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1300 is ready now!
 47%|████▋     | 1301/2742 [32:22<5:56:47, 14.86s/it] 47%|████▋     | 1302/2742 [32:23<4:18:29, 10.77s/it] 48%|████▊     | 1303/2742 [32:24<3:07:32,  7.82s/it] 48%|████▊     | 1304/2742 [32:25<2:18:11,  5.77s/it] 48%|████▊     | 1305/2742 [32:26<1:43:22,  4.32s/it]                                                     {'loss': 1.2183, 'learning_rate': 2.694577177599509e-05, 'epoch': 1.43}
 48%|████▊     | 1305/2742 [32:26<1:43:22,  4.32s/it] 48%|████▊     | 1306/2742 [32:27<1:19:08,  3.31s/it] 48%|████▊     | 1307/2742 [32:28<1:01:59,  2.59s/it] 48%|████▊     | 1308/2742 [32:29<50:37,  2.12s/it]   48%|████▊     | 1309/2742 [32:30<42:05,  1.76s/it] 48%|████▊     | 1310/2742 [32:31<35:53,  1.50s/it]                                                   {'loss': 1.154, 'learning_rate': 2.6802958769721105e-05, 'epoch': 1.43}
 48%|████▊     | 1310/2742 [32:31<35:53,  1.50s/it] 48%|████▊     | 1311/2742 [32:32<32:26,  1.36s/it] 48%|████▊     | 1312/2742 [32:33<29:14,  1.23s/it] 48%|████▊     | 1313/2742 [32:34<27:08,  1.14s/it] 48%|████▊     | 1314/2742 [32:35<25:31,  1.07s/it] 48%|████▊     | 1315/2742 [32:36<24:18,  1.02s/it]                                                   {'loss': 1.3136, 'learning_rate': 2.66600865950968e-05, 'epoch': 1.44}
 48%|████▊     | 1315/2742 [32:36<24:18,  1.02s/it] 48%|████▊     | 1316/2742 [32:36<23:19,  1.02it/s] 48%|████▊     | 1317/2742 [32:37<23:35,  1.01it/s] 48%|████▊     | 1318/2742 [32:38<22:52,  1.04it/s] 48%|████▊     | 1319/2742 [32:39<22:54,  1.04it/s] 48%|████▊     | 1320/2742 [32:40<24:19,  1.03s/it]                                                   {'loss': 1.4991, 'learning_rate': 2.6517159940810037e-05, 'epoch': 1.44}
 48%|████▊     | 1320/2742 [32:40<24:19,  1.03s/it] 48%|████▊     | 1321/2742 [32:41<24:05,  1.02s/it] 48%|████▊     | 1322/2742 [32:42<23:57,  1.01s/it] 48%|████▊     | 1323/2742 [32:43<23:50,  1.01s/it] 48%|████▊     | 1324/2742 [32:44<23:21,  1.01it/s] 48%|████▊     | 1325/2742 [32:45<22:48,  1.04it/s]                                                   {'loss': 1.231, 'learning_rate': 2.6374183497336536e-05, 'epoch': 1.45}
 48%|████▊     | 1325/2742 [32:45<22:48,  1.04it/s] 48%|████▊     | 1326/2742 [32:46<22:17,  1.06it/s] 48%|████▊     | 1327/2742 [32:47<22:21,  1.06it/s] 48%|████▊     | 1328/2742 [32:48<23:02,  1.02it/s] 48%|████▊     | 1329/2742 [32:49<23:09,  1.02it/s] 49%|████▊     | 1330/2742 [32:50<24:06,  1.02s/it]                                                   {'loss': 1.2835, 'learning_rate': 2.6231161956785988e-05, 'epoch': 1.45}
 49%|████▊     | 1330/2742 [32:50<24:06,  1.02s/it] 49%|████▊     | 1331/2742 [32:51<23:32,  1.00s/it] 49%|████▊     | 1332/2742 [32:52<23:31,  1.00s/it] 49%|████▊     | 1333/2742 [32:53<23:07,  1.02it/s] 49%|████▊     | 1334/2742 [32:54<22:39,  1.04it/s] 49%|████▊     | 1335/2742 [32:55<22:23,  1.05it/s]                                                   {'loss': 1.2973, 'learning_rate': 2.6088100012748017e-05, 'epoch': 1.46}
 49%|████▊     | 1335/2742 [32:55<22:23,  1.05it/s] 49%|████▊     | 1336/2742 [32:56<22:34,  1.04it/s] 49%|████▉     | 1337/2742 [32:57<22:14,  1.05it/s] 49%|████▉     | 1338/2742 [32:58<21:53,  1.07it/s] 49%|████▉     | 1339/2742 [32:59<21:58,  1.06it/s] 49%|████▉     | 1340/2742 [33:00<22:02,  1.06it/s]                                                   {'loss': 1.2982, 'learning_rate': 2.5945002360138228e-05, 'epoch': 1.46}
 49%|████▉     | 1340/2742 [33:00<22:02,  1.06it/s] 49%|████▉     | 1341/2742 [33:01<21:38,  1.08it/s] 49%|████▉     | 1342/2742 [33:02<21:42,  1.07it/s] 49%|████▉     | 1343/2742 [33:03<21:47,  1.07it/s] 49%|████▉     | 1344/2742 [33:03<21:38,  1.08it/s] 49%|████▉     | 1345/2742 [33:04<21:56,  1.06it/s]                                                   {'loss': 1.2602, 'learning_rate': 2.5801873695044053e-05, 'epoch': 1.47}
 49%|████▉     | 1345/2742 [33:04<21:56,  1.06it/s] 49%|████▉     | 1346/2742 [33:05<22:12,  1.05it/s] 49%|████▉     | 1347/2742 [33:06<22:52,  1.02it/s] 49%|████▉     | 1348/2742 [33:07<22:28,  1.03it/s] 49%|████▉     | 1349/2742 [33:09<23:56,  1.03s/it] 49%|████▉     | 1350/2742 [33:10<24:28,  1.06s/it]                                                   {'loss': 1.3525, 'learning_rate': 2.5658718714570677e-05, 'epoch': 1.48}
 49%|████▉     | 1350/2742 [33:10<24:28,  1.06s/it] 49%|████▉     | 1351/2742 [33:11<23:35,  1.02s/it] 49%|████▉     | 1352/2742 [33:12<22:43,  1.02it/s] 49%|████▉     | 1353/2742 [33:12<22:07,  1.05it/s] 49%|████▉     | 1354/2742 [33:13<21:52,  1.06it/s] 49%|████▉     | 1355/2742 [33:14<22:43,  1.02it/s]                                                   {'loss': 1.0926, 'learning_rate': 2.5515542116686908e-05, 'epoch': 1.48}
 49%|████▉     | 1355/2742 [33:14<22:43,  1.02it/s] 49%|████▉     | 1356/2742 [33:15<22:21,  1.03it/s] 49%|████▉     | 1357/2742 [33:16<22:14,  1.04it/s] 50%|████▉     | 1358/2742 [33:17<21:55,  1.05it/s] 50%|████▉     | 1359/2742 [33:18<21:44,  1.06it/s] 50%|████▉     | 1360/2742 [33:19<21:25,  1.08it/s]                                                   {'loss': 1.1975, 'learning_rate': 2.537234860007095e-05, 'epoch': 1.49}
 50%|████▉     | 1360/2742 [33:19<21:25,  1.08it/s] 50%|████▉     | 1361/2742 [33:20<21:18,  1.08it/s] 50%|████▉     | 1362/2742 [33:21<22:09,  1.04it/s] 50%|████▉     | 1363/2742 [33:22<23:16,  1.01s/it] 50%|████▉     | 1364/2742 [33:23<23:04,  1.01s/it] 50%|████▉     | 1365/2742 [33:24<22:33,  1.02it/s]                                                   {'loss': 1.1791, 'learning_rate': 2.5229142863956256e-05, 'epoch': 1.49}
 50%|████▉     | 1365/2742 [33:24<22:33,  1.02it/s] 50%|████▉     | 1366/2742 [33:25<22:12,  1.03it/s] 50%|████▉     | 1367/2742 [33:26<22:23,  1.02it/s] 50%|████▉     | 1368/2742 [33:27<23:05,  1.01s/it] 50%|████▉     | 1369/2742 [33:28<22:27,  1.02it/s] 50%|████▉     | 1370/2742 [33:29<22:05,  1.04it/s]                                                   {'loss': 1.2104, 'learning_rate': 2.5085929607977287e-05, 'epoch': 1.5}
 50%|████▉     | 1370/2742 [33:29<22:05,  1.04it/s] 50%|█████     | 1371/2742 [33:30<21:58,  1.04it/s] 50%|█████     | 1372/2742 [33:31<21:57,  1.04it/s] 50%|█████     | 1373/2742 [33:32<21:37,  1.06it/s] 50%|█████     | 1374/2742 [33:33<21:52,  1.04it/s] 50%|█████     | 1375/2742 [33:34<21:32,  1.06it/s]                                                   {'loss': 1.4028, 'learning_rate': 2.4942713532015285e-05, 'epoch': 1.5}
 50%|█████     | 1375/2742 [33:34<21:32,  1.06it/s] 50%|█████     | 1376/2742 [33:35<21:13,  1.07it/s] 50%|█████     | 1377/2742 [33:35<21:14,  1.07it/s] 50%|█████     | 1378/2742 [33:36<21:16,  1.07it/s] 50%|█████     | 1379/2742 [33:37<21:55,  1.04it/s] 50%|█████     | 1380/2742 [33:38<21:45,  1.04it/s]                                                   {'loss': 1.411, 'learning_rate': 2.479949933604402e-05, 'epoch': 1.51}
 50%|█████     | 1380/2742 [33:38<21:45,  1.04it/s] 50%|█████     | 1381/2742 [33:39<21:49,  1.04it/s] 50%|█████     | 1382/2742 [33:41<23:17,  1.03s/it] 50%|█████     | 1383/2742 [33:42<22:45,  1.00s/it] 50%|█████     | 1384/2742 [33:42<22:15,  1.02it/s] 51%|█████     | 1385/2742 [33:43<22:31,  1.00it/s]                                                   {'loss': 1.3205, 'learning_rate': 2.4656291719975598e-05, 'epoch': 1.51}
 51%|█████     | 1385/2742 [33:43<22:31,  1.00it/s] 51%|█████     | 1386/2742 [33:44<21:57,  1.03it/s] 51%|█████     | 1387/2742 [33:45<22:39,  1.00s/it] 51%|█████     | 1388/2742 [33:46<22:42,  1.01s/it] 51%|█████     | 1389/2742 [33:47<22:34,  1.00s/it] 51%|█████     | 1390/2742 [33:48<22:36,  1.00s/it]                                                   {'loss': 1.3864, 'learning_rate': 2.451309538350617e-05, 'epoch': 1.52}
 51%|█████     | 1390/2742 [33:48<22:36,  1.00s/it] 51%|█████     | 1391/2742 [33:49<21:54,  1.03it/s] 51%|█████     | 1392/2742 [33:50<22:10,  1.02it/s] 51%|█████     | 1393/2742 [33:51<21:40,  1.04it/s] 51%|█████     | 1394/2742 [33:52<21:25,  1.05it/s] 51%|█████     | 1395/2742 [33:53<21:16,  1.06it/s]                                                   {'loss': 1.1681, 'learning_rate': 2.436991502596171e-05, 'epoch': 1.53}
 51%|█████     | 1395/2742 [33:53<21:16,  1.06it/s] 51%|█████     | 1396/2742 [33:54<21:30,  1.04it/s] 51%|█████     | 1397/2742 [33:55<22:00,  1.02it/s] 51%|█████     | 1398/2742 [33:57<25:02,  1.12s/it] 51%|█████     | 1399/2742 [33:58<23:50,  1.07s/it] 51%|█████     | 1400/2742 [33:58<22:50,  1.02s/it]                                                   {'loss': 1.1356, 'learning_rate': 2.4226755346143836e-05, 'epoch': 1.53}
 51%|█████     | 1400/2742 [33:58<22:50,  1.02s/it][INFO|trainer.py:2881] 2023-11-06 14:32:17,959 >> Saving model checkpoint to saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1400
[INFO|tokenization_utils_base.py:2428] 2023-11-06 14:32:17,997 >> tokenizer config file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-06 14:32:17,998 >> Special tokens file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1400/special_tokens_map.json
[2023-11-06 14:32:18,688] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1400 is about to be saved!
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-11-06 14:32:25,706] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1400/global_step1400/mp_rank_00_model_states.pt
[2023-11-06 14:32:25,706] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1400/global_step1400/mp_rank_00_model_states.pt...
[2023-11-06 14:32:55,351] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1400/global_step1400/mp_rank_00_model_states.pt.
[2023-11-06 14:32:55,899] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1400/global_step1400/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 14:32:55,949] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1400/global_step1400/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 14:32:55,949] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1400/global_step1400/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-11-06 14:32:55,950] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1400 is ready now!
 51%|█████     | 1401/2742 [34:46<5:33:28, 14.92s/it] 51%|█████     | 1402/2742 [34:47<4:00:27, 10.77s/it] 51%|█████     | 1403/2742 [34:48<2:54:16,  7.81s/it] 51%|█████     | 1404/2742 [34:49<2:08:20,  5.76s/it] 51%|█████     | 1405/2742 [34:50<1:35:42,  4.29s/it]                                                     {'loss': 1.269, 'learning_rate': 2.408362104217555e-05, 'epoch': 1.54}
 51%|█████     | 1405/2742 [34:50<1:35:42,  4.29s/it] 51%|█████▏    | 1406/2742 [34:51<1:13:11,  3.29s/it] 51%|█████▏    | 1407/2742 [34:52<57:56,  2.60s/it]   51%|█████▏    | 1408/2742 [34:53<47:25,  2.13s/it] 51%|█████▏    | 1409/2742 [34:54<39:09,  1.76s/it] 51%|█████▏    | 1410/2742 [34:55<33:52,  1.53s/it]                                                   {'loss': 1.1213, 'learning_rate': 2.3940516811347093e-05, 'epoch': 1.54}
 51%|█████▏    | 1410/2742 [34:55<33:52,  1.53s/it] 51%|█████▏    | 1411/2742 [34:55<29:48,  1.34s/it] 51%|█████▏    | 1412/2742 [34:56<27:27,  1.24s/it] 52%|█████▏    | 1413/2742 [34:57<25:04,  1.13s/it] 52%|█████▏    | 1414/2742 [34:58<23:37,  1.07s/it] 52%|█████▏    | 1415/2742 [34:59<23:29,  1.06s/it]                                                   {'loss': 1.1864, 'learning_rate': 2.37974473499618e-05, 'epoch': 1.55}
 52%|█████▏    | 1415/2742 [34:59<23:29,  1.06s/it] 52%|█████▏    | 1416/2742 [35:00<22:19,  1.01s/it] 52%|█████▏    | 1417/2742 [35:01<21:42,  1.02it/s] 52%|█████▏    | 1418/2742 [35:02<21:51,  1.01it/s] 52%|█████▏    | 1419/2742 [35:03<21:50,  1.01it/s] 52%|█████▏    | 1420/2742 [35:04<21:47,  1.01it/s]                                                   {'loss': 1.0699, 'learning_rate': 2.3654417353181946e-05, 'epoch': 1.55}
 52%|█████▏    | 1420/2742 [35:04<21:47,  1.01it/s] 52%|█████▏    | 1421/2742 [35:05<21:41,  1.01it/s] 52%|█████▏    | 1422/2742 [35:06<22:09,  1.01s/it] 52%|█████▏    | 1423/2742 [35:07<22:17,  1.01s/it] 52%|█████▏    | 1424/2742 [35:08<22:10,  1.01s/it] 52%|█████▏    | 1425/2742 [35:09<21:19,  1.03it/s]                                                   {'loss': 1.142, 'learning_rate': 2.3511431514874687e-05, 'epoch': 1.56}
 52%|█████▏    | 1425/2742 [35:09<21:19,  1.03it/s] 52%|█████▏    | 1426/2742 [35:10<21:08,  1.04it/s] 52%|█████▏    | 1427/2742 [35:11<21:08,  1.04it/s] 52%|█████▏    | 1428/2742 [35:12<20:43,  1.06it/s] 52%|█████▏    | 1429/2742 [35:13<20:29,  1.07it/s] 52%|█████▏    | 1430/2742 [35:14<22:00,  1.01s/it]                                                   {'loss': 1.3845, 'learning_rate': 2.336849452745801e-05, 'epoch': 1.56}
 52%|█████▏    | 1430/2742 [35:14<22:00,  1.01s/it] 52%|█████▏    | 1431/2742 [35:15<21:56,  1.00s/it] 52%|█████▏    | 1432/2742 [35:16<21:23,  1.02it/s] 52%|█████▏    | 1433/2742 [35:17<22:07,  1.01s/it] 52%|█████▏    | 1434/2742 [35:18<22:25,  1.03s/it] 52%|█████▏    | 1435/2742 [35:19<21:36,  1.01it/s]                                                   {'loss': 1.3848, 'learning_rate': 2.3225611081746767e-05, 'epoch': 1.57}
 52%|█████▏    | 1435/2742 [35:19<21:36,  1.01it/s] 52%|█████▏    | 1436/2742 [35:20<21:24,  1.02it/s] 52%|█████▏    | 1437/2742 [35:21<20:57,  1.04it/s] 52%|█████▏    | 1438/2742 [35:22<20:27,  1.06it/s] 52%|█████▏    | 1439/2742 [35:23<20:26,  1.06it/s] 53%|█████▎    | 1440/2742 [35:24<20:58,  1.03it/s]                                                   {'loss': 1.3334, 'learning_rate': 2.308278586679868e-05, 'epoch': 1.57}
 53%|█████▎    | 1440/2742 [35:24<20:58,  1.03it/s] 53%|█████▎    | 1441/2742 [35:25<20:44,  1.05it/s] 53%|█████▎    | 1442/2742 [35:26<21:05,  1.03it/s] 53%|█████▎    | 1443/2742 [35:27<21:41,  1.00s/it] 53%|█████▎    | 1444/2742 [35:28<21:28,  1.01it/s] 53%|█████▎    | 1445/2742 [35:29<21:26,  1.01it/s]                                                   {'loss': 1.3876, 'learning_rate': 2.2940023569760503e-05, 'epoch': 1.58}
 53%|█████▎    | 1445/2742 [35:29<21:26,  1.01it/s] 53%|█████▎    | 1446/2742 [35:30<21:13,  1.02it/s] 53%|█████▎    | 1447/2742 [35:31<20:56,  1.03it/s] 53%|█████▎    | 1448/2742 [35:31<20:33,  1.05it/s] 53%|█████▎    | 1449/2742 [35:32<20:07,  1.07it/s] 53%|█████▎    | 1450/2742 [35:33<20:40,  1.04it/s]                                                   {'loss': 1.1711, 'learning_rate': 2.2797328875714217e-05, 'epoch': 1.59}
 53%|█████▎    | 1450/2742 [35:33<20:40,  1.04it/s] 53%|█████▎    | 1451/2742 [35:34<20:26,  1.05it/s] 53%|█████▎    | 1452/2742 [35:35<20:07,  1.07it/s] 53%|█████▎    | 1453/2742 [35:36<19:56,  1.08it/s] 53%|█████▎    | 1454/2742 [35:37<20:17,  1.06it/s] 53%|█████▎    | 1455/2742 [35:38<21:37,  1.01s/it]                                                   {'loss': 1.1461, 'learning_rate': 2.26547064675232e-05, 'epoch': 1.59}
 53%|█████▎    | 1455/2742 [35:38<21:37,  1.01s/it] 53%|█████▎    | 1456/2742 [35:39<21:15,  1.01it/s] 53%|█████▎    | 1457/2742 [35:40<20:40,  1.04it/s] 53%|█████▎    | 1458/2742 [35:41<20:21,  1.05it/s] 53%|█████▎    | 1459/2742 [35:42<20:49,  1.03it/s] 53%|█████▎    | 1460/2742 [35:43<20:39,  1.03it/s]                                                   {'loss': 1.3308, 'learning_rate': 2.2512161025678655e-05, 'epoch': 1.6}
 53%|█████▎    | 1460/2742 [35:43<20:39,  1.03it/s] 53%|█████▎    | 1461/2742 [35:44<20:37,  1.03it/s] 53%|█████▎    | 1462/2742 [35:45<20:10,  1.06it/s] 53%|█████▎    | 1463/2742 [35:46<20:13,  1.05it/s] 53%|█████▎    | 1464/2742 [35:47<19:59,  1.07it/s] 53%|█████▎    | 1465/2742 [35:48<19:51,  1.07it/s]                                                   {'loss': 1.1683, 'learning_rate': 2.2369697228145904e-05, 'epoch': 1.6}
 53%|█████▎    | 1465/2742 [35:48<19:51,  1.07it/s] 53%|█████▎    | 1466/2742 [35:49<20:02,  1.06it/s] 54%|█████▎    | 1467/2742 [35:49<19:44,  1.08it/s] 54%|█████▎    | 1468/2742 [35:50<19:38,  1.08it/s] 54%|█████▎    | 1469/2742 [35:51<20:35,  1.03it/s] 54%|█████▎    | 1470/2742 [35:53<20:58,  1.01it/s]                                                   {'loss': 1.4484, 'learning_rate': 2.222731975021095e-05, 'epoch': 1.61}
 54%|█████▎    | 1470/2742 [35:53<20:58,  1.01it/s] 54%|█████▎    | 1471/2742 [35:53<20:37,  1.03it/s] 54%|█████▎    | 1472/2742 [35:54<20:15,  1.04it/s] 54%|█████▎    | 1473/2742 [35:55<20:38,  1.02it/s] 54%|█████▍    | 1474/2742 [35:56<20:15,  1.04it/s] 54%|█████▍    | 1475/2742 [35:57<20:22,  1.04it/s]                                                   {'loss': 1.2582, 'learning_rate': 2.208503326432699e-05, 'epoch': 1.61}
 54%|█████▍    | 1475/2742 [35:57<20:22,  1.04it/s] 54%|█████▍    | 1476/2742 [35:58<20:47,  1.01it/s] 54%|█████▍    | 1477/2742 [35:59<20:44,  1.02it/s] 54%|█████▍    | 1478/2742 [36:00<20:20,  1.04it/s] 54%|█████▍    | 1479/2742 [36:01<21:38,  1.03s/it] 54%|█████▍    | 1480/2742 [36:02<21:08,  1.01s/it]                                                   {'loss': 1.3134, 'learning_rate': 2.194284243996112e-05, 'epoch': 1.62}
 54%|█████▍    | 1480/2742 [36:02<21:08,  1.01s/it] 54%|█████▍    | 1481/2742 [36:03<20:30,  1.02it/s] 54%|█████▍    | 1482/2742 [36:04<20:02,  1.05it/s] 54%|█████▍    | 1483/2742 [36:05<20:05,  1.04it/s] 54%|█████▍    | 1484/2742 [36:06<20:28,  1.02it/s] 54%|█████▍    | 1485/2742 [36:07<20:05,  1.04it/s]                                                   {'loss': 1.1106, 'learning_rate': 2.180075194344109e-05, 'epoch': 1.62}
 54%|█████▍    | 1485/2742 [36:07<20:05,  1.04it/s] 54%|█████▍    | 1486/2742 [36:08<20:13,  1.04it/s] 54%|█████▍    | 1487/2742 [36:09<22:25,  1.07s/it] 54%|█████▍    | 1488/2742 [36:10<21:52,  1.05s/it] 54%|█████▍    | 1489/2742 [36:11<21:36,  1.03s/it] 54%|█████▍    | 1490/2742 [36:12<21:35,  1.03s/it]                                                   {'loss': 1.1998, 'learning_rate': 2.165876643780211e-05, 'epoch': 1.63}
 54%|█████▍    | 1490/2742 [36:12<21:35,  1.03s/it] 54%|█████▍    | 1491/2742 [36:13<21:21,  1.02s/it] 54%|█████▍    | 1492/2742 [36:14<21:01,  1.01s/it] 54%|█████▍    | 1493/2742 [36:15<20:28,  1.02it/s] 54%|█████▍    | 1494/2742 [36:16<20:06,  1.03it/s] 55%|█████▍    | 1495/2742 [36:17<19:59,  1.04it/s]                                                   {'loss': 1.2849, 'learning_rate': 2.1516890582633917e-05, 'epoch': 1.63}
 55%|█████▍    | 1495/2742 [36:17<19:59,  1.04it/s] 55%|█████▍    | 1496/2742 [36:18<19:46,  1.05it/s] 55%|█████▍    | 1497/2742 [36:19<20:13,  1.03it/s] 55%|█████▍    | 1498/2742 [36:20<21:03,  1.02s/it] 55%|█████▍    | 1499/2742 [36:21<20:29,  1.01it/s] 55%|█████▍    | 1500/2742 [36:22<21:05,  1.02s/it]                                                   {'loss': 1.071, 'learning_rate': 2.137512903392777e-05, 'epoch': 1.64}
 55%|█████▍    | 1500/2742 [36:22<21:05,  1.02s/it][INFO|trainer.py:2881] 2023-11-06 14:34:41,738 >> Saving model checkpoint to saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1500
[INFO|tokenization_utils_base.py:2428] 2023-11-06 14:34:41,776 >> tokenizer config file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-06 14:34:41,776 >> Special tokens file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1500/special_tokens_map.json
[2023-11-06 14:34:42,471] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1500 is about to be saved!
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-11-06 14:34:49,476] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1500/global_step1500/mp_rank_00_model_states.pt
[2023-11-06 14:34:49,477] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1500/global_step1500/mp_rank_00_model_states.pt...
[2023-11-06 14:35:20,004] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1500/global_step1500/mp_rank_00_model_states.pt.
[2023-11-06 14:35:20,550] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1500/global_step1500/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 14:35:20,629] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1500/global_step1500/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 14:35:20,630] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1500/global_step1500/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-11-06 14:35:20,630] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1500 is ready now!
 55%|█████▍    | 1501/2742 [37:11<5:16:14, 15.29s/it] 55%|█████▍    | 1502/2742 [37:12<3:46:51, 10.98s/it] 55%|█████▍    | 1503/2742 [37:13<2:44:07,  7.95s/it] 55%|█████▍    | 1504/2742 [37:14<2:00:30,  5.84s/it] 55%|█████▍    | 1505/2742 [37:14<1:29:50,  4.36s/it]                                                     {'loss': 1.1816, 'learning_rate': 2.1233486443923704e-05, 'epoch': 1.65}
 55%|█████▍    | 1505/2742 [37:14<1:29:50,  4.36s/it] 55%|█████▍    | 1506/2742 [37:15<1:08:18,  3.32s/it] 55%|█████▍    | 1507/2742 [37:16<53:39,  2.61s/it]   55%|█████▍    | 1508/2742 [37:17<43:21,  2.11s/it] 55%|█████▌    | 1509/2742 [37:18<36:47,  1.79s/it] 55%|█████▌    | 1510/2742 [37:19<32:28,  1.58s/it]                                                   {'loss': 1.1609, 'learning_rate': 2.1091967460957865e-05, 'epoch': 1.65}
 55%|█████▌    | 1510/2742 [37:19<32:28,  1.58s/it] 55%|█████▌    | 1511/2742 [37:20<28:28,  1.39s/it] 55%|█████▌    | 1512/2742 [37:21<26:03,  1.27s/it] 55%|█████▌    | 1513/2742 [37:22<24:40,  1.20s/it] 55%|█████▌    | 1514/2742 [37:23<23:22,  1.14s/it] 55%|█████▌    | 1515/2742 [37:24<22:54,  1.12s/it]                                                   {'loss': 1.175, 'learning_rate': 2.0950576729309906e-05, 'epoch': 1.66}
 55%|█████▌    | 1515/2742 [37:24<22:54,  1.12s/it] 55%|█████▌    | 1516/2742 [37:25<21:33,  1.05s/it] 55%|█████▌    | 1517/2742 [37:26<20:28,  1.00s/it] 55%|█████▌    | 1518/2742 [37:27<20:41,  1.01s/it] 55%|█████▌    | 1519/2742 [37:28<20:54,  1.03s/it] 55%|█████▌    | 1520/2742 [37:29<21:16,  1.04s/it]                                                   {'loss': 1.2959, 'learning_rate': 2.080931888905064e-05, 'epoch': 1.66}
 55%|█████▌    | 1520/2742 [37:29<21:16,  1.04s/it] 55%|█████▌    | 1521/2742 [37:30<21:00,  1.03s/it] 56%|█████▌    | 1522/2742 [37:31<20:39,  1.02s/it] 56%|█████▌    | 1523/2742 [37:33<21:30,  1.06s/it] 56%|█████▌    | 1524/2742 [37:34<21:08,  1.04s/it] 56%|█████▌    | 1525/2742 [37:35<20:39,  1.02s/it]                                                   {'loss': 1.2219, 'learning_rate': 2.0668198575889702e-05, 'epoch': 1.67}
 56%|█████▌    | 1525/2742 [37:35<20:39,  1.02s/it] 56%|█████▌    | 1526/2742 [37:35<20:01,  1.01it/s] 56%|█████▌    | 1527/2742 [37:36<20:26,  1.01s/it] 56%|█████▌    | 1528/2742 [37:38<20:30,  1.01s/it] 56%|█████▌    | 1529/2742 [37:39<20:46,  1.03s/it] 56%|█████▌    | 1530/2742 [37:39<20:05,  1.01it/s]                                                   {'loss': 1.2643, 'learning_rate': 2.0527220421023497e-05, 'epoch': 1.67}
 56%|█████▌    | 1530/2742 [37:39<20:05,  1.01it/s] 56%|█████▌    | 1531/2742 [37:40<19:56,  1.01it/s] 56%|█████▌    | 1532/2742 [37:41<19:49,  1.02it/s] 56%|█████▌    | 1533/2742 [37:42<19:40,  1.02it/s] 56%|█████▌    | 1534/2742 [37:43<19:40,  1.02it/s] 56%|█████▌    | 1535/2742 [37:44<19:47,  1.02it/s]                                                   {'loss': 1.3587, 'learning_rate': 2.0386389050983118e-05, 'epoch': 1.68}
 56%|█████▌    | 1535/2742 [37:44<19:47,  1.02it/s] 56%|█████▌    | 1536/2742 [37:45<19:17,  1.04it/s] 56%|█████▌    | 1537/2742 [37:46<18:47,  1.07it/s] 56%|█████▌    | 1538/2742 [37:47<18:54,  1.06it/s] 56%|█████▌    | 1539/2742 [37:48<19:32,  1.03it/s] 56%|█████▌    | 1540/2742 [37:49<19:30,  1.03it/s]                                                   {'loss': 1.0768, 'learning_rate': 2.0245709087482613e-05, 'epoch': 1.68}
 56%|█████▌    | 1540/2742 [37:49<19:30,  1.03it/s] 56%|█████▌    | 1541/2742 [37:50<19:01,  1.05it/s] 56%|█████▌    | 1542/2742 [37:51<19:09,  1.04it/s] 56%|█████▋    | 1543/2742 [37:52<19:42,  1.01it/s] 56%|█████▋    | 1544/2742 [37:53<19:21,  1.03it/s] 56%|█████▋    | 1545/2742 [37:54<19:51,  1.00it/s]                                                   {'loss': 1.1694, 'learning_rate': 2.0105185147267243e-05, 'epoch': 1.69}
 56%|█████▋    | 1545/2742 [37:54<19:51,  1.00it/s] 56%|█████▋    | 1546/2742 [37:55<19:29,  1.02it/s] 56%|█████▋    | 1547/2742 [37:56<19:35,  1.02it/s] 56%|█████▋    | 1548/2742 [37:57<20:42,  1.04s/it] 56%|█████▋    | 1549/2742 [37:58<20:12,  1.02s/it] 57%|█████▋    | 1550/2742 [37:59<19:34,  1.01it/s]                                                   {'loss': 1.1688, 'learning_rate': 1.9964821841961984e-05, 'epoch': 1.69}
 57%|█████▋    | 1550/2742 [37:59<19:34,  1.01it/s] 57%|█████▋    | 1551/2742 [38:00<20:19,  1.02s/it] 57%|█████▋    | 1552/2742 [38:01<21:05,  1.06s/it] 57%|█████▋    | 1553/2742 [38:02<20:09,  1.02s/it] 57%|█████▋    | 1554/2742 [38:03<19:46,  1.00it/s] 57%|█████▋    | 1555/2742 [38:04<20:03,  1.01s/it]                                                   {'loss': 1.227, 'learning_rate': 1.982462377792024e-05, 'epoch': 1.7}
 57%|█████▋    | 1555/2742 [38:04<20:03,  1.01s/it] 57%|█████▋    | 1556/2742 [38:05<19:46,  1.00s/it] 57%|█████▋    | 1557/2742 [38:06<19:31,  1.01it/s] 57%|█████▋    | 1558/2742 [38:07<18:51,  1.05it/s] 57%|█████▋    | 1559/2742 [38:08<19:29,  1.01it/s] 57%|█████▋    | 1560/2742 [38:09<21:12,  1.08s/it]                                                   {'loss': 1.1843, 'learning_rate': 1.9684595556072587e-05, 'epoch': 1.71}
 57%|█████▋    | 1560/2742 [38:09<21:12,  1.08s/it] 57%|█████▋    | 1561/2742 [38:10<20:32,  1.04s/it] 57%|█████▋    | 1562/2742 [38:11<20:22,  1.04s/it] 57%|█████▋    | 1563/2742 [38:12<19:45,  1.01s/it] 57%|█████▋    | 1564/2742 [38:13<19:45,  1.01s/it] 57%|█████▋    | 1565/2742 [38:14<20:04,  1.02s/it]                                                   {'loss': 1.0681, 'learning_rate': 1.954474177177585e-05, 'epoch': 1.71}
 57%|█████▋    | 1565/2742 [38:14<20:04,  1.02s/it] 57%|█████▋    | 1566/2742 [38:15<19:29,  1.01it/s] 57%|█████▋    | 1567/2742 [38:16<18:56,  1.03it/s] 57%|█████▋    | 1568/2742 [38:17<18:42,  1.05it/s] 57%|█████▋    | 1569/2742 [38:18<18:47,  1.04it/s] 57%|█████▋    | 1570/2742 [38:19<18:57,  1.03it/s]                                                   {'loss': 1.2198, 'learning_rate': 1.9405067014662287e-05, 'epoch': 1.72}
 57%|█████▋    | 1570/2742 [38:19<18:57,  1.03it/s] 57%|█████▋    | 1571/2742 [38:20<19:49,  1.02s/it] 57%|█████▋    | 1572/2742 [38:21<20:35,  1.06s/it] 57%|█████▋    | 1573/2742 [38:22<20:03,  1.03s/it] 57%|█████▋    | 1574/2742 [38:23<19:30,  1.00s/it] 57%|█████▋    | 1575/2742 [38:24<19:48,  1.02s/it]                                                   {'loss': 1.3283, 'learning_rate': 1.9265575868488924e-05, 'epoch': 1.72}
 57%|█████▋    | 1575/2742 [38:24<19:48,  1.02s/it] 57%|█████▋    | 1576/2742 [38:25<19:42,  1.01s/it] 58%|█████▊    | 1577/2742 [38:26<19:10,  1.01it/s] 58%|█████▊    | 1578/2742 [38:27<19:05,  1.02it/s] 58%|█████▊    | 1579/2742 [38:28<18:36,  1.04it/s] 58%|█████▊    | 1580/2742 [38:29<18:42,  1.04it/s]                                                   {'loss': 1.1837, 'learning_rate': 1.912627291098719e-05, 'epoch': 1.73}
 58%|█████▊    | 1580/2742 [38:29<18:42,  1.04it/s] 58%|█████▊    | 1581/2742 [38:30<19:01,  1.02it/s] 58%|█████▊    | 1582/2742 [38:31<19:47,  1.02s/it] 58%|█████▊    | 1583/2742 [38:32<19:45,  1.02s/it] 58%|█████▊    | 1584/2742 [38:33<19:21,  1.00s/it] 58%|█████▊    | 1585/2742 [38:34<19:46,  1.03s/it]                                                   {'loss': 1.3933, 'learning_rate': 1.898716271371263e-05, 'epoch': 1.73}
 58%|█████▊    | 1585/2742 [38:34<19:46,  1.03s/it] 58%|█████▊    | 1586/2742 [38:35<20:33,  1.07s/it] 58%|█████▊    | 1587/2742 [38:37<21:33,  1.12s/it] 58%|█████▊    | 1588/2742 [38:38<21:31,  1.12s/it] 58%|█████▊    | 1589/2742 [38:39<20:27,  1.06s/it] 58%|█████▊    | 1590/2742 [38:40<19:36,  1.02s/it]                                                   {'loss': 1.2073, 'learning_rate': 1.8848249841894945e-05, 'epoch': 1.74}
 58%|█████▊    | 1590/2742 [38:40<19:36,  1.02s/it] 58%|█████▊    | 1591/2742 [38:41<19:38,  1.02s/it] 58%|█████▊    | 1592/2742 [38:42<19:15,  1.00s/it] 58%|█████▊    | 1593/2742 [38:43<19:42,  1.03s/it] 58%|█████▊    | 1594/2742 [38:44<19:18,  1.01s/it] 58%|█████▊    | 1595/2742 [38:45<18:50,  1.01it/s]                                                   {'loss': 1.3581, 'learning_rate': 1.8709538854288088e-05, 'epoch': 1.74}
 58%|█████▊    | 1595/2742 [38:45<18:50,  1.01it/s] 58%|█████▊    | 1596/2742 [38:46<18:26,  1.04it/s] 58%|█████▊    | 1597/2742 [38:47<18:45,  1.02it/s] 58%|█████▊    | 1598/2742 [38:48<18:50,  1.01it/s] 58%|█████▊    | 1599/2742 [38:49<18:26,  1.03it/s] 58%|█████▊    | 1600/2742 [38:50<18:33,  1.03it/s]                                                   {'loss': 1.2821, 'learning_rate': 1.857103430302074e-05, 'epoch': 1.75}
 58%|█████▊    | 1600/2742 [38:50<18:33,  1.03it/s][INFO|trainer.py:2881] 2023-11-06 14:37:09,042 >> Saving model checkpoint to saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1600
[INFO|tokenization_utils_base.py:2428] 2023-11-06 14:37:09,083 >> tokenizer config file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-06 14:37:09,083 >> Special tokens file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1600/special_tokens_map.json
[2023-11-06 14:37:09,738] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1600 is about to be saved!
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-11-06 14:37:16,684] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1600/global_step1600/mp_rank_00_model_states.pt
[2023-11-06 14:37:16,684] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1600/global_step1600/mp_rank_00_model_states.pt...
[2023-11-06 14:37:48,918] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1600/global_step1600/mp_rank_00_model_states.pt.
[2023-11-06 14:37:49,442] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1600/global_step1600/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 14:37:49,520] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1600/global_step1600/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 14:37:49,520] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1600/global_step1600/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-11-06 14:37:49,520] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1600 is ready now!
 58%|█████▊    | 1601/2742 [39:40<4:58:55, 15.72s/it] 58%|█████▊    | 1602/2742 [39:41<3:34:21, 11.28s/it] 58%|█████▊    | 1603/2742 [39:42<2:37:17,  8.29s/it] 58%|█████▊    | 1604/2742 [39:43<1:55:26,  6.09s/it] 59%|█████▊    | 1605/2742 [39:44<1:26:44,  4.58s/it]                                                     {'loss': 1.259, 'learning_rate': 1.8432740733446886e-05, 'epoch': 1.75}
 59%|█████▊    | 1605/2742 [39:44<1:26:44,  4.58s/it] 59%|█████▊    | 1606/2742 [39:45<1:05:44,  3.47s/it] 59%|█████▊    | 1607/2742 [39:46<51:07,  2.70s/it]   59%|█████▊    | 1608/2742 [39:47<41:32,  2.20s/it] 59%|█████▊    | 1609/2742 [39:48<34:24,  1.82s/it] 59%|█████▊    | 1610/2742 [39:49<29:17,  1.55s/it]                                                   {'loss': 1.203, 'learning_rate': 1.829466268399662e-05, 'epoch': 1.76}
 59%|█████▊    | 1610/2742 [39:49<29:17,  1.55s/it] 59%|█████▉    | 1611/2742 [39:49<25:50,  1.37s/it] 59%|█████▉    | 1612/2742 [39:50<23:08,  1.23s/it] 59%|█████▉    | 1613/2742 [39:51<21:45,  1.16s/it] 59%|█████▉    | 1614/2742 [39:52<20:43,  1.10s/it] 59%|█████▉    | 1615/2742 [39:54<21:21,  1.14s/it]                                                   {'loss': 1.068, 'learning_rate': 1.8156804686027267e-05, 'epoch': 1.77}
 59%|█████▉    | 1615/2742 [39:54<21:21,  1.14s/it] 59%|█████▉    | 1616/2742 [39:55<21:39,  1.15s/it] 59%|█████▉    | 1617/2742 [39:56<21:11,  1.13s/it] 59%|█████▉    | 1618/2742 [39:57<22:20,  1.19s/it] 59%|█████▉    | 1619/2742 [39:58<21:39,  1.16s/it] 59%|█████▉    | 1620/2742 [39:59<20:29,  1.10s/it]                                                   {'loss': 1.1301, 'learning_rate': 1.801917126367461e-05, 'epoch': 1.77}
 59%|█████▉    | 1620/2742 [39:59<20:29,  1.10s/it] 59%|█████▉    | 1621/2742 [40:00<19:48,  1.06s/it] 59%|█████▉    | 1622/2742 [40:01<19:36,  1.05s/it] 59%|█████▉    | 1623/2742 [40:02<18:43,  1.00s/it] 59%|█████▉    | 1624/2742 [40:03<18:31,  1.01it/s] 59%|█████▉    | 1625/2742 [40:04<18:12,  1.02it/s]                                                   {'loss': 1.13, 'learning_rate': 1.7881766933704495e-05, 'epoch': 1.78}
 59%|█████▉    | 1625/2742 [40:04<18:12,  1.02it/s] 59%|█████▉    | 1626/2742 [40:05<18:08,  1.03it/s] 59%|█████▉    | 1627/2742 [40:06<18:16,  1.02it/s] 59%|█████▉    | 1628/2742 [40:07<18:05,  1.03it/s] 59%|█████▉    | 1629/2742 [40:08<18:21,  1.01it/s] 59%|█████▉    | 1630/2742 [40:09<17:50,  1.04it/s]                                                   {'loss': 1.0264, 'learning_rate': 1.7744596205364505e-05, 'epoch': 1.78}
 59%|█████▉    | 1630/2742 [40:09<17:50,  1.04it/s] 59%|█████▉    | 1631/2742 [40:10<18:05,  1.02it/s] 60%|█████▉    | 1632/2742 [40:11<18:03,  1.02it/s] 60%|█████▉    | 1633/2742 [40:12<18:27,  1.00it/s] 60%|█████▉    | 1634/2742 [40:13<18:35,  1.01s/it] 60%|█████▉    | 1635/2742 [40:14<18:45,  1.02s/it]                                                   {'loss': 1.2065, 'learning_rate': 1.7607663580236095e-05, 'epoch': 1.79}
 60%|█████▉    | 1635/2742 [40:14<18:45,  1.02s/it] 60%|█████▉    | 1636/2742 [40:15<19:15,  1.04s/it] 60%|█████▉    | 1637/2742 [40:16<19:09,  1.04s/it] 60%|█████▉    | 1638/2742 [40:17<19:16,  1.05s/it] 60%|█████▉    | 1639/2742 [40:18<18:45,  1.02s/it] 60%|█████▉    | 1640/2742 [40:19<18:18,  1.00it/s]                                                   {'loss': 1.3065, 'learning_rate': 1.7470973552086756e-05, 'epoch': 1.79}
 60%|█████▉    | 1640/2742 [40:19<18:18,  1.00it/s] 60%|█████▉    | 1641/2742 [40:20<18:56,  1.03s/it] 60%|█████▉    | 1642/2742 [40:21<18:22,  1.00s/it] 60%|█████▉    | 1643/2742 [40:22<17:53,  1.02it/s] 60%|█████▉    | 1644/2742 [40:23<18:37,  1.02s/it] 60%|█████▉    | 1645/2742 [40:24<17:54,  1.02it/s]                                                   {'loss': 1.2904, 'learning_rate': 1.7334530606722592e-05, 'epoch': 1.8}
 60%|█████▉    | 1645/2742 [40:24<17:54,  1.02it/s] 60%|██████    | 1646/2742 [40:25<18:03,  1.01it/s] 60%|██████    | 1647/2742 [40:26<17:43,  1.03it/s] 60%|██████    | 1648/2742 [40:27<19:24,  1.06s/it] 60%|██████    | 1649/2742 [40:28<19:03,  1.05s/it] 60%|██████    | 1650/2742 [40:29<18:50,  1.04s/it]                                                   {'loss': 1.1876, 'learning_rate': 1.7198339221841135e-05, 'epoch': 1.8}
 60%|██████    | 1650/2742 [40:29<18:50,  1.04s/it] 60%|██████    | 1651/2742 [40:30<18:21,  1.01s/it] 60%|██████    | 1652/2742 [40:31<17:54,  1.01it/s] 60%|██████    | 1653/2742 [40:32<17:34,  1.03it/s] 60%|██████    | 1654/2742 [40:33<17:51,  1.01it/s] 60%|██████    | 1655/2742 [40:34<17:34,  1.03it/s]                                                   {'loss': 1.1413, 'learning_rate': 1.706240386688432e-05, 'epoch': 1.81}
 60%|██████    | 1655/2742 [40:34<17:34,  1.03it/s] 60%|██████    | 1656/2742 [40:35<17:15,  1.05it/s] 60%|██████    | 1657/2742 [40:36<17:25,  1.04it/s] 60%|██████    | 1658/2742 [40:37<17:18,  1.04it/s] 61%|██████    | 1659/2742 [40:38<17:40,  1.02it/s] 61%|██████    | 1660/2742 [40:39<17:24,  1.04it/s]                                                   {'loss': 1.1512, 'learning_rate': 1.6926729002891877e-05, 'epoch': 1.81}
 61%|██████    | 1660/2742 [40:39<17:24,  1.04it/s] 61%|██████    | 1661/2742 [40:40<17:05,  1.05it/s] 61%|██████    | 1662/2742 [40:41<16:54,  1.06it/s] 61%|██████    | 1663/2742 [40:42<16:56,  1.06it/s] 61%|██████    | 1664/2742 [40:43<16:59,  1.06it/s] 61%|██████    | 1665/2742 [40:44<17:26,  1.03it/s]                                                   {'loss': 1.2905, 'learning_rate': 1.6791319082354925e-05, 'epoch': 1.82}
 61%|██████    | 1665/2742 [40:44<17:26,  1.03it/s] 61%|██████    | 1666/2742 [40:45<17:54,  1.00it/s] 61%|██████    | 1667/2742 [40:46<17:25,  1.03it/s] 61%|██████    | 1668/2742 [40:47<17:13,  1.04it/s] 61%|██████    | 1669/2742 [40:47<17:04,  1.05it/s] 61%|██████    | 1670/2742 [40:48<17:03,  1.05it/s]                                                   {'loss': 1.3271, 'learning_rate': 1.665617854906981e-05, 'epoch': 1.83}
 61%|██████    | 1670/2742 [40:48<17:03,  1.05it/s] 61%|██████    | 1671/2742 [40:50<18:01,  1.01s/it] 61%|██████    | 1672/2742 [40:51<18:17,  1.03s/it] 61%|██████    | 1673/2742 [40:52<18:26,  1.03s/it] 61%|██████    | 1674/2742 [40:53<17:56,  1.01s/it] 61%|██████    | 1675/2742 [40:54<17:43,  1.00it/s]                                                   {'loss': 1.1081, 'learning_rate': 1.6521311837992315e-05, 'epoch': 1.83}
 61%|██████    | 1675/2742 [40:54<17:43,  1.00it/s] 61%|██████    | 1676/2742 [40:55<17:31,  1.01it/s] 61%|██████    | 1677/2742 [40:56<17:23,  1.02it/s] 61%|██████    | 1678/2742 [40:57<17:23,  1.02it/s] 61%|██████    | 1679/2742 [40:58<17:31,  1.01it/s] 61%|██████▏   | 1680/2742 [40:59<17:56,  1.01s/it]                                                   {'loss': 1.1205, 'learning_rate': 1.6386723375092084e-05, 'epoch': 1.84}
 61%|██████▏   | 1680/2742 [40:59<17:56,  1.01s/it] 61%|██████▏   | 1681/2742 [41:00<17:50,  1.01s/it] 61%|██████▏   | 1682/2742 [41:01<17:31,  1.01it/s] 61%|██████▏   | 1683/2742 [41:02<17:27,  1.01it/s] 61%|██████▏   | 1684/2742 [41:02<17:00,  1.04it/s] 61%|██████▏   | 1685/2742 [41:03<16:54,  1.04it/s]                                                   {'loss': 1.5053, 'learning_rate': 1.6252417577207424e-05, 'epoch': 1.84}
 61%|██████▏   | 1685/2742 [41:03<16:54,  1.04it/s] 61%|██████▏   | 1686/2742 [41:04<17:08,  1.03it/s] 62%|██████▏   | 1687/2742 [41:05<17:15,  1.02it/s] 62%|██████▏   | 1688/2742 [41:06<16:54,  1.04it/s] 62%|██████▏   | 1689/2742 [41:07<16:59,  1.03it/s] 62%|██████▏   | 1690/2742 [41:08<16:36,  1.06it/s]                                                   {'loss': 1.2253, 'learning_rate': 1.6118398851900286e-05, 'epoch': 1.85}
 62%|██████▏   | 1690/2742 [41:08<16:36,  1.06it/s] 62%|██████▏   | 1691/2742 [41:09<17:49,  1.02s/it] 62%|██████▏   | 1692/2742 [41:10<17:24,  1.01it/s] 62%|██████▏   | 1693/2742 [41:11<17:04,  1.02it/s] 62%|██████▏   | 1694/2742 [41:12<16:41,  1.05it/s] 62%|██████▏   | 1695/2742 [41:13<16:34,  1.05it/s]                                                   {'loss': 1.0717, 'learning_rate': 1.5984671597311666e-05, 'epoch': 1.85}
 62%|██████▏   | 1695/2742 [41:13<16:34,  1.05it/s] 62%|██████▏   | 1696/2742 [41:14<16:47,  1.04it/s] 62%|██████▏   | 1697/2742 [41:15<16:46,  1.04it/s] 62%|██████▏   | 1698/2742 [41:16<16:59,  1.02it/s] 62%|██████▏   | 1699/2742 [41:17<16:50,  1.03it/s] 62%|██████▏   | 1700/2742 [41:18<17:09,  1.01it/s]                                                   {'loss': 1.3303, 'learning_rate': 1.5851240202017284e-05, 'epoch': 1.86}
 62%|██████▏   | 1700/2742 [41:18<17:09,  1.01it/s][INFO|trainer.py:2881] 2023-11-06 14:39:37,771 >> Saving model checkpoint to saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1700
[INFO|tokenization_utils_base.py:2428] 2023-11-06 14:39:37,816 >> tokenizer config file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-06 14:39:37,816 >> Special tokens file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1700/special_tokens_map.json
[2023-11-06 14:39:38,662] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1700 is about to be saved!
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-11-06 14:39:45,848] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1700/global_step1700/mp_rank_00_model_states.pt
[2023-11-06 14:39:45,849] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1700/global_step1700/mp_rank_00_model_states.pt...
[2023-11-06 14:40:16,851] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1700/global_step1700/mp_rank_00_model_states.pt.
[2023-11-06 14:40:17,393] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1700/global_step1700/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 14:40:17,457] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1700/global_step1700/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 14:40:17,458] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1700/global_step1700/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-11-06 14:40:17,458] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1700 is ready now!
 62%|██████▏   | 1701/2742 [42:07<4:28:41, 15.49s/it] 62%|██████▏   | 1702/2742 [42:08<3:12:53, 11.13s/it] 62%|██████▏   | 1703/2742 [42:09<2:19:41,  8.07s/it] 62%|██████▏   | 1704/2742 [42:10<1:42:53,  5.95s/it] 62%|██████▏   | 1705/2742 [42:11<1:17:20,  4.48s/it]                                                     {'loss': 1.1504, 'learning_rate': 1.5718109044883502e-05, 'epoch': 1.86}
 62%|██████▏   | 1705/2742 [42:11<1:17:20,  4.48s/it] 62%|██████▏   | 1706/2742 [42:12<59:38,  3.45s/it]   62%|██████▏   | 1707/2742 [42:13<46:21,  2.69s/it] 62%|██████▏   | 1708/2742 [42:14<37:16,  2.16s/it] 62%|██████▏   | 1709/2742 [42:15<31:00,  1.80s/it] 62%|██████▏   | 1710/2742 [42:16<26:57,  1.57s/it]                                                   {'loss': 1.101, 'learning_rate': 1.5585282494923688e-05, 'epoch': 1.87}
 62%|██████▏   | 1710/2742 [42:16<26:57,  1.57s/it] 62%|██████▏   | 1711/2742 [42:17<24:08,  1.41s/it] 62%|██████▏   | 1712/2742 [42:18<21:58,  1.28s/it] 62%|██████▏   | 1713/2742 [42:19<20:21,  1.19s/it] 63%|██████▎   | 1714/2742 [42:20<19:06,  1.11s/it] 63%|██████▎   | 1715/2742 [42:21<18:49,  1.10s/it]                                                   {'loss': 1.4694, 'learning_rate': 1.545276491115477e-05, 'epoch': 1.87}
 63%|██████▎   | 1715/2742 [42:21<18:49,  1.10s/it] 63%|██████▎   | 1716/2742 [42:22<18:03,  1.06s/it] 63%|██████▎   | 1717/2742 [42:23<17:35,  1.03s/it] 63%|██████▎   | 1718/2742 [42:24<18:05,  1.06s/it] 63%|██████▎   | 1719/2742 [42:25<17:46,  1.04s/it] 63%|██████▎   | 1720/2742 [42:26<17:38,  1.04s/it]                                                   {'loss': 1.379, 'learning_rate': 1.5320560642454264e-05, 'epoch': 1.88}
 63%|██████▎   | 1720/2742 [42:26<17:38,  1.04s/it] 63%|██████▎   | 1721/2742 [42:27<18:04,  1.06s/it] 63%|██████▎   | 1722/2742 [42:28<18:29,  1.09s/it] 63%|██████▎   | 1723/2742 [42:29<18:01,  1.06s/it] 63%|██████▎   | 1724/2742 [42:30<17:23,  1.03s/it] 63%|██████▎   | 1725/2742 [42:31<16:43,  1.01it/s]                                                   {'loss': 1.2228, 'learning_rate': 1.5188674027417499e-05, 'epoch': 1.89}
 63%|██████▎   | 1725/2742 [42:31<16:43,  1.01it/s] 63%|██████▎   | 1726/2742 [42:32<16:39,  1.02it/s] 63%|██████▎   | 1727/2742 [42:33<16:27,  1.03it/s] 63%|██████▎   | 1728/2742 [42:34<16:19,  1.03it/s] 63%|██████▎   | 1729/2742 [42:35<16:53,  1.00s/it] 63%|██████▎   | 1730/2742 [42:36<16:30,  1.02it/s]                                                   {'loss': 1.0892, 'learning_rate': 1.5057109394215219e-05, 'epoch': 1.89}
 63%|██████▎   | 1730/2742 [42:36<16:30,  1.02it/s] 63%|██████▎   | 1731/2742 [42:37<16:47,  1.00it/s] 63%|██████▎   | 1732/2742 [42:38<16:36,  1.01it/s] 63%|██████▎   | 1733/2742 [42:39<16:05,  1.04it/s] 63%|██████▎   | 1734/2742 [42:40<16:10,  1.04it/s] 63%|██████▎   | 1735/2742 [42:41<16:17,  1.03it/s]                                                   {'loss': 1.2746, 'learning_rate': 1.4925871060451616e-05, 'epoch': 1.9}
 63%|██████▎   | 1735/2742 [42:41<16:17,  1.03it/s] 63%|██████▎   | 1736/2742 [42:42<16:10,  1.04it/s] 63%|██████▎   | 1737/2742 [42:43<15:57,  1.05it/s] 63%|██████▎   | 1738/2742 [42:44<15:52,  1.05it/s][2023-11-06 14:40:56,119] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
 63%|██████▎   | 1739/2742 [42:45<16:24,  1.02it/s] 63%|██████▎   | 1740/2742 [42:46<16:11,  1.03it/s]                                                   {'loss': 1.4239, 'learning_rate': 1.4821118223634012e-05, 'epoch': 1.9}
 63%|██████▎   | 1740/2742 [42:46<16:11,  1.03it/s] 63%|██████▎   | 1741/2742 [42:47<16:09,  1.03it/s] 64%|██████▎   | 1742/2742 [42:48<15:58,  1.04it/s] 64%|██████▎   | 1743/2742 [42:49<15:46,  1.06it/s] 64%|██████▎   | 1744/2742 [42:50<15:46,  1.05it/s] 64%|██████▎   | 1745/2742 [42:51<15:39,  1.06it/s]                                                   {'loss': 1.0837, 'learning_rate': 1.4690478074951426e-05, 'epoch': 1.91}
 64%|██████▎   | 1745/2742 [42:51<15:39,  1.06it/s] 64%|██████▎   | 1746/2742 [42:52<16:04,  1.03it/s] 64%|██████▎   | 1747/2742 [42:53<15:50,  1.05it/s] 64%|██████▎   | 1748/2742 [42:54<15:52,  1.04it/s] 64%|██████▍   | 1749/2742 [42:55<17:33,  1.06s/it] 64%|██████▍   | 1750/2742 [42:56<16:45,  1.01s/it]                                                   {'loss': 1.271, 'learning_rate': 1.456017625758017e-05, 'epoch': 1.91}
 64%|██████▍   | 1750/2742 [42:56<16:45,  1.01s/it] 64%|██████▍   | 1751/2742 [42:57<16:26,  1.00it/s] 64%|██████▍   | 1752/2742 [42:58<17:39,  1.07s/it] 64%|██████▍   | 1753/2742 [42:59<16:50,  1.02s/it] 64%|██████▍   | 1754/2742 [43:00<16:38,  1.01s/it] 64%|██████▍   | 1755/2742 [43:01<17:36,  1.07s/it]                                                   {'loss': 1.3751, 'learning_rate': 1.4430217047682132e-05, 'epoch': 1.92}
 64%|██████▍   | 1755/2742 [43:01<17:36,  1.07s/it] 64%|██████▍   | 1756/2742 [43:02<16:44,  1.02s/it] 64%|██████▍   | 1757/2742 [43:03<16:06,  1.02it/s] 64%|██████▍   | 1758/2742 [43:04<15:48,  1.04it/s] 64%|██████▍   | 1759/2742 [43:05<16:08,  1.01it/s] 64%|██████▍   | 1760/2742 [43:06<15:53,  1.03it/s]                                                   {'loss': 1.1729, 'learning_rate': 1.430060471017573e-05, 'epoch': 1.92}
 64%|██████▍   | 1760/2742 [43:06<15:53,  1.03it/s] 64%|██████▍   | 1761/2742 [43:07<15:50,  1.03it/s] 64%|██████▍   | 1762/2742 [43:08<16:26,  1.01s/it] 64%|██████▍   | 1763/2742 [43:09<16:37,  1.02s/it] 64%|██████▍   | 1764/2742 [43:10<16:26,  1.01s/it] 64%|██████▍   | 1765/2742 [43:11<15:54,  1.02it/s]                                                   {'loss': 1.3652, 'learning_rate': 1.4171343498595934e-05, 'epoch': 1.93}
 64%|██████▍   | 1765/2742 [43:11<15:54,  1.02it/s] 64%|██████▍   | 1766/2742 [43:12<16:01,  1.02it/s] 64%|██████▍   | 1767/2742 [43:13<15:44,  1.03it/s] 64%|██████▍   | 1768/2742 [43:14<15:23,  1.06it/s] 65%|██████▍   | 1769/2742 [43:14<15:25,  1.05it/s] 65%|██████▍   | 1770/2742 [43:15<15:25,  1.05it/s]                                                   {'loss': 1.0981, 'learning_rate': 1.4042437654954698e-05, 'epoch': 1.93}
 65%|██████▍   | 1770/2742 [43:15<15:25,  1.05it/s] 65%|██████▍   | 1771/2742 [43:16<15:49,  1.02it/s] 65%|██████▍   | 1772/2742 [43:17<15:25,  1.05it/s] 65%|██████▍   | 1773/2742 [43:18<15:25,  1.05it/s] 65%|██████▍   | 1774/2742 [43:19<15:12,  1.06it/s] 65%|██████▍   | 1775/2742 [43:20<15:09,  1.06it/s]                                                   {'loss': 1.2622, 'learning_rate': 1.3913891409601748e-05, 'epoch': 1.94}
 65%|██████▍   | 1775/2742 [43:20<15:09,  1.06it/s] 65%|██████▍   | 1776/2742 [43:21<15:50,  1.02it/s] 65%|██████▍   | 1777/2742 [43:22<15:52,  1.01it/s] 65%|██████▍   | 1778/2742 [43:23<15:33,  1.03it/s] 65%|██████▍   | 1779/2742 [43:24<15:18,  1.05it/s] 65%|██████▍   | 1780/2742 [43:25<15:11,  1.06it/s]                                                   {'loss': 1.4418, 'learning_rate': 1.3785708981085721e-05, 'epoch': 1.95}
 65%|██████▍   | 1780/2742 [43:25<15:11,  1.06it/s] 65%|██████▍   | 1781/2742 [43:26<15:10,  1.06it/s] 65%|██████▍   | 1782/2742 [43:27<15:09,  1.06it/s] 65%|██████▌   | 1783/2742 [43:28<15:23,  1.04it/s] 65%|██████▌   | 1784/2742 [43:29<15:04,  1.06it/s] 65%|██████▌   | 1785/2742 [43:30<14:59,  1.06it/s]                                                   {'loss': 1.3805, 'learning_rate': 1.365789457601575e-05, 'epoch': 1.95}
 65%|██████▌   | 1785/2742 [43:30<14:59,  1.06it/s] 65%|██████▌   | 1786/2742 [43:31<15:12,  1.05it/s] 65%|██████▌   | 1787/2742 [43:32<14:52,  1.07it/s] 65%|██████▌   | 1788/2742 [43:33<15:17,  1.04it/s] 65%|██████▌   | 1789/2742 [43:34<15:33,  1.02it/s] 65%|██████▌   | 1790/2742 [43:35<15:22,  1.03it/s]                                                   {'loss': 1.1882, 'learning_rate': 1.353045238892341e-05, 'epoch': 1.96}
 65%|██████▌   | 1790/2742 [43:35<15:22,  1.03it/s] 65%|██████▌   | 1791/2742 [43:36<15:42,  1.01it/s] 65%|██████▌   | 1792/2742 [43:37<15:53,  1.00s/it] 65%|██████▌   | 1793/2742 [43:38<16:01,  1.01s/it] 65%|██████▌   | 1794/2742 [43:39<15:41,  1.01it/s] 65%|██████▌   | 1795/2742 [43:40<15:36,  1.01it/s]                                                   {'loss': 1.2638, 'learning_rate': 1.3403386602125073e-05, 'epoch': 1.96}
 65%|██████▌   | 1795/2742 [43:40<15:36,  1.01it/s] 65%|██████▌   | 1796/2742 [43:41<15:37,  1.01it/s] 66%|██████▌   | 1797/2742 [43:42<16:39,  1.06s/it] 66%|██████▌   | 1798/2742 [43:43<16:34,  1.05s/it] 66%|██████▌   | 1799/2742 [43:44<16:32,  1.05s/it] 66%|██████▌   | 1800/2742 [43:45<17:06,  1.09s/it]                                                   {'loss': 1.2007, 'learning_rate': 1.327670138558465e-05, 'epoch': 1.97}
 66%|██████▌   | 1800/2742 [43:45<17:06,  1.09s/it][INFO|trainer.py:2881] 2023-11-06 14:42:04,941 >> Saving model checkpoint to saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1800
[INFO|tokenization_utils_base.py:2428] 2023-11-06 14:42:04,982 >> tokenizer config file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-06 14:42:04,982 >> Special tokens file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1800/special_tokens_map.json
[2023-11-06 14:42:05,989] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1800 is about to be saved!
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-11-06 14:42:13,345] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1800/global_step1800/mp_rank_00_model_states.pt
[2023-11-06 14:42:13,345] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1800/global_step1800/mp_rank_00_model_states.pt...
[2023-11-06 14:42:46,641] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1800/global_step1800/mp_rank_00_model_states.pt.
[2023-11-06 14:42:47,225] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1800/global_step1800/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 14:42:47,289] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1800/global_step1800/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 14:42:47,290] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1800/global_step1800/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-11-06 14:42:47,290] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1800 is ready now!
 66%|██████▌   | 1801/2742 [44:37<4:16:29, 16.35s/it] 66%|██████▌   | 1802/2742 [44:38<3:03:55, 11.74s/it] 66%|██████▌   | 1803/2742 [44:39<2:12:56,  8.49s/it] 66%|██████▌   | 1804/2742 [44:40<1:37:47,  6.26s/it] 66%|██████▌   | 1805/2742 [44:41<1:13:53,  4.73s/it]                                                     {'loss': 1.0785, 'learning_rate': 1.3150400896776716e-05, 'epoch': 1.97}
 66%|██████▌   | 1805/2742 [44:41<1:13:53,  4.73s/it] 66%|██████▌   | 1806/2742 [44:42<56:06,  3.60s/it]   66%|██████▌   | 1807/2742 [44:43<43:39,  2.80s/it] 66%|██████▌   | 1808/2742 [44:44<35:09,  2.26s/it] 66%|██████▌   | 1809/2742 [44:45<28:53,  1.86s/it] 66%|██████▌   | 1810/2742 [44:46<24:40,  1.59s/it]                                                   {'loss': 1.1365, 'learning_rate': 1.3024489280550123e-05, 'epoch': 1.98}
 66%|██████▌   | 1810/2742 [44:46<24:40,  1.59s/it] 66%|██████▌   | 1811/2742 [44:47<21:48,  1.41s/it] 66%|██████▌   | 1812/2742 [44:48<20:50,  1.34s/it] 66%|██████▌   | 1813/2742 [44:49<19:17,  1.25s/it] 66%|██████▌   | 1814/2742 [44:51<19:55,  1.29s/it] 66%|██████▌   | 1815/2742 [44:52<18:53,  1.22s/it]                                                   {'loss': 1.1058, 'learning_rate': 1.2898970668991939e-05, 'epoch': 1.98}
 66%|██████▌   | 1815/2742 [44:52<18:53,  1.22s/it] 66%|██████▌   | 1816/2742 [44:53<17:40,  1.15s/it] 66%|██████▋   | 1817/2742 [44:54<16:50,  1.09s/it] 66%|██████▋   | 1818/2742 [44:55<16:16,  1.06s/it] 66%|██████▋   | 1819/2742 [44:56<16:33,  1.08s/it] 66%|██████▋   | 1820/2742 [44:57<15:56,  1.04s/it]                                                   {'loss': 1.2463, 'learning_rate': 1.277384918129185e-05, 'epoch': 1.99}
 66%|██████▋   | 1820/2742 [44:57<15:56,  1.04s/it] 66%|██████▋   | 1821/2742 [44:58<15:33,  1.01s/it] 66%|██████▋   | 1822/2742 [44:58<15:07,  1.01it/s] 66%|██████▋   | 1823/2742 [44:59<14:47,  1.04it/s] 67%|██████▋   | 1824/2742 [45:00<14:44,  1.04it/s] 67%|██████▋   | 1825/2742 [45:01<14:45,  1.04it/s]                                                   {'loss': 1.1279, 'learning_rate': 1.2649128923606984e-05, 'epoch': 2.0}
 67%|██████▋   | 1825/2742 [45:01<14:45,  1.04it/s] 67%|██████▋   | 1826/2742 [45:02<14:47,  1.03it/s] 67%|██████▋   | 1827/2742 [45:03<15:04,  1.01it/s] 67%|██████▋   | 1828/2742 [45:04<15:17,  1.00s/it] 67%|██████▋   | 1829/2742 [45:05<15:13,  1.00s/it] 67%|██████▋   | 1830/2742 [45:07<15:50,  1.04s/it]                                                   {'loss': 1.3352, 'learning_rate': 1.2524813988927176e-05, 'epoch': 2.0}
 67%|██████▋   | 1830/2742 [45:07<15:50,  1.04s/it] 67%|██████▋   | 1831/2742 [45:07<15:28,  1.02s/it] 67%|██████▋   | 1832/2742 [45:08<15:21,  1.01s/it] 67%|██████▋   | 1833/2742 [45:10<15:22,  1.01s/it] 67%|██████▋   | 1834/2742 [45:11<15:15,  1.01s/it] 67%|██████▋   | 1835/2742 [45:11<15:06,  1.00it/s]                                                   {'loss': 1.3973, 'learning_rate': 1.2400908456940621e-05, 'epoch': 2.01}
 67%|██████▋   | 1835/2742 [45:11<15:06,  1.00it/s] 67%|██████▋   | 1836/2742 [45:12<14:56,  1.01it/s] 67%|██████▋   | 1837/2742 [45:13<14:49,  1.02it/s] 67%|██████▋   | 1838/2742 [45:15<15:16,  1.01s/it] 67%|██████▋   | 1839/2742 [45:16<15:13,  1.01s/it] 67%|██████▋   | 1840/2742 [45:16<14:52,  1.01it/s]                                                   {'loss': 1.2038, 'learning_rate': 1.2277416393899988e-05, 'epoch': 2.01}
 67%|██████▋   | 1840/2742 [45:16<14:52,  1.01it/s] 67%|██████▋   | 1841/2742 [45:17<14:36,  1.03it/s] 67%|██████▋   | 1842/2742 [45:18<14:47,  1.01it/s] 67%|██████▋   | 1843/2742 [45:19<14:33,  1.03it/s] 67%|██████▋   | 1844/2742 [45:20<14:27,  1.03it/s] 67%|██████▋   | 1845/2742 [45:21<14:29,  1.03it/s]                                                   {'loss': 1.3603, 'learning_rate': 1.2154341852489002e-05, 'epoch': 2.02}
 67%|██████▋   | 1845/2742 [45:21<14:29,  1.03it/s] 67%|██████▋   | 1846/2742 [45:22<14:24,  1.04it/s] 67%|██████▋   | 1847/2742 [45:23<14:23,  1.04it/s] 67%|██████▋   | 1848/2742 [45:24<14:22,  1.04it/s] 67%|██████▋   | 1849/2742 [45:25<14:15,  1.04it/s] 67%|██████▋   | 1850/2742 [45:26<14:10,  1.05it/s]                                                   {'loss': 1.1206, 'learning_rate': 1.2031688871689415e-05, 'epoch': 2.02}
 67%|██████▋   | 1850/2742 [45:26<14:10,  1.05it/s] 68%|██████▊   | 1851/2742 [45:27<14:52,  1.00s/it] 68%|██████▊   | 1852/2742 [45:28<14:49,  1.00it/s] 68%|██████▊   | 1853/2742 [45:29<14:27,  1.02it/s] 68%|██████▊   | 1854/2742 [45:30<14:12,  1.04it/s] 68%|██████▊   | 1855/2742 [45:31<14:27,  1.02it/s]                                                   {'loss': 1.238, 'learning_rate': 1.1909461476648467e-05, 'epoch': 2.03}
 68%|██████▊   | 1855/2742 [45:31<14:27,  1.02it/s] 68%|██████▊   | 1856/2742 [45:32<14:40,  1.01it/s] 68%|██████▊   | 1857/2742 [45:33<14:19,  1.03it/s] 68%|██████▊   | 1858/2742 [45:34<14:14,  1.03it/s] 68%|██████▊   | 1859/2742 [45:35<14:56,  1.02s/it] 68%|██████▊   | 1860/2742 [45:36<15:06,  1.03s/it]                                                   {'loss': 1.24, 'learning_rate': 1.1787663678546828e-05, 'epoch': 2.03}
 68%|██████▊   | 1860/2742 [45:36<15:06,  1.03s/it] 68%|██████▊   | 1861/2742 [45:37<14:44,  1.00s/it] 68%|██████▊   | 1862/2742 [45:38<14:31,  1.01it/s] 68%|██████▊   | 1863/2742 [45:39<14:42,  1.00s/it] 68%|██████▊   | 1864/2742 [45:40<14:21,  1.02it/s] 68%|██████▊   | 1865/2742 [45:41<14:18,  1.02it/s]                                                   {'loss': 1.0129, 'learning_rate': 1.166629947446689e-05, 'epoch': 2.04}
 68%|██████▊   | 1865/2742 [45:41<14:18,  1.02it/s] 68%|██████▊   | 1866/2742 [45:42<14:25,  1.01it/s] 68%|██████▊   | 1867/2742 [45:43<14:01,  1.04it/s] 68%|██████▊   | 1868/2742 [45:44<14:27,  1.01it/s] 68%|██████▊   | 1869/2742 [45:45<14:31,  1.00it/s] 68%|██████▊   | 1870/2742 [45:46<14:11,  1.02it/s]                                                   {'loss': 1.2224, 'learning_rate': 1.1545372847261671e-05, 'epoch': 2.04}
 68%|██████▊   | 1870/2742 [45:46<14:11,  1.02it/s] 68%|██████▊   | 1871/2742 [45:47<14:20,  1.01it/s] 68%|██████▊   | 1872/2742 [45:48<14:19,  1.01it/s] 68%|██████▊   | 1873/2742 [45:49<13:54,  1.04it/s] 68%|██████▊   | 1874/2742 [45:50<13:54,  1.04it/s] 68%|██████▊   | 1875/2742 [45:51<14:14,  1.01it/s]                                                   {'loss': 1.1525, 'learning_rate': 1.1424887765424027e-05, 'epoch': 2.05}
 68%|██████▊   | 1875/2742 [45:51<14:14,  1.01it/s] 68%|██████▊   | 1876/2742 [45:52<15:19,  1.06s/it] 68%|██████▊   | 1877/2742 [45:53<15:01,  1.04s/it] 68%|██████▊   | 1878/2742 [45:54<15:29,  1.08s/it] 69%|██████▊   | 1879/2742 [45:55<14:45,  1.03s/it] 69%|██████▊   | 1880/2742 [45:56<15:39,  1.09s/it]                                                   {'loss': 1.2414, 'learning_rate': 1.130484818295651e-05, 'epoch': 2.06}
 69%|██████▊   | 1880/2742 [45:56<15:39,  1.09s/it] 69%|██████▊   | 1881/2742 [45:57<14:59,  1.04s/it] 69%|██████▊   | 1882/2742 [45:58<15:47,  1.10s/it] 69%|██████▊   | 1883/2742 [45:59<15:25,  1.08s/it] 69%|██████▊   | 1884/2742 [46:01<15:51,  1.11s/it] 69%|██████▊   | 1885/2742 [46:02<15:00,  1.05s/it]                                                   {'loss': 1.1227, 'learning_rate': 1.1185258039241506e-05, 'epoch': 2.06}
 69%|██████▊   | 1885/2742 [46:02<15:00,  1.05s/it] 69%|██████▉   | 1886/2742 [46:03<14:41,  1.03s/it] 69%|██████▉   | 1887/2742 [46:04<14:19,  1.01s/it] 69%|██████▉   | 1888/2742 [46:05<14:21,  1.01s/it] 69%|██████▉   | 1889/2742 [46:05<14:05,  1.01it/s] 69%|██████▉   | 1890/2742 [46:06<14:00,  1.01it/s]                                                   {'loss': 1.2221, 'learning_rate': 1.1066121258912058e-05, 'epoch': 2.07}
 69%|██████▉   | 1890/2742 [46:06<14:00,  1.01it/s] 69%|██████▉   | 1891/2742 [46:07<14:08,  1.00it/s] 69%|██████▉   | 1892/2742 [46:08<13:56,  1.02it/s] 69%|██████▉   | 1893/2742 [46:09<13:40,  1.03it/s] 69%|██████▉   | 1894/2742 [46:10<13:58,  1.01it/s] 69%|██████▉   | 1895/2742 [46:11<13:58,  1.01it/s]                                                   {'loss': 1.1967, 'learning_rate': 1.094744175172298e-05, 'epoch': 2.07}
 69%|██████▉   | 1895/2742 [46:11<13:58,  1.01it/s] 69%|██████▉   | 1896/2742 [46:12<13:42,  1.03it/s] 69%|██████▉   | 1897/2742 [46:13<14:02,  1.00it/s] 69%|██████▉   | 1898/2742 [46:14<13:44,  1.02it/s] 69%|██████▉   | 1899/2742 [46:15<14:13,  1.01s/it] 69%|██████▉   | 1900/2742 [46:17<14:45,  1.05s/it]                                                   {'loss': 1.1518, 'learning_rate': 1.0829223412422596e-05, 'epoch': 2.08}
 69%|██████▉   | 1900/2742 [46:17<14:45,  1.05s/it][INFO|trainer.py:2881] 2023-11-06 14:44:36,383 >> Saving model checkpoint to saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1900
[INFO|tokenization_utils_base.py:2428] 2023-11-06 14:44:36,425 >> tokenizer config file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-06 14:44:36,425 >> Special tokens file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1900/special_tokens_map.json
[2023-11-06 14:44:37,667] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1900 is about to be saved!
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-11-06 14:44:44,670] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1900/global_step1900/mp_rank_00_model_states.pt
[2023-11-06 14:44:44,670] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1900/global_step1900/mp_rank_00_model_states.pt...
[2023-11-06 14:45:18,170] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1900/global_step1900/mp_rank_00_model_states.pt.
[2023-11-06 14:45:18,685] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1900/global_step1900/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 14:45:18,762] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1900/global_step1900/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 14:45:18,762] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-1900/global_step1900/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-11-06 14:45:18,763] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1900 is ready now!
 69%|██████▉   | 1901/2742 [47:09<3:49:25, 16.37s/it] 69%|██████▉   | 1902/2742 [47:10<2:44:19, 11.74s/it] 69%|██████▉   | 1903/2742 [47:11<1:58:49,  8.50s/it] 69%|██████▉   | 1904/2742 [47:11<1:26:54,  6.22s/it] 69%|██████▉   | 1905/2742 [47:12<1:04:48,  4.65s/it]                                                     {'loss': 0.931, 'learning_rate': 1.0711470120624936e-05, 'epoch': 2.08}
 69%|██████▉   | 1905/2742 [47:12<1:04:48,  4.65s/it] 70%|██████▉   | 1906/2742 [47:13<49:49,  3.58s/it]   70%|██████▉   | 1907/2742 [47:14<38:46,  2.79s/it] 70%|██████▉   | 1908/2742 [47:15<31:29,  2.27s/it] 70%|██████▉   | 1909/2742 [47:16<26:09,  1.88s/it] 70%|██████▉   | 1910/2742 [47:18<22:33,  1.63s/it]                                                   {'loss': 1.2742, 'learning_rate': 1.0594185740682367e-05, 'epoch': 2.09}
 70%|██████▉   | 1910/2742 [47:18<22:33,  1.63s/it] 70%|██████▉   | 1911/2742 [47:18<19:53,  1.44s/it] 70%|██████▉   | 1912/2742 [47:19<18:02,  1.30s/it] 70%|██████▉   | 1913/2742 [47:20<16:41,  1.21s/it] 70%|██████▉   | 1914/2742 [47:22<16:50,  1.22s/it] 70%|██████▉   | 1915/2742 [47:23<16:00,  1.16s/it]                                                   {'loss': 1.0787, 'learning_rate': 1.0477374121558845e-05, 'epoch': 2.09}
 70%|██████▉   | 1915/2742 [47:23<16:00,  1.16s/it] 70%|██████▉   | 1916/2742 [47:24<14:58,  1.09s/it] 70%|██████▉   | 1917/2742 [47:25<14:22,  1.05s/it] 70%|██████▉   | 1918/2742 [47:26<13:49,  1.01s/it] 70%|██████▉   | 1919/2742 [47:27<13:44,  1.00s/it] 70%|███████   | 1920/2742 [47:27<13:32,  1.01it/s]                                                   {'loss': 1.074, 'learning_rate': 1.0361039096703518e-05, 'epoch': 2.1}
 70%|███████   | 1920/2742 [47:27<13:32,  1.01it/s] 70%|███████   | 1921/2742 [47:28<13:22,  1.02it/s] 70%|███████   | 1922/2742 [47:29<13:00,  1.05it/s] 70%|███████   | 1923/2742 [47:30<12:50,  1.06it/s] 70%|███████   | 1924/2742 [47:31<12:58,  1.05it/s] 70%|███████   | 1925/2742 [47:32<12:53,  1.06it/s]                                                   {'loss': 1.064, 'learning_rate': 1.0245184483925011e-05, 'epoch': 2.1}
 70%|███████   | 1925/2742 [47:32<12:53,  1.06it/s] 70%|███████   | 1926/2742 [47:33<12:42,  1.07it/s] 70%|███████   | 1927/2742 [47:34<12:47,  1.06it/s] 70%|███████   | 1928/2742 [47:35<13:00,  1.04it/s] 70%|███████   | 1929/2742 [47:36<12:57,  1.05it/s] 70%|███████   | 1930/2742 [47:37<13:11,  1.03it/s]                                                   {'loss': 1.2106, 'learning_rate': 1.0129814085266059e-05, 'epoch': 2.11}
 70%|███████   | 1930/2742 [47:37<13:11,  1.03it/s] 70%|███████   | 1931/2742 [47:38<13:00,  1.04it/s] 70%|███████   | 1932/2742 [47:39<13:06,  1.03it/s] 70%|███████   | 1933/2742 [47:40<12:57,  1.04it/s] 71%|███████   | 1934/2742 [47:41<12:44,  1.06it/s] 71%|███████   | 1935/2742 [47:42<13:32,  1.01s/it]                                                   {'loss': 1.2048, 'learning_rate': 1.0014931686878776e-05, 'epoch': 2.12}
 71%|███████   | 1935/2742 [47:42<13:32,  1.01s/it] 71%|███████   | 1936/2742 [47:43<13:21,  1.01it/s] 71%|███████   | 1937/2742 [47:44<13:07,  1.02it/s] 71%|███████   | 1938/2742 [47:45<13:39,  1.02s/it] 71%|███████   | 1939/2742 [47:46<13:20,  1.00it/s] 71%|███████   | 1940/2742 [47:47<13:01,  1.03it/s]                                                   {'loss': 1.2354, 'learning_rate': 9.900541058900411e-06, 'epoch': 2.12}
 71%|███████   | 1940/2742 [47:47<13:01,  1.03it/s] 71%|███████   | 1941/2742 [47:48<12:55,  1.03it/s] 71%|███████   | 1942/2742 [47:49<12:39,  1.05it/s] 71%|███████   | 1943/2742 [47:50<12:27,  1.07it/s] 71%|███████   | 1944/2742 [47:50<12:25,  1.07it/s] 71%|███████   | 1945/2742 [47:51<12:24,  1.07it/s]                                                   {'loss': 1.2697, 'learning_rate': 9.78664595532958e-06, 'epoch': 2.13}
 71%|███████   | 1945/2742 [47:51<12:24,  1.07it/s] 71%|███████   | 1946/2742 [47:52<12:34,  1.05it/s] 71%|███████   | 1947/2742 [47:53<12:50,  1.03it/s] 71%|███████   | 1948/2742 [47:54<12:49,  1.03it/s] 71%|███████   | 1949/2742 [47:55<12:40,  1.04it/s] 71%|███████   | 1950/2742 [47:56<12:25,  1.06it/s]                                                   {'loss': 1.157, 'learning_rate': 9.67325011390311e-06, 'epoch': 2.13}
 71%|███████   | 1950/2742 [47:56<12:25,  1.06it/s] 71%|███████   | 1951/2742 [47:57<12:21,  1.07it/s] 71%|███████   | 1952/2742 [47:58<12:36,  1.04it/s] 71%|███████   | 1953/2742 [47:59<12:46,  1.03it/s] 71%|███████▏  | 1954/2742 [48:00<13:02,  1.01it/s] 71%|███████▏  | 1955/2742 [48:01<13:42,  1.04s/it]                                                   {'loss': 1.0936, 'learning_rate': 9.560357255973373e-06, 'epoch': 2.14}
 71%|███████▏  | 1955/2742 [48:01<13:42,  1.04s/it] 71%|███████▏  | 1956/2742 [48:02<13:39,  1.04s/it] 71%|███████▏  | 1957/2742 [48:03<13:11,  1.01s/it] 71%|███████▏  | 1958/2742 [48:05<14:00,  1.07s/it] 71%|███████▏  | 1959/2742 [48:05<13:25,  1.03s/it] 71%|███████▏  | 1960/2742 [48:06<13:09,  1.01s/it]                                                   {'loss': 1.1418, 'learning_rate': 9.447971086386118e-06, 'epoch': 2.14}
 71%|███████▏  | 1960/2742 [48:06<13:09,  1.01s/it] 72%|███████▏  | 1961/2742 [48:08<13:53,  1.07s/it] 72%|███████▏  | 1962/2742 [48:09<14:44,  1.13s/it] 72%|███████▏  | 1963/2742 [48:10<13:59,  1.08s/it] 72%|███████▏  | 1964/2742 [48:11<13:51,  1.07s/it] 72%|███████▏  | 1965/2742 [48:12<13:43,  1.06s/it]                                                   {'loss': 1.1931, 'learning_rate': 9.336095293358955e-06, 'epoch': 2.15}
 72%|███████▏  | 1965/2742 [48:12<13:43,  1.06s/it] 72%|███████▏  | 1966/2742 [48:13<13:08,  1.02s/it] 72%|███████▏  | 1967/2742 [48:14<12:47,  1.01it/s] 72%|███████▏  | 1968/2742 [48:15<12:52,  1.00it/s] 72%|███████▏  | 1969/2742 [48:16<13:16,  1.03s/it] 72%|███████▏  | 1970/2742 [48:17<13:16,  1.03s/it]                                                   {'loss': 1.1774, 'learning_rate': 9.224733548360252e-06, 'epoch': 2.15}
 72%|███████▏  | 1970/2742 [48:17<13:16,  1.03s/it] 72%|███████▏  | 1971/2742 [48:18<13:04,  1.02s/it] 72%|███████▏  | 1972/2742 [48:19<13:09,  1.03s/it] 72%|███████▏  | 1973/2742 [48:20<12:44,  1.01it/s] 72%|███████▏  | 1974/2742 [48:21<13:43,  1.07s/it] 72%|███████▏  | 1975/2742 [48:22<13:18,  1.04s/it]                                                   {'loss': 1.2217, 'learning_rate': 9.11388950598869e-06, 'epoch': 2.16}
 72%|███████▏  | 1975/2742 [48:22<13:18,  1.04s/it] 72%|███████▏  | 1976/2742 [48:23<13:04,  1.02s/it] 72%|███████▏  | 1977/2742 [48:24<12:55,  1.01s/it] 72%|███████▏  | 1978/2742 [48:25<12:52,  1.01s/it] 72%|███████▏  | 1979/2742 [48:26<12:46,  1.00s/it] 72%|███████▏  | 1980/2742 [48:27<12:37,  1.01it/s]                                                   {'loss': 1.1759, 'learning_rate': 9.003566803853308e-06, 'epoch': 2.16}
 72%|███████▏  | 1980/2742 [48:27<12:37,  1.01it/s] 72%|███████▏  | 1981/2742 [48:28<13:02,  1.03s/it] 72%|███████▏  | 1982/2742 [48:29<12:42,  1.00s/it] 72%|███████▏  | 1983/2742 [48:30<12:43,  1.01s/it] 72%|███████▏  | 1984/2742 [48:31<13:10,  1.04s/it] 72%|███████▏  | 1985/2742 [48:32<12:33,  1.00it/s]                                                   {'loss': 1.1047, 'learning_rate': 8.893769062454146e-06, 'epoch': 2.17}
 72%|███████▏  | 1985/2742 [48:32<12:33,  1.00it/s] 72%|███████▏  | 1986/2742 [48:33<12:24,  1.02it/s] 72%|███████▏  | 1987/2742 [48:34<12:22,  1.02it/s] 73%|███████▎  | 1988/2742 [48:35<12:14,  1.03it/s] 73%|███████▎  | 1989/2742 [48:36<12:22,  1.01it/s] 73%|███████▎  | 1990/2742 [48:37<12:20,  1.02it/s]                                                   {'loss': 1.1825, 'learning_rate': 8.784499885063426e-06, 'epoch': 2.18}
 73%|███████▎  | 1990/2742 [48:37<12:20,  1.02it/s] 73%|███████▎  | 1991/2742 [48:38<12:12,  1.02it/s] 73%|███████▎  | 1992/2742 [48:39<12:24,  1.01it/s] 73%|███████▎  | 1993/2742 [48:40<12:37,  1.01s/it] 73%|███████▎  | 1994/2742 [48:41<12:39,  1.02s/it] 73%|███████▎  | 1995/2742 [48:42<13:06,  1.05s/it]                                                   {'loss': 1.2333, 'learning_rate': 8.675762857607256e-06, 'epoch': 2.18}
 73%|███████▎  | 1995/2742 [48:42<13:06,  1.05s/it] 73%|███████▎  | 1996/2742 [48:43<12:41,  1.02s/it] 73%|███████▎  | 1997/2742 [48:44<12:40,  1.02s/it] 73%|███████▎  | 1998/2742 [48:45<12:17,  1.01it/s] 73%|███████▎  | 1999/2742 [48:46<12:10,  1.02it/s] 73%|███████▎  | 2000/2742 [48:47<12:02,  1.03it/s]                                                   {'loss': 1.0783, 'learning_rate': 8.567561548548031e-06, 'epoch': 2.19}
 73%|███████▎  | 2000/2742 [48:47<12:02,  1.03it/s][INFO|trainer.py:2881] 2023-11-06 14:47:06,859 >> Saving model checkpoint to saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2000
[INFO|tokenization_utils_base.py:2428] 2023-11-06 14:47:06,904 >> tokenizer config file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-06 14:47:06,904 >> Special tokens file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2000/special_tokens_map.json
[2023-11-06 14:47:08,051] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2000 is about to be saved!
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-11-06 14:47:14,990] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2000/global_step2000/mp_rank_00_model_states.pt
[2023-11-06 14:47:14,990] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2000/global_step2000/mp_rank_00_model_states.pt...
[2023-11-06 14:47:46,341] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2000/global_step2000/mp_rank_00_model_states.pt.
[2023-11-06 14:47:46,921] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2000/global_step2000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 14:47:46,973] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2000/global_step2000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 14:47:46,973] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2000/global_step2000/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-11-06 14:47:46,974] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
 73%|███████▎  | 2001/2742 [49:37<3:12:46, 15.61s/it] 73%|███████▎  | 2002/2742 [49:38<2:18:26, 11.23s/it] 73%|███████▎  | 2003/2742 [49:39<1:40:33,  8.16s/it] 73%|███████▎  | 2004/2742 [49:40<1:14:05,  6.02s/it] 73%|███████▎  | 2005/2742 [49:41<55:15,  4.50s/it]                                                     {'loss': 1.1155, 'learning_rate': 8.459899508767252e-06, 'epoch': 2.19}
 73%|███████▎  | 2005/2742 [49:41<55:15,  4.50s/it] 73%|███████▎  | 2006/2742 [49:42<43:07,  3.52s/it] 73%|███████▎  | 2007/2742 [49:43<33:22,  2.72s/it] 73%|███████▎  | 2008/2742 [49:44<26:48,  2.19s/it] 73%|███████▎  | 2009/2742 [49:45<22:34,  1.85s/it] 73%|███████▎  | 2010/2742 [49:46<19:27,  1.59s/it]                                                   {'loss': 1.1402, 'learning_rate': 8.352780271449026e-06, 'epoch': 2.2}
 73%|███████▎  | 2010/2742 [49:46<19:27,  1.59s/it] 73%|███████▎  | 2011/2742 [49:47<17:46,  1.46s/it] 73%|███████▎  | 2012/2742 [49:48<16:06,  1.32s/it] 73%|███████▎  | 2013/2742 [49:49<14:55,  1.23s/it] 73%|███████▎  | 2014/2742 [49:50<14:11,  1.17s/it] 73%|███████▎  | 2015/2742 [49:51<13:24,  1.11s/it]                                                   {'loss': 1.2683, 'learning_rate': 8.246207351964138e-06, 'epoch': 2.2}
 73%|███████▎  | 2015/2742 [49:51<13:24,  1.11s/it] 74%|███████▎  | 2016/2742 [49:52<12:42,  1.05s/it] 74%|███████▎  | 2017/2742 [49:53<12:09,  1.01s/it] 74%|███████▎  | 2018/2742 [49:54<12:36,  1.04s/it] 74%|███████▎  | 2019/2742 [49:55<12:28,  1.04s/it] 74%|███████▎  | 2020/2742 [49:56<12:15,  1.02s/it]                                                   {'loss': 1.2739, 'learning_rate': 8.140184247754635e-06, 'epoch': 2.21}
 74%|███████▎  | 2020/2742 [49:56<12:15,  1.02s/it] 74%|███████▎  | 2021/2742 [49:57<11:59,  1.00it/s] 74%|███████▎  | 2022/2742 [49:58<12:00,  1.00s/it] 74%|███████▍  | 2023/2742 [49:59<11:41,  1.03it/s] 74%|███████▍  | 2024/2742 [50:00<11:30,  1.04it/s] 74%|███████▍  | 2025/2742 [50:01<11:20,  1.05it/s]                                                   {'loss': 0.9553, 'learning_rate': 8.034714438219102e-06, 'epoch': 2.21}
 74%|███████▍  | 2025/2742 [50:01<11:20,  1.05it/s] 74%|███████▍  | 2026/2742 [50:02<11:32,  1.03it/s] 74%|███████▍  | 2027/2742 [50:03<11:35,  1.03it/s] 74%|███████▍  | 2028/2742 [50:04<11:34,  1.03it/s] 74%|███████▍  | 2029/2742 [50:05<11:57,  1.01s/it] 74%|███████▍  | 2030/2742 [50:06<11:43,  1.01it/s]                                                   {'loss': 1.1623, 'learning_rate': 7.92980138459843e-06, 'epoch': 2.22}
 74%|███████▍  | 2030/2742 [50:06<11:43,  1.01it/s] 74%|███████▍  | 2031/2742 [50:07<11:50,  1.00it/s] 74%|███████▍  | 2032/2742 [50:08<11:49,  1.00it/s] 74%|███████▍  | 2033/2742 [50:09<11:56,  1.01s/it] 74%|███████▍  | 2034/2742 [50:10<11:47,  1.00it/s] 74%|███████▍  | 2035/2742 [50:11<11:29,  1.03it/s]                                                   {'loss': 1.0843, 'learning_rate': 7.825448529862273e-06, 'epoch': 2.22}
 74%|███████▍  | 2035/2742 [50:11<11:29,  1.03it/s] 74%|███████▍  | 2036/2742 [50:12<12:02,  1.02s/it] 74%|███████▍  | 2037/2742 [50:13<11:42,  1.00it/s] 74%|███████▍  | 2038/2742 [50:14<11:40,  1.00it/s] 74%|███████▍  | 2039/2742 [50:15<11:30,  1.02it/s] 74%|███████▍  | 2040/2742 [50:16<11:20,  1.03it/s]                                                   {'loss': 1.1619, 'learning_rate': 7.72165929859601e-06, 'epoch': 2.23}
 74%|███████▍  | 2040/2742 [50:16<11:20,  1.03it/s] 74%|███████▍  | 2041/2742 [50:17<11:13,  1.04it/s] 74%|███████▍  | 2042/2742 [50:18<12:06,  1.04s/it] 75%|███████▍  | 2043/2742 [50:19<11:49,  1.02s/it] 75%|███████▍  | 2044/2742 [50:20<11:58,  1.03s/it] 75%|███████▍  | 2045/2742 [50:21<12:08,  1.04s/it]                                                   {'loss': 1.2431, 'learning_rate': 7.618437096888412e-06, 'epoch': 2.24}
 75%|███████▍  | 2045/2742 [50:21<12:08,  1.04s/it] 75%|███████▍  | 2046/2742 [50:22<11:56,  1.03s/it] 75%|███████▍  | 2047/2742 [50:23<11:38,  1.00s/it] 75%|███████▍  | 2048/2742 [50:24<11:37,  1.00s/it] 75%|███████▍  | 2049/2742 [50:25<11:25,  1.01it/s] 75%|███████▍  | 2050/2742 [50:26<11:16,  1.02it/s]                                                   {'loss': 1.2529, 'learning_rate': 7.515785312219811e-06, 'epoch': 2.24}
 75%|███████▍  | 2050/2742 [50:26<11:16,  1.02it/s] 75%|███████▍  | 2051/2742 [50:27<11:04,  1.04it/s] 75%|███████▍  | 2052/2742 [50:28<11:22,  1.01it/s] 75%|███████▍  | 2053/2742 [50:29<11:21,  1.01it/s] 75%|███████▍  | 2054/2742 [50:30<11:12,  1.02it/s] 75%|███████▍  | 2055/2742 [50:31<11:05,  1.03it/s]                                                   {'loss': 1.0114, 'learning_rate': 7.413707313350965e-06, 'epoch': 2.25}
 75%|███████▍  | 2055/2742 [50:31<11:05,  1.03it/s] 75%|███████▍  | 2056/2742 [50:32<11:08,  1.03it/s] 75%|███████▌  | 2057/2742 [50:33<11:01,  1.03it/s] 75%|███████▌  | 2058/2742 [50:33<10:51,  1.05it/s] 75%|███████▌  | 2059/2742 [50:34<10:54,  1.04it/s] 75%|███████▌  | 2060/2742 [50:35<10:56,  1.04it/s]                                                   {'loss': 1.1463, 'learning_rate': 7.312206450212508e-06, 'epoch': 2.25}
 75%|███████▌  | 2060/2742 [50:35<10:56,  1.04it/s] 75%|███████▌  | 2061/2742 [50:36<11:21,  1.00s/it] 75%|███████▌  | 2062/2742 [50:38<12:00,  1.06s/it] 75%|███████▌  | 2063/2742 [50:39<11:41,  1.03s/it] 75%|███████▌  | 2064/2742 [50:40<11:19,  1.00s/it] 75%|███████▌  | 2065/2742 [50:41<11:11,  1.01it/s]                                                   {'loss': 1.3529, 'learning_rate': 7.211286053794983e-06, 'epoch': 2.26}
 75%|███████▌  | 2065/2742 [50:41<11:11,  1.01it/s] 75%|███████▌  | 2066/2742 [50:41<10:56,  1.03it/s] 75%|███████▌  | 2067/2742 [50:42<10:59,  1.02it/s] 75%|███████▌  | 2068/2742 [50:43<10:59,  1.02it/s] 75%|███████▌  | 2069/2742 [50:44<11:00,  1.02it/s] 75%|███████▌  | 2070/2742 [50:45<10:46,  1.04it/s]                                                   {'loss': 1.1342, 'learning_rate': 7.110949436039555e-06, 'epoch': 2.26}
 75%|███████▌  | 2070/2742 [50:45<10:46,  1.04it/s] 76%|███████▌  | 2071/2742 [50:46<10:37,  1.05it/s] 76%|███████▌  | 2072/2742 [50:47<10:39,  1.05it/s] 76%|███████▌  | 2073/2742 [50:48<10:36,  1.05it/s] 76%|███████▌  | 2074/2742 [50:49<11:29,  1.03s/it] 76%|███████▌  | 2075/2742 [50:51<11:57,  1.08s/it]                                                   {'loss': 1.1713, 'learning_rate': 7.011199889729328e-06, 'epoch': 2.27}
 76%|███████▌  | 2075/2742 [50:51<11:57,  1.08s/it] 76%|███████▌  | 2076/2742 [50:52<11:27,  1.03s/it] 76%|███████▌  | 2077/2742 [50:52<11:05,  1.00s/it] 76%|███████▌  | 2078/2742 [50:53<10:52,  1.02it/s] 76%|███████▌  | 2079/2742 [50:54<10:56,  1.01it/s] 76%|███████▌  | 2080/2742 [50:55<10:42,  1.03it/s]                                                   {'loss': 1.1082, 'learning_rate': 6.912040688381246e-06, 'epoch': 2.27}
 76%|███████▌  | 2080/2742 [50:55<10:42,  1.03it/s] 76%|███████▌  | 2081/2742 [50:56<10:34,  1.04it/s] 76%|███████▌  | 2082/2742 [50:57<10:33,  1.04it/s] 76%|███████▌  | 2083/2742 [50:58<11:02,  1.01s/it] 76%|███████▌  | 2084/2742 [50:59<10:55,  1.00it/s] 76%|███████▌  | 2085/2742 [51:00<11:15,  1.03s/it]                                                   {'loss': 1.1975, 'learning_rate': 6.813475086138699e-06, 'epoch': 2.28}
 76%|███████▌  | 2085/2742 [51:00<11:15,  1.03s/it] 76%|███████▌  | 2086/2742 [51:01<11:04,  1.01s/it] 76%|███████▌  | 2087/2742 [51:03<11:39,  1.07s/it] 76%|███████▌  | 2088/2742 [51:04<11:12,  1.03s/it] 76%|███████▌  | 2089/2742 [51:05<11:13,  1.03s/it] 76%|███████▌  | 2090/2742 [51:06<11:19,  1.04s/it]                                                   {'loss': 1.0008, 'learning_rate': 6.71550631766471e-06, 'epoch': 2.28}
 76%|███████▌  | 2090/2742 [51:06<11:19,  1.04s/it] 76%|███████▋  | 2091/2742 [51:07<11:17,  1.04s/it] 76%|███████▋  | 2092/2742 [51:08<10:58,  1.01s/it] 76%|███████▋  | 2093/2742 [51:09<10:47,  1.00it/s] 76%|███████▋  | 2094/2742 [51:10<10:39,  1.01it/s] 76%|███████▋  | 2095/2742 [51:10<10:31,  1.02it/s]                                                   {'loss': 1.1076, 'learning_rate': 6.61813759803582e-06, 'epoch': 2.29}
 76%|███████▋  | 2095/2742 [51:10<10:31,  1.02it/s] 76%|███████▋  | 2096/2742 [51:11<10:18,  1.05it/s] 76%|███████▋  | 2097/2742 [51:12<10:15,  1.05it/s] 77%|███████▋  | 2098/2742 [51:13<10:14,  1.05it/s] 77%|███████▋  | 2099/2742 [51:14<10:32,  1.02it/s] 77%|███████▋  | 2100/2742 [51:16<11:27,  1.07s/it]                                                   {'loss': 1.0043, 'learning_rate': 6.521372122636518e-06, 'epoch': 2.3}
 77%|███████▋  | 2100/2742 [51:16<11:27,  1.07s/it][INFO|trainer.py:2881] 2023-11-06 14:49:35,835 >> Saving model checkpoint to saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2100
[INFO|tokenization_utils_base.py:2428] 2023-11-06 14:49:35,882 >> tokenizer config file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-06 14:49:35,882 >> Special tokens file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2100/special_tokens_map.json
[2023-11-06 14:49:37,087] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2100 is about to be saved!
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-11-06 14:49:44,265] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2100/global_step2100/mp_rank_00_model_states.pt
[2023-11-06 14:49:44,265] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2100/global_step2100/mp_rank_00_model_states.pt...
[2023-11-06 14:50:17,805] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2100/global_step2100/mp_rank_00_model_states.pt.
[2023-11-06 14:50:18,408] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2100/global_step2100/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 14:50:18,526] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2100/global_step2100/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 14:50:18,526] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2100/global_step2100/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-11-06 14:50:18,527] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2100 is ready now!
 77%|███████▋  | 2101/2742 [52:09<2:57:34, 16.62s/it] 77%|███████▋  | 2102/2742 [52:10<2:08:06, 12.01s/it] 77%|███████▋  | 2103/2742 [52:11<1:32:47,  8.71s/it] 77%|███████▋  | 2104/2742 [52:12<1:08:03,  6.40s/it] 77%|███████▋  | 2105/2742 [52:13<51:08,  4.82s/it]                                                     {'loss': 1.1992, 'learning_rate': 6.425213067054428e-06, 'epoch': 2.3}
 77%|███████▋  | 2105/2742 [52:13<51:08,  4.82s/it] 77%|███████▋  | 2106/2742 [52:14<39:03,  3.68s/it] 77%|███████▋  | 2107/2742 [52:15<30:51,  2.92s/it] 77%|███████▋  | 2108/2742 [52:16<24:26,  2.31s/it] 77%|███████▋  | 2109/2742 [52:17<20:04,  1.90s/it] 77%|███████▋  | 2110/2742 [52:18<17:15,  1.64s/it]                                                   {'loss': 1.1246, 'learning_rate': 6.329663586976079e-06, 'epoch': 2.31}
 77%|███████▋  | 2110/2742 [52:18<17:15,  1.64s/it] 77%|███████▋  | 2111/2742 [52:19<15:22,  1.46s/it] 77%|███████▋  | 2112/2742 [52:20<13:52,  1.32s/it] 77%|███████▋  | 2113/2742 [52:21<12:33,  1.20s/it] 77%|███████▋  | 2114/2742 [52:22<11:46,  1.12s/it] 77%|███████▋  | 2115/2742 [52:23<11:34,  1.11s/it]                                                   {'loss': 1.0789, 'learning_rate': 6.234726818083322e-06, 'epoch': 2.31}
 77%|███████▋  | 2115/2742 [52:23<11:34,  1.11s/it] 77%|███████▋  | 2116/2742 [52:24<11:16,  1.08s/it] 77%|███████▋  | 2117/2742 [52:25<10:59,  1.06s/it] 77%|███████▋  | 2118/2742 [52:26<10:35,  1.02s/it] 77%|███████▋  | 2119/2742 [52:27<10:37,  1.02s/it] 77%|███████▋  | 2120/2742 [52:28<10:50,  1.05s/it]                                                   {'loss': 1.191, 'learning_rate': 6.140405875950458e-06, 'epoch': 2.32}
 77%|███████▋  | 2120/2742 [52:28<10:50,  1.05s/it] 77%|███████▋  | 2121/2742 [52:29<11:04,  1.07s/it] 77%|███████▋  | 2122/2742 [52:30<10:55,  1.06s/it] 77%|███████▋  | 2123/2742 [52:31<10:47,  1.05s/it] 77%|███████▋  | 2124/2742 [52:32<10:35,  1.03s/it] 77%|███████▋  | 2125/2742 [52:33<10:51,  1.06s/it]                                                   {'loss': 1.1783, 'learning_rate': 6.046703855941963e-06, 'epoch': 2.32}
 77%|███████▋  | 2125/2742 [52:33<10:51,  1.06s/it] 78%|███████▊  | 2126/2742 [52:34<10:24,  1.01s/it] 78%|███████▊  | 2127/2742 [52:35<10:13,  1.00it/s] 78%|███████▊  | 2128/2742 [52:36<10:05,  1.01it/s] 78%|███████▊  | 2129/2742 [52:37<09:58,  1.02it/s] 78%|███████▊  | 2130/2742 [52:38<09:52,  1.03it/s]                                                   {'loss': 1.2225, 'learning_rate': 5.9536238331109465e-06, 'epoch': 2.33}
 78%|███████▊  | 2130/2742 [52:38<09:52,  1.03it/s] 78%|███████▊  | 2131/2742 [52:39<09:39,  1.05it/s] 78%|███████▊  | 2132/2742 [52:40<09:39,  1.05it/s] 78%|███████▊  | 2133/2742 [52:41<09:54,  1.02it/s] 78%|███████▊  | 2134/2742 [52:42<09:45,  1.04it/s] 78%|███████▊  | 2135/2742 [52:43<09:47,  1.03it/s]                                                   {'loss': 1.2241, 'learning_rate': 5.861168862098182e-06, 'epoch': 2.33}
 78%|███████▊  | 2135/2742 [52:43<09:47,  1.03it/s] 78%|███████▊  | 2136/2742 [52:44<09:41,  1.04it/s] 78%|███████▊  | 2137/2742 [52:45<09:52,  1.02it/s] 78%|███████▊  | 2138/2742 [52:46<09:53,  1.02it/s] 78%|███████▊  | 2139/2742 [52:47<09:47,  1.03it/s] 78%|███████▊  | 2140/2742 [52:48<09:47,  1.03it/s]                                                   {'loss': 1.1714, 'learning_rate': 5.769341977031914e-06, 'epoch': 2.34}
 78%|███████▊  | 2140/2742 [52:48<09:47,  1.03it/s] 78%|███████▊  | 2141/2742 [52:49<09:51,  1.02it/s] 78%|███████▊  | 2142/2742 [52:50<09:41,  1.03it/s] 78%|███████▊  | 2143/2742 [52:51<09:50,  1.01it/s] 78%|███████▊  | 2144/2742 [52:52<09:39,  1.03it/s] 78%|███████▊  | 2145/2742 [52:53<09:42,  1.03it/s]                                                   {'loss': 1.2595, 'learning_rate': 5.678146191428266e-06, 'epoch': 2.34}
 78%|███████▊  | 2145/2742 [52:53<09:42,  1.03it/s] 78%|███████▊  | 2146/2742 [52:54<09:29,  1.05it/s] 78%|███████▊  | 2147/2742 [52:55<09:59,  1.01s/it] 78%|███████▊  | 2148/2742 [52:56<09:55,  1.00s/it] 78%|███████▊  | 2149/2742 [52:57<09:38,  1.03it/s] 78%|███████▊  | 2150/2742 [52:57<09:32,  1.03it/s]                                                   {'loss': 1.0286, 'learning_rate': 5.5875844980923105e-06, 'epoch': 2.35}
 78%|███████▊  | 2150/2742 [52:58<09:32,  1.03it/s] 78%|███████▊  | 2151/2742 [52:58<09:24,  1.05it/s] 78%|███████▊  | 2152/2742 [52:59<09:23,  1.05it/s] 79%|███████▊  | 2153/2742 [53:00<09:20,  1.05it/s] 79%|███████▊  | 2154/2742 [53:01<09:43,  1.01it/s] 79%|███████▊  | 2155/2742 [53:02<09:53,  1.01s/it]                                                   {'loss': 1.0466, 'learning_rate': 5.497659869019928e-06, 'epoch': 2.36}
 79%|███████▊  | 2155/2742 [53:02<09:53,  1.01s/it] 79%|███████▊  | 2156/2742 [53:03<09:42,  1.01it/s] 79%|███████▊  | 2157/2742 [53:04<09:31,  1.02it/s] 79%|███████▊  | 2158/2742 [53:05<09:34,  1.02it/s] 79%|███████▊  | 2159/2742 [53:07<10:09,  1.05s/it] 79%|███████▉  | 2160/2742 [53:07<09:47,  1.01s/it]                                                   {'loss': 0.9756, 'learning_rate': 5.408375255300194e-06, 'epoch': 2.36}
 79%|███████▉  | 2160/2742 [53:07<09:47,  1.01s/it] 79%|███████▉  | 2161/2742 [53:08<09:31,  1.02it/s] 79%|███████▉  | 2162/2742 [53:09<09:20,  1.04it/s] 79%|███████▉  | 2163/2742 [53:10<09:32,  1.01it/s] 79%|███████▉  | 2164/2742 [53:12<10:12,  1.06s/it] 79%|███████▉  | 2165/2742 [53:12<09:45,  1.01s/it]                                                   {'loss': 0.9993, 'learning_rate': 5.319733587018583e-06, 'epoch': 2.37}
 79%|███████▉  | 2165/2742 [53:12<09:45,  1.01s/it] 79%|███████▉  | 2166/2742 [53:14<09:43,  1.01s/it] 79%|███████▉  | 2167/2742 [53:15<09:43,  1.02s/it] 79%|███████▉  | 2168/2742 [53:16<09:50,  1.03s/it] 79%|███████▉  | 2169/2742 [53:17<09:33,  1.00s/it] 79%|███████▉  | 2170/2742 [53:18<09:28,  1.01it/s]                                                   {'loss': 1.1511, 'learning_rate': 5.231737773160805e-06, 'epoch': 2.37}
 79%|███████▉  | 2170/2742 [53:18<09:28,  1.01it/s] 79%|███████▉  | 2171/2742 [53:18<09:24,  1.01it/s] 79%|███████▉  | 2172/2742 [53:19<09:13,  1.03it/s] 79%|███████▉  | 2173/2742 [53:20<09:09,  1.04it/s] 79%|███████▉  | 2174/2742 [53:21<09:06,  1.04it/s] 79%|███████▉  | 2175/2742 [53:22<09:04,  1.04it/s]                                                   {'loss': 1.094, 'learning_rate': 5.1443907015173096e-06, 'epoch': 2.38}
 79%|███████▉  | 2175/2742 [53:22<09:04,  1.04it/s] 79%|███████▉  | 2176/2742 [53:23<09:03,  1.04it/s] 79%|███████▉  | 2177/2742 [53:24<09:25,  1.00s/it] 79%|███████▉  | 2178/2742 [53:25<09:09,  1.03it/s] 79%|███████▉  | 2179/2742 [53:26<09:04,  1.03it/s] 80%|███████▉  | 2180/2742 [53:27<09:03,  1.03it/s]                                                   {'loss': 1.418, 'learning_rate': 5.05769523858857e-06, 'epoch': 2.38}
 80%|███████▉  | 2180/2742 [53:27<09:03,  1.03it/s] 80%|███████▉  | 2181/2742 [53:28<08:51,  1.06it/s] 80%|███████▉  | 2182/2742 [53:29<08:49,  1.06it/s] 80%|███████▉  | 2183/2742 [53:30<08:59,  1.04it/s] 80%|███████▉  | 2184/2742 [53:31<08:59,  1.03it/s] 80%|███████▉  | 2185/2742 [53:32<08:58,  1.03it/s]                                                   {'loss': 1.1193, 'learning_rate': 4.971654229490946e-06, 'epoch': 2.39}
 80%|███████▉  | 2185/2742 [53:32<08:58,  1.03it/s] 80%|███████▉  | 2186/2742 [53:33<09:08,  1.01it/s] 80%|███████▉  | 2187/2742 [53:34<09:01,  1.02it/s] 80%|███████▉  | 2188/2742 [53:35<09:01,  1.02it/s] 80%|███████▉  | 2189/2742 [53:36<09:07,  1.01it/s] 80%|███████▉  | 2190/2742 [53:37<09:05,  1.01it/s]                                                   {'loss': 1.1873, 'learning_rate': 4.886270497863377e-06, 'epoch': 2.39}
 80%|███████▉  | 2190/2742 [53:37<09:05,  1.01it/s] 80%|███████▉  | 2191/2742 [53:38<08:49,  1.04it/s] 80%|███████▉  | 2192/2742 [53:39<09:01,  1.02it/s] 80%|███████▉  | 2193/2742 [53:40<08:56,  1.02it/s] 80%|████████  | 2194/2742 [53:41<09:01,  1.01it/s] 80%|████████  | 2195/2742 [53:42<08:55,  1.02it/s]                                                   {'loss': 1.1281, 'learning_rate': 4.8015468457746736e-06, 'epoch': 2.4}
 80%|████████  | 2195/2742 [53:42<08:55,  1.02it/s] 80%|████████  | 2196/2742 [53:43<08:49,  1.03it/s] 80%|████████  | 2197/2742 [53:44<08:55,  1.02it/s] 80%|████████  | 2198/2742 [53:45<08:42,  1.04it/s] 80%|████████  | 2199/2742 [53:46<08:50,  1.02it/s] 80%|████████  | 2200/2742 [53:47<08:52,  1.02it/s]                                                   {'loss': 1.0174, 'learning_rate': 4.717486053631576e-06, 'epoch': 2.41}
 80%|████████  | 2200/2742 [53:47<08:52,  1.02it/s][INFO|trainer.py:2881] 2023-11-06 14:52:06,470 >> Saving model checkpoint to saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2200
[INFO|tokenization_utils_base.py:2428] 2023-11-06 14:52:06,521 >> tokenizer config file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-06 14:52:06,521 >> Special tokens file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2200/special_tokens_map.json
[2023-11-06 14:52:07,451] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2200 is about to be saved!
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-11-06 14:52:14,604] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2200/global_step2200/mp_rank_00_model_states.pt
[2023-11-06 14:52:14,604] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2200/global_step2200/mp_rank_00_model_states.pt...
[2023-11-06 14:52:46,986] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2200/global_step2200/mp_rank_00_model_states.pt.
[2023-11-06 14:52:47,487] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2200/global_step2200/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 14:52:47,535] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2200/global_step2200/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 14:52:47,536] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2200/global_step2200/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-11-06 14:52:47,536] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2200 is ready now!
 80%|████████  | 2201/2742 [54:37<2:23:21, 15.90s/it] 80%|████████  | 2202/2742 [54:38<1:42:40, 11.41s/it] 80%|████████  | 2203/2742 [54:39<1:14:32,  8.30s/it] 80%|████████  | 2204/2742 [54:40<54:51,  6.12s/it]   80%|████████  | 2205/2742 [54:41<40:55,  4.57s/it]                                                   {'loss': 1.105, 'learning_rate': 4.63409088008753e-06, 'epoch': 2.41}
 80%|████████  | 2205/2742 [54:41<40:55,  4.57s/it] 80%|████████  | 2206/2742 [54:43<32:01,  3.58s/it] 80%|████████  | 2207/2742 [54:44<24:53,  2.79s/it] 81%|████████  | 2208/2742 [54:44<19:46,  2.22s/it] 81%|████████  | 2209/2742 [54:45<16:20,  1.84s/it] 81%|████████  | 2210/2742 [54:46<14:05,  1.59s/it]                                                   {'loss': 0.9773, 'learning_rate': 4.551364061952104e-06, 'epoch': 2.42}
 81%|████████  | 2210/2742 [54:46<14:05,  1.59s/it] 81%|████████  | 2211/2742 [54:47<12:15,  1.39s/it] 81%|████████  | 2212/2742 [54:48<11:11,  1.27s/it] 81%|████████  | 2213/2742 [54:49<10:17,  1.17s/it] 81%|████████  | 2214/2742 [54:50<09:47,  1.11s/it] 81%|████████  | 2215/2742 [54:51<09:32,  1.09s/it]                                                   {'loss': 0.9762, 'learning_rate': 4.469308314101239e-06, 'epoch': 2.42}
 81%|████████  | 2215/2742 [54:51<09:32,  1.09s/it] 81%|████████  | 2216/2742 [54:52<09:17,  1.06s/it] 81%|████████  | 2217/2742 [54:54<09:47,  1.12s/it] 81%|████████  | 2218/2742 [54:55<09:35,  1.10s/it] 81%|████████  | 2219/2742 [54:56<09:26,  1.08s/it] 81%|████████  | 2220/2742 [54:57<09:23,  1.08s/it]                                                   {'loss': 1.0915, 'learning_rate': 4.387926329388087e-06, 'epoch': 2.43}
 81%|████████  | 2220/2742 [54:57<09:23,  1.08s/it] 81%|████████  | 2221/2742 [54:58<09:05,  1.05s/it] 81%|████████  | 2222/2742 [54:59<08:48,  1.02s/it] 81%|████████  | 2223/2742 [55:00<09:31,  1.10s/it] 81%|████████  | 2224/2742 [55:01<09:00,  1.04s/it] 81%|████████  | 2225/2742 [55:02<08:38,  1.00s/it]                                                   {'loss': 1.0452, 'learning_rate': 4.307220778554694e-06, 'epoch': 2.43}
 81%|████████  | 2225/2742 [55:02<08:38,  1.00s/it] 81%|████████  | 2226/2742 [55:03<08:41,  1.01s/it] 81%|████████  | 2227/2742 [55:04<08:31,  1.01it/s] 81%|████████▏ | 2228/2742 [55:05<08:19,  1.03it/s] 81%|████████▏ | 2229/2742 [55:06<08:10,  1.05it/s] 81%|████████▏ | 2230/2742 [55:06<08:01,  1.06it/s]                                                   {'loss': 1.2659, 'learning_rate': 4.227194310144331e-06, 'epoch': 2.44}
 81%|████████▏ | 2230/2742 [55:06<08:01,  1.06it/s] 81%|████████▏ | 2231/2742 [55:07<07:55,  1.07it/s] 81%|████████▏ | 2232/2742 [55:08<08:07,  1.05it/s] 81%|████████▏ | 2233/2742 [55:09<07:55,  1.07it/s] 81%|████████▏ | 2234/2742 [55:10<07:51,  1.08it/s] 82%|████████▏ | 2235/2742 [55:11<08:18,  1.02it/s]                                                   {'loss': 1.2965, 'learning_rate': 4.147849550414562e-06, 'epoch': 2.44}
 82%|████████▏ | 2235/2742 [55:11<08:18,  1.02it/s] 82%|████████▏ | 2236/2742 [55:12<08:08,  1.04it/s] 82%|████████▏ | 2237/2742 [55:13<08:01,  1.05it/s] 82%|████████▏ | 2238/2742 [55:14<07:56,  1.06it/s] 82%|████████▏ | 2239/2742 [55:15<07:55,  1.06it/s] 82%|████████▏ | 2240/2742 [55:16<07:46,  1.08it/s]                                                   {'loss': 1.1494, 'learning_rate': 4.069189103251072e-06, 'epoch': 2.45}
 82%|████████▏ | 2240/2742 [55:16<07:46,  1.08it/s] 82%|████████▏ | 2241/2742 [55:17<07:53,  1.06it/s] 82%|████████▏ | 2242/2742 [55:18<07:42,  1.08it/s] 82%|████████▏ | 2243/2742 [55:19<08:00,  1.04it/s] 82%|████████▏ | 2244/2742 [55:20<08:05,  1.02it/s] 82%|████████▏ | 2245/2742 [55:21<08:25,  1.02s/it]                                                   {'loss': 1.1751, 'learning_rate': 3.99121555008222e-06, 'epoch': 2.45}
 82%|████████▏ | 2245/2742 [55:21<08:25,  1.02s/it] 82%|████████▏ | 2246/2742 [55:22<08:28,  1.03s/it] 82%|████████▏ | 2247/2742 [55:23<08:14,  1.00it/s] 82%|████████▏ | 2248/2742 [55:24<08:05,  1.02it/s] 82%|████████▏ | 2249/2742 [55:25<07:52,  1.04it/s] 82%|████████▏ | 2250/2742 [55:26<07:50,  1.05it/s]                                                   {'loss': 1.1052, 'learning_rate': 3.913931449794317e-06, 'epoch': 2.46}
 82%|████████▏ | 2250/2742 [55:26<07:50,  1.05it/s] 82%|████████▏ | 2251/2742 [55:27<07:52,  1.04it/s] 82%|████████▏ | 2252/2742 [55:28<07:45,  1.05it/s] 82%|████████▏ | 2253/2742 [55:29<07:41,  1.06it/s] 82%|████████▏ | 2254/2742 [55:29<07:38,  1.06it/s] 82%|████████▏ | 2255/2742 [55:30<07:41,  1.06it/s]                                                   {'loss': 1.1553, 'learning_rate': 3.837339338647639e-06, 'epoch': 2.47}
 82%|████████▏ | 2255/2742 [55:30<07:41,  1.06it/s] 82%|████████▏ | 2256/2742 [55:31<07:35,  1.07it/s] 82%|████████▏ | 2257/2742 [55:32<07:32,  1.07it/s] 82%|████████▏ | 2258/2742 [55:33<07:35,  1.06it/s] 82%|████████▏ | 2259/2742 [55:34<07:41,  1.05it/s] 82%|████████▏ | 2260/2742 [55:35<07:36,  1.06it/s]                                                   {'loss': 1.1395, 'learning_rate': 3.761441730193213e-06, 'epoch': 2.47}
 82%|████████▏ | 2260/2742 [55:35<07:36,  1.06it/s] 82%|████████▏ | 2261/2742 [55:36<07:39,  1.05it/s] 82%|████████▏ | 2262/2742 [55:37<07:57,  1.01it/s] 83%|████████▎ | 2263/2742 [55:38<08:00,  1.00s/it] 83%|████████▎ | 2264/2742 [55:39<08:04,  1.01s/it] 83%|████████▎ | 2265/2742 [55:40<08:19,  1.05s/it]                                                   {'loss': 1.2251, 'learning_rate': 3.686241115190328e-06, 'epoch': 2.48}
 83%|████████▎ | 2265/2742 [55:40<08:19,  1.05s/it] 83%|████████▎ | 2266/2742 [55:41<08:03,  1.01s/it] 83%|████████▎ | 2267/2742 [55:42<07:45,  1.02it/s] 83%|████████▎ | 2268/2742 [55:43<07:33,  1.05it/s] 83%|████████▎ | 2269/2742 [55:44<07:42,  1.02it/s] 83%|████████▎ | 2270/2742 [55:45<07:43,  1.02it/s]                                                   {'loss': 0.9765, 'learning_rate': 3.611739961524771e-06, 'epoch': 2.48}
 83%|████████▎ | 2270/2742 [55:45<07:43,  1.02it/s] 83%|████████▎ | 2271/2742 [55:46<07:39,  1.02it/s] 83%|████████▎ | 2272/2742 [55:47<07:29,  1.04it/s] 83%|████████▎ | 2273/2742 [55:48<07:27,  1.05it/s] 83%|████████▎ | 2274/2742 [55:49<07:16,  1.07it/s] 83%|████████▎ | 2275/2742 [55:50<07:09,  1.09it/s]                                                   {'loss': 1.0, 'learning_rate': 3.5379407141278547e-06, 'epoch': 2.49}
 83%|████████▎ | 2275/2742 [55:50<07:09,  1.09it/s] 83%|████████▎ | 2276/2742 [55:51<07:10,  1.08it/s] 83%|████████▎ | 2277/2742 [55:52<07:36,  1.02it/s] 83%|████████▎ | 2278/2742 [55:53<07:50,  1.01s/it] 83%|████████▎ | 2279/2742 [55:54<07:40,  1.01it/s] 83%|████████▎ | 2280/2742 [55:55<07:35,  1.02it/s]                                                   {'loss': 1.063, 'learning_rate': 3.4648457948961792e-06, 'epoch': 2.49}
 83%|████████▎ | 2280/2742 [55:55<07:35,  1.02it/s] 83%|████████▎ | 2281/2742 [55:56<07:27,  1.03it/s] 83%|████████▎ | 2282/2742 [55:57<07:29,  1.02it/s] 83%|████████▎ | 2283/2742 [55:58<07:44,  1.01s/it] 83%|████████▎ | 2284/2742 [55:59<07:31,  1.02it/s] 83%|████████▎ | 2285/2742 [56:00<07:22,  1.03it/s]                                                   {'loss': 1.1315, 'learning_rate': 3.3924576026121645e-06, 'epoch': 2.5}
 83%|████████▎ | 2285/2742 [56:00<07:22,  1.03it/s] 83%|████████▎ | 2286/2742 [56:01<07:25,  1.02it/s] 83%|████████▎ | 2287/2742 [56:02<07:17,  1.04it/s] 83%|████████▎ | 2288/2742 [56:03<07:13,  1.05it/s] 83%|████████▎ | 2289/2742 [56:03<07:16,  1.04it/s] 84%|████████▎ | 2290/2742 [56:04<07:09,  1.05it/s]                                                   {'loss': 1.2453, 'learning_rate': 3.320778512865316e-06, 'epoch': 2.5}
 84%|████████▎ | 2290/2742 [56:04<07:09,  1.05it/s] 84%|████████▎ | 2291/2742 [56:10<16:41,  2.22s/it] 84%|████████▎ | 2292/2742 [56:11<13:52,  1.85s/it] 84%|████████▎ | 2293/2742 [56:12<11:52,  1.59s/it] 84%|████████▎ | 2294/2742 [56:13<10:41,  1.43s/it] 84%|████████▎ | 2295/2742 [56:14<09:34,  1.28s/it]                                                   {'loss': 1.2681, 'learning_rate': 3.2498108779742435e-06, 'epoch': 2.51}
 84%|████████▎ | 2295/2742 [56:14<09:34,  1.28s/it] 84%|████████▎ | 2296/2742 [56:15<08:49,  1.19s/it] 84%|████████▍ | 2297/2742 [56:15<08:18,  1.12s/it] 84%|████████▍ | 2298/2742 [56:16<07:52,  1.06s/it] 84%|████████▍ | 2299/2742 [56:17<07:45,  1.05s/it] 84%|████████▍ | 2300/2742 [56:18<07:31,  1.02s/it]                                                   {'loss': 1.2045, 'learning_rate': 3.1795570269095103e-06, 'epoch': 2.51}
 84%|████████▍ | 2300/2742 [56:18<07:31,  1.02s/it][INFO|trainer.py:2881] 2023-11-06 14:54:39,569 >> Saving model checkpoint to saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2300
[INFO|tokenization_utils_base.py:2428] 2023-11-06 14:54:39,620 >> tokenizer config file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-06 14:54:39,620 >> Special tokens file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2300/special_tokens_map.json
[2023-11-06 14:54:42,100] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2300 is about to be saved!
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-11-06 14:54:49,413] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2300/global_step2300/mp_rank_00_model_states.pt
[2023-11-06 14:54:49,413] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2300/global_step2300/mp_rank_00_model_states.pt...
[2023-11-06 14:55:27,855] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2300/global_step2300/mp_rank_00_model_states.pt.
[2023-11-06 14:55:28,461] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2300/global_step2300/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 14:55:28,656] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2300/global_step2300/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 14:55:28,659] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2300/global_step2300/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-11-06 14:55:28,659] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2300 is ready now!
 84%|████████▍ | 2301/2742 [57:19<2:17:59, 18.77s/it] 84%|████████▍ | 2302/2742 [57:20<1:38:42, 13.46s/it] 84%|████████▍ | 2303/2742 [57:21<1:11:15,  9.74s/it] 84%|████████▍ | 2304/2742 [57:22<51:51,  7.10s/it]   84%|████████▍ | 2305/2742 [57:23<38:21,  5.27s/it]                                                   {'loss': 1.2205, 'learning_rate': 3.1100192652171595e-06, 'epoch': 2.52}
 84%|████████▍ | 2305/2742 [57:23<38:21,  5.27s/it] 84%|████████▍ | 2306/2742 [57:24<28:53,  3.98s/it] 84%|████████▍ | 2307/2742 [57:25<22:18,  3.08s/it] 84%|████████▍ | 2308/2742 [57:26<17:40,  2.44s/it] 84%|████████▍ | 2309/2742 [57:26<14:20,  1.99s/it] 84%|████████▍ | 2310/2742 [57:27<12:06,  1.68s/it]                                                   {'loss': 1.0125, 'learning_rate': 3.0411998749430848e-06, 'epoch': 2.53}
 84%|████████▍ | 2310/2742 [57:27<12:06,  1.68s/it] 84%|████████▍ | 2311/2742 [57:28<10:30,  1.46s/it] 84%|████████▍ | 2312/2742 [57:29<09:32,  1.33s/it] 84%|████████▍ | 2313/2742 [57:31<09:07,  1.28s/it] 84%|████████▍ | 2314/2742 [57:31<08:20,  1.17s/it] 84%|████████▍ | 2315/2742 [57:32<07:49,  1.10s/it]                                                   {'loss': 1.0583, 'learning_rate': 2.9731011145581134e-06, 'epoch': 2.53}
 84%|████████▍ | 2315/2742 [57:32<07:49,  1.10s/it] 84%|████████▍ | 2316/2742 [57:33<07:43,  1.09s/it] 85%|████████▍ | 2317/2742 [57:35<07:32,  1.06s/it] 85%|████████▍ | 2318/2742 [57:35<07:09,  1.01s/it] 85%|████████▍ | 2319/2742 [57:36<07:02,  1.00it/s] 85%|████████▍ | 2320/2742 [57:37<06:50,  1.03it/s]                                                   {'loss': 1.1084, 'learning_rate': 2.9057252188839007e-06, 'epoch': 2.54}
 85%|████████▍ | 2320/2742 [57:37<06:50,  1.03it/s] 85%|████████▍ | 2321/2742 [57:38<06:54,  1.02it/s] 85%|████████▍ | 2322/2742 [57:39<07:00,  1.00s/it] 85%|████████▍ | 2323/2742 [57:40<06:51,  1.02it/s] 85%|████████▍ | 2324/2742 [57:41<06:41,  1.04it/s] 85%|████████▍ | 2325/2742 [57:42<06:39,  1.04it/s]                                                   {'loss': 1.0231, 'learning_rate': 2.8390743990196063e-06, 'epoch': 2.54}
 85%|████████▍ | 2325/2742 [57:42<06:39,  1.04it/s] 85%|████████▍ | 2326/2742 [57:43<06:36,  1.05it/s] 85%|████████▍ | 2327/2742 [57:44<06:37,  1.05it/s] 85%|████████▍ | 2328/2742 [57:45<06:29,  1.06it/s] 85%|████████▍ | 2329/2742 [57:46<06:26,  1.07it/s] 85%|████████▍ | 2330/2742 [57:47<06:41,  1.03it/s]                                                   {'loss': 1.0798, 'learning_rate': 2.773150842269287e-06, 'epoch': 2.55}
 85%|████████▍ | 2330/2742 [57:47<06:41,  1.03it/s] 85%|████████▌ | 2331/2742 [57:48<06:32,  1.05it/s] 85%|████████▌ | 2332/2742 [57:49<06:24,  1.07it/s] 85%|████████▌ | 2333/2742 [57:50<06:32,  1.04it/s] 85%|████████▌ | 2334/2742 [57:51<06:23,  1.06it/s] 85%|████████▌ | 2335/2742 [57:52<06:19,  1.07it/s]                                                   {'loss': 0.9753, 'learning_rate': 2.7079567120701697e-06, 'epoch': 2.55}
 85%|████████▌ | 2335/2742 [57:52<06:19,  1.07it/s] 85%|████████▌ | 2336/2742 [57:52<06:17,  1.08it/s] 85%|████████▌ | 2337/2742 [57:53<06:24,  1.05it/s] 85%|████████▌ | 2338/2742 [57:54<06:19,  1.06it/s] 85%|████████▌ | 2339/2742 [57:55<06:19,  1.06it/s] 85%|████████▌ | 2340/2742 [57:56<06:14,  1.07it/s]                                                   {'loss': 0.977, 'learning_rate': 2.6434941479216072e-06, 'epoch': 2.56}
 85%|████████▌ | 2340/2742 [57:56<06:14,  1.07it/s] 85%|████████▌ | 2341/2742 [57:57<06:15,  1.07it/s] 85%|████████▌ | 2342/2742 [57:58<06:18,  1.06it/s] 85%|████████▌ | 2343/2742 [57:59<06:10,  1.08it/s] 85%|████████▌ | 2344/2742 [58:00<06:37,  1.00it/s] 86%|████████▌ | 2345/2742 [58:01<06:26,  1.03it/s]                                                   {'loss': 1.2864, 'learning_rate': 2.5797652653149014e-06, 'epoch': 2.56}
 86%|████████▌ | 2345/2742 [58:01<06:26,  1.03it/s] 86%|████████▌ | 2346/2742 [58:02<06:28,  1.02it/s] 86%|████████▌ | 2347/2742 [58:03<06:24,  1.03it/s] 86%|████████▌ | 2348/2742 [58:04<06:36,  1.01s/it] 86%|████████▌ | 2349/2742 [58:05<07:07,  1.09s/it] 86%|████████▌ | 2350/2742 [58:06<06:51,  1.05s/it]                                                   {'loss': 1.2499, 'learning_rate': 2.5167721556638435e-06, 'epoch': 2.57}
 86%|████████▌ | 2350/2742 [58:06<06:51,  1.05s/it] 86%|████████▌ | 2351/2742 [58:07<06:36,  1.01s/it] 86%|████████▌ | 2352/2742 [58:08<06:26,  1.01it/s] 86%|████████▌ | 2353/2742 [58:09<06:13,  1.04it/s] 86%|████████▌ | 2354/2742 [58:10<06:21,  1.02it/s] 86%|████████▌ | 2355/2742 [58:11<06:14,  1.03it/s]                                                   {'loss': 1.1761, 'learning_rate': 2.4545168862361024e-06, 'epoch': 2.57}
 86%|████████▌ | 2355/2742 [58:11<06:14,  1.03it/s] 86%|████████▌ | 2356/2742 [58:12<06:10,  1.04it/s] 86%|████████▌ | 2357/2742 [58:13<06:25,  1.00s/it] 86%|████████▌ | 2358/2742 [58:14<06:24,  1.00s/it] 86%|████████▌ | 2359/2742 [58:15<06:20,  1.01it/s] 86%|████████▌ | 2360/2742 [58:16<06:28,  1.02s/it]                                                   {'loss': 1.3343, 'learning_rate': 2.393001500085393e-06, 'epoch': 2.58}
 86%|████████▌ | 2360/2742 [58:16<06:28,  1.02s/it] 86%|████████▌ | 2361/2742 [58:17<06:18,  1.01it/s] 86%|████████▌ | 2362/2742 [58:18<06:13,  1.02it/s] 86%|████████▌ | 2363/2742 [58:19<06:02,  1.04it/s] 86%|████████▌ | 2364/2742 [58:20<05:55,  1.06it/s] 86%|████████▋ | 2365/2742 [58:21<06:03,  1.04it/s]                                                   {'loss': 1.0167, 'learning_rate': 2.332228015984386e-06, 'epoch': 2.59}
 86%|████████▋ | 2365/2742 [58:21<06:03,  1.04it/s] 86%|████████▋ | 2366/2742 [58:22<05:57,  1.05it/s] 86%|████████▋ | 2367/2742 [58:23<05:54,  1.06it/s] 86%|████████▋ | 2368/2742 [58:24<05:52,  1.06it/s] 86%|████████▋ | 2369/2742 [58:25<05:56,  1.05it/s] 86%|████████▋ | 2370/2742 [58:28<10:26,  1.68s/it]                                                   {'loss': 1.0816, 'learning_rate': 2.272198428358516e-06, 'epoch': 2.59}
 86%|████████▋ | 2370/2742 [58:28<10:26,  1.68s/it] 86%|████████▋ | 2371/2742 [58:29<09:02,  1.46s/it] 87%|████████▋ | 2372/2742 [58:30<08:01,  1.30s/it] 87%|████████▋ | 2373/2742 [58:31<07:17,  1.19s/it] 87%|████████▋ | 2374/2742 [58:32<07:03,  1.15s/it] 87%|████████▋ | 2375/2742 [58:33<06:43,  1.10s/it]                                                   {'loss': 1.2037, 'learning_rate': 2.2129147072204646e-06, 'epoch': 2.6}
 87%|████████▋ | 2375/2742 [58:33<06:43,  1.10s/it] 87%|████████▋ | 2376/2742 [58:34<06:31,  1.07s/it] 87%|████████▋ | 2377/2742 [58:35<06:22,  1.05s/it] 87%|████████▋ | 2378/2742 [58:36<06:13,  1.03s/it] 87%|████████▋ | 2379/2742 [58:37<06:05,  1.01s/it] 87%|████████▋ | 2380/2742 [58:38<06:00,  1.00it/s]                                                   {'loss': 1.0855, 'learning_rate': 2.1543787981055717e-06, 'epoch': 2.6}
 87%|████████▋ | 2380/2742 [58:38<06:00,  1.00it/s] 87%|████████▋ | 2381/2742 [58:39<05:57,  1.01it/s] 87%|████████▋ | 2382/2742 [58:40<05:48,  1.03it/s] 87%|████████▋ | 2383/2742 [58:41<05:49,  1.03it/s] 87%|████████▋ | 2384/2742 [58:42<06:01,  1.01s/it] 87%|████████▋ | 2385/2742 [58:43<06:02,  1.02s/it]                                                   {'loss': 1.2638, 'learning_rate': 2.0965926220079398e-06, 'epoch': 2.61}
 87%|████████▋ | 2385/2742 [58:43<06:02,  1.02s/it] 87%|████████▋ | 2386/2742 [58:44<05:57,  1.00s/it] 87%|████████▋ | 2387/2742 [58:45<05:52,  1.01it/s] 87%|████████▋ | 2388/2742 [58:46<05:53,  1.00it/s] 87%|████████▋ | 2389/2742 [58:47<05:46,  1.02it/s] 87%|████████▋ | 2390/2742 [58:48<06:01,  1.03s/it]                                                   {'loss': 1.1733, 'learning_rate': 2.039558075317419e-06, 'epoch': 2.61}
 87%|████████▋ | 2390/2742 [58:48<06:01,  1.03s/it] 87%|████████▋ | 2391/2742 [58:49<05:53,  1.01s/it] 87%|████████▋ | 2392/2742 [58:50<05:52,  1.01s/it] 87%|████████▋ | 2393/2742 [58:51<05:43,  1.02it/s] 87%|████████▋ | 2394/2742 [58:52<06:07,  1.06s/it] 87%|████████▋ | 2395/2742 [58:53<05:53,  1.02s/it]                                                   {'loss': 1.1731, 'learning_rate': 1.9832770297573687e-06, 'epoch': 2.62}
 87%|████████▋ | 2395/2742 [58:53<05:53,  1.02s/it] 87%|████████▋ | 2396/2742 [58:54<05:43,  1.01it/s] 87%|████████▋ | 2397/2742 [58:55<05:35,  1.03it/s] 87%|████████▋ | 2398/2742 [58:56<05:33,  1.03it/s] 87%|████████▋ | 2399/2742 [58:57<05:40,  1.01it/s] 88%|████████▊ | 2400/2742 [58:58<05:37,  1.01it/s]                                                   {'loss': 1.0263, 'learning_rate': 1.9277513323232215e-06, 'epoch': 2.62}
 88%|████████▊ | 2400/2742 [58:58<05:37,  1.01it/s][INFO|trainer.py:2881] 2023-11-06 14:57:24,139 >> Saving model checkpoint to saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2400
[INFO|tokenization_utils_base.py:2428] 2023-11-06 14:57:24,280 >> tokenizer config file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-06 14:57:24,281 >> Special tokens file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2400/special_tokens_map.json
[2023-11-06 14:57:25,404] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2400 is about to be saved!
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-11-06 14:57:34,259] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2400/global_step2400/mp_rank_00_model_states.pt
[2023-11-06 14:57:34,260] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2400/global_step2400/mp_rank_00_model_states.pt...
[2023-11-06 14:58:08,033] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2400/global_step2400/mp_rank_00_model_states.pt.
[2023-11-06 14:58:08,807] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2400/global_step2400/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 14:58:08,879] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2400/global_step2400/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 14:58:08,882] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2400/global_step2400/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-11-06 14:58:08,882] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2400 is ready now!
 88%|████████▊ | 2401/2742 [59:59<1:48:29, 19.09s/it] 88%|████████▊ | 2402/2742 [1:00:00<1:17:23, 13.66s/it] 88%|████████▊ | 2403/2742 [1:00:01<55:44,  9.86s/it]   88%|████████▊ | 2404/2742 [1:00:02<40:32,  7.20s/it] 88%|████████▊ | 2405/2742 [1:00:03<30:06,  5.36s/it]                                                     {'loss': 1.0988, 'learning_rate': 1.8729828052218846e-06, 'epoch': 2.63}
 88%|████████▊ | 2405/2742 [1:00:03<30:06,  5.36s/it] 88%|████████▊ | 2406/2742 [1:00:04<22:47,  4.07s/it] 88%|████████▊ | 2407/2742 [1:00:19<40:38,  7.28s/it] 88%|████████▊ | 2408/2742 [1:00:20<30:33,  5.49s/it] 88%|████████▊ | 2409/2742 [1:00:22<23:28,  4.23s/it] 88%|████████▊ | 2410/2742 [1:00:23<18:15,  3.30s/it]                                                     {'loss': 1.174, 'learning_rate': 1.8189732458119252e-06, 'epoch': 2.63}
 88%|████████▊ | 2410/2742 [1:00:23<18:15,  3.30s/it] 88%|████████▊ | 2411/2742 [1:00:24<14:38,  2.65s/it] 88%|████████▊ | 2412/2742 [1:00:25<12:02,  2.19s/it] 88%|████████▊ | 2413/2742 [1:00:26<10:08,  1.85s/it] 88%|████████▊ | 2414/2742 [1:00:27<08:46,  1.60s/it] 88%|████████▊ | 2415/2742 [1:00:28<08:11,  1.50s/it]                                                     {'loss': 1.0039, 'learning_rate': 1.765724426544596e-06, 'epoch': 2.64}
 88%|████████▊ | 2415/2742 [1:00:28<08:11,  1.50s/it] 88%|████████▊ | 2416/2742 [1:00:29<07:27,  1.37s/it] 88%|████████▊ | 2417/2742 [1:00:30<06:41,  1.24s/it] 88%|████████▊ | 2418/2742 [1:00:31<06:11,  1.15s/it] 88%|████████▊ | 2419/2742 [1:00:32<05:49,  1.08s/it] 88%|████████▊ | 2420/2742 [1:00:33<05:33,  1.04s/it]                                                     {'loss': 1.0285, 'learning_rate': 1.7132380949056752e-06, 'epoch': 2.65}
 88%|████████▊ | 2420/2742 [1:00:33<05:33,  1.04s/it] 88%|████████▊ | 2421/2742 [1:00:34<05:21,  1.00s/it] 88%|████████▊ | 2422/2742 [1:00:35<05:17,  1.01it/s] 88%|████████▊ | 2423/2742 [1:00:36<05:25,  1.02s/it] 88%|████████▊ | 2424/2742 [1:00:37<05:33,  1.05s/it] 88%|████████▊ | 2425/2742 [1:00:38<05:22,  1.02s/it]                                                     {'loss': 1.0505, 'learning_rate': 1.661515973358091e-06, 'epoch': 2.65}
 88%|████████▊ | 2425/2742 [1:00:38<05:22,  1.02s/it] 88%|████████▊ | 2426/2742 [1:00:39<05:24,  1.03s/it] 89%|████████▊ | 2427/2742 [1:00:40<05:23,  1.03s/it] 89%|████████▊ | 2428/2742 [1:00:41<05:16,  1.01s/it] 89%|████████▊ | 2429/2742 [1:00:42<05:16,  1.01s/it] 89%|████████▊ | 2430/2742 [1:00:43<05:24,  1.04s/it]                                                     {'loss': 1.1451, 'learning_rate': 1.6105597592854238e-06, 'epoch': 2.66}
 89%|████████▊ | 2430/2742 [1:00:43<05:24,  1.04s/it] 89%|████████▊ | 2431/2742 [1:00:44<05:13,  1.01s/it] 89%|████████▊ | 2432/2742 [1:00:45<05:06,  1.01it/s] 89%|████████▊ | 2433/2742 [1:00:46<05:10,  1.00s/it] 89%|████████▉ | 2434/2742 [1:00:47<05:07,  1.00it/s] 89%|████████▉ | 2435/2742 [1:00:48<05:07,  1.00s/it]                                                     {'loss': 1.1142, 'learning_rate': 1.560371124936183e-06, 'epoch': 2.66}
 89%|████████▉ | 2435/2742 [1:00:48<05:07,  1.00s/it] 89%|████████▉ | 2436/2742 [1:00:49<05:03,  1.01it/s] 89%|████████▉ | 2437/2742 [1:00:50<04:58,  1.02it/s] 89%|████████▉ | 2438/2742 [1:00:51<04:57,  1.02it/s] 89%|████████▉ | 2439/2742 [1:00:52<05:01,  1.01it/s] 89%|████████▉ | 2440/2742 [1:00:53<04:59,  1.01it/s]                                                     {'loss': 1.1767, 'learning_rate': 1.510951717368947e-06, 'epoch': 2.67}
 89%|████████▉ | 2440/2742 [1:00:53<04:59,  1.01it/s] 89%|████████▉ | 2441/2742 [1:00:54<05:00,  1.00it/s] 89%|████████▉ | 2442/2742 [1:00:55<05:02,  1.01s/it] 89%|████████▉ | 2443/2742 [1:00:56<05:05,  1.02s/it] 89%|████████▉ | 2444/2742 [1:00:57<05:09,  1.04s/it] 89%|████████▉ | 2445/2742 [1:00:58<05:06,  1.03s/it]                                                     {'loss': 1.1435, 'learning_rate': 1.4623031583982982e-06, 'epoch': 2.67}
 89%|████████▉ | 2445/2742 [1:00:58<05:06,  1.03s/it] 89%|████████▉ | 2446/2742 [1:00:59<05:03,  1.02s/it] 89%|████████▉ | 2447/2742 [1:01:00<04:59,  1.02s/it] 89%|████████▉ | 2448/2742 [1:01:01<04:56,  1.01s/it] 89%|████████▉ | 2449/2742 [1:01:02<04:55,  1.01s/it] 89%|████████▉ | 2450/2742 [1:01:03<04:55,  1.01s/it]                                                     {'loss': 1.2202, 'learning_rate': 1.4144270445415942e-06, 'epoch': 2.68}
 89%|████████▉ | 2450/2742 [1:01:03<04:55,  1.01s/it] 89%|████████▉ | 2451/2742 [1:01:04<04:49,  1.00it/s] 89%|████████▉ | 2452/2742 [1:01:06<05:41,  1.18s/it] 89%|████████▉ | 2453/2742 [1:01:07<05:25,  1.12s/it] 89%|████████▉ | 2454/2742 [1:01:08<05:19,  1.11s/it] 90%|████████▉ | 2455/2742 [1:01:09<05:09,  1.08s/it]                                                     {'loss': 1.0219, 'learning_rate': 1.3673249469665888e-06, 'epoch': 2.68}
 90%|████████▉ | 2455/2742 [1:01:09<05:09,  1.08s/it] 90%|████████▉ | 2456/2742 [1:01:10<04:57,  1.04s/it] 90%|████████▉ | 2457/2742 [1:01:12<05:57,  1.25s/it] 90%|████████▉ | 2458/2742 [1:01:13<05:43,  1.21s/it] 90%|████████▉ | 2459/2742 [1:01:14<05:20,  1.13s/it] 90%|████████▉ | 2460/2742 [1:01:15<05:17,  1.13s/it]                                                     {'loss': 1.0716, 'learning_rate': 1.3209984114398638e-06, 'epoch': 2.69}
 90%|████████▉ | 2460/2742 [1:01:15<05:17,  1.13s/it] 90%|████████▉ | 2461/2742 [1:01:16<05:01,  1.07s/it] 90%|████████▉ | 2462/2742 [1:01:17<05:13,  1.12s/it] 90%|████████▉ | 2463/2742 [1:01:18<04:58,  1.07s/it] 90%|████████▉ | 2464/2742 [1:01:19<04:50,  1.04s/it] 90%|████████▉ | 2465/2742 [1:01:20<04:42,  1.02s/it]                                                     {'loss': 1.0159, 'learning_rate': 1.2754489582760904e-06, 'epoch': 2.69}
 90%|████████▉ | 2465/2742 [1:01:20<04:42,  1.02s/it] 90%|████████▉ | 2466/2742 [1:01:21<04:49,  1.05s/it] 90%|████████▉ | 2467/2742 [1:01:22<04:58,  1.09s/it] 90%|█████████ | 2468/2742 [1:01:23<04:45,  1.04s/it] 90%|█████████ | 2469/2742 [1:01:24<04:52,  1.07s/it] 90%|█████████ | 2470/2742 [1:01:25<04:38,  1.02s/it]                                                     {'loss': 1.1153, 'learning_rate': 1.2306780822881585e-06, 'epoch': 2.7}
 90%|█████████ | 2470/2742 [1:01:25<04:38,  1.02s/it] 90%|█████████ | 2471/2742 [1:01:26<04:36,  1.02s/it] 90%|█████████ | 2472/2742 [1:01:27<04:30,  1.00s/it] 90%|█████████ | 2473/2742 [1:01:28<04:23,  1.02it/s] 90%|█████████ | 2474/2742 [1:01:29<04:51,  1.09s/it] 90%|█████████ | 2475/2742 [1:01:30<04:46,  1.07s/it]                                                     {'loss': 1.1164, 'learning_rate': 1.186687252738103e-06, 'epoch': 2.71}
 90%|█████████ | 2475/2742 [1:01:30<04:46,  1.07s/it] 90%|█████████ | 2476/2742 [1:01:31<04:38,  1.05s/it] 90%|█████████ | 2477/2742 [1:01:33<04:38,  1.05s/it] 90%|█████████ | 2478/2742 [1:01:33<04:27,  1.01s/it] 90%|█████████ | 2479/2742 [1:01:34<04:23,  1.00s/it] 90%|█████████ | 2480/2742 [1:01:35<04:28,  1.02s/it]                                                     {'loss': 0.9967, 'learning_rate': 1.143477913288901e-06, 'epoch': 2.71}
 90%|█████████ | 2480/2742 [1:01:35<04:28,  1.02s/it] 90%|█████████ | 2481/2742 [1:01:36<04:21,  1.00s/it] 91%|█████████ | 2482/2742 [1:01:37<04:14,  1.02it/s] 91%|█████████ | 2483/2742 [1:01:38<04:11,  1.03it/s] 91%|█████████ | 2484/2742 [1:01:39<04:15,  1.01it/s] 91%|█████████ | 2485/2742 [1:01:40<04:09,  1.03it/s]                                                     {'loss': 1.1037, 'learning_rate': 1.1010514819570716e-06, 'epoch': 2.72}
 91%|█████████ | 2485/2742 [1:01:40<04:09,  1.03it/s] 91%|█████████ | 2486/2742 [1:01:41<04:23,  1.03s/it] 91%|█████████ | 2487/2742 [1:01:43<04:30,  1.06s/it] 91%|█████████ | 2488/2742 [1:01:44<04:22,  1.03s/it] 91%|█████████ | 2489/2742 [1:01:44<04:15,  1.01s/it] 91%|█████████ | 2490/2742 [1:01:46<04:18,  1.03s/it]                                                     {'loss': 1.2074, 'learning_rate': 1.0594093510661708e-06, 'epoch': 2.72}
 91%|█████████ | 2490/2742 [1:01:46<04:18,  1.03s/it] 91%|█████████ | 2491/2742 [1:01:47<04:15,  1.02s/it] 91%|█████████ | 2492/2742 [1:01:48<04:09,  1.00it/s] 91%|█████████ | 2493/2742 [1:01:48<04:07,  1.01it/s] 91%|█████████ | 2494/2742 [1:01:49<04:02,  1.02it/s] 91%|█████████ | 2495/2742 [1:01:50<04:03,  1.01it/s]                                                     {'loss': 1.082, 'learning_rate': 1.0185528872010659e-06, 'epoch': 2.73}
 91%|█████████ | 2495/2742 [1:01:50<04:03,  1.01it/s] 91%|█████████ | 2496/2742 [1:01:52<04:15,  1.04s/it] 91%|█████████ | 2497/2742 [1:01:53<04:11,  1.03s/it] 91%|█████████ | 2498/2742 [1:01:54<04:12,  1.03s/it] 91%|█████████ | 2499/2742 [1:01:55<04:05,  1.01s/it] 91%|█████████ | 2500/2742 [1:01:56<04:10,  1.04s/it]                                                     {'loss': 1.3109, 'learning_rate': 9.784834311631241e-07, 'epoch': 2.73}
 91%|█████████ | 2500/2742 [1:01:56<04:10,  1.04s/it][INFO|trainer.py:2881] 2023-11-06 15:00:19,855 >> Saving model checkpoint to saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2500
[INFO|tokenization_utils_base.py:2428] 2023-11-06 15:00:19,971 >> tokenizer config file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-06 15:00:19,972 >> Special tokens file saved in saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2500/special_tokens_map.json
[2023-11-06 15:00:20,857] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2500 is about to be saved!
/root/miniconda3/envs/llama/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-11-06 15:00:29,991] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2500/global_step2500/mp_rank_00_model_states.pt
[2023-11-06 15:00:29,992] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2500/global_step2500/mp_rank_00_model_states.pt...
[2023-11-06 15:01:02,062] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2500/global_step2500/mp_rank_00_model_states.pt.
[2023-11-06 15:01:02,675] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2500/global_step2500/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 15:01:02,727] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2500/global_step2500/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 15:01:02,730] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved saves/Qwen-14B-Chat/lora/2023-11-06-11-11-296/checkpoint-2500/global_step2500/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-11-06 15:01:02,730] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2500 is ready now!
 91%|█████████ | 2501/2742 [1:02:53<1:11:43, 17.86s/it] 91%|█████████ | 2502/2742 [1:02:54<51:15, 12.81s/it]   91%|█████████▏| 2503/2742 [1:02:55<36:56,  9.27s/it] 91%|█████████▏| 2504/2742 [1:02:56<26:53,  6.78s/it] 91%|█████████▏| 2505/2742 [1:02:57<19:51,  5.03s/it]                                                     {'loss': 1.1372, 'learning_rate': 9.392022979261811e-07, 'epoch': 2.74}
 91%|█████████▏| 2505/2742 [1:02:57<19:51,  5.03s/it] 91%|█████████▏| 2506/2742 [1:02:58<14:58,  3.81s/it] 91%|█████████▏| 2507/2742 [1:02:59<11:41,  2.98s/it] 91%|█████████▏| 2508/2742 [1:03:00<09:19,  2.39s/it] 92%|█████████▏| 2509/2742 [1:03:01<07:37,  1.96s/it] 92%|█████████▏| 2510/2742 [1:03:02<06:25,  1.66s/it]                                                     {'loss': 1.2251, 'learning_rate': 9.007107765933914e-07, 'epoch': 2.74}
 92%|█████████▏| 2510/2742 [1:03:02<06:25,  1.66s/it] 92%|█████████▏| 2511/2742 [1:03:03<05:34,  1.45s/it] 92%|█████████▏| 2512/2742 [1:03:04<05:04,  1.32s/it] 92%|█████████▏| 2513/2742 [1:03:05<04:51,  1.27s/it] 92%|█████████▏| 2514/2742 [1:03:06<04:32,  1.19s/it] 92%|█████████▏| 2515/2742 [1:03:07<04:17,  1.14s/it]                                                     {'loss': 1.1966, 'learning_rate': 8.630101303549493e-07, 'epoch': 2.75}
 92%|█████████▏| 2515/2742 [1:03:07<04:17,  1.14s/it] 92%|█████████▏| 2516/2742 [1:03:08<04:21,  1.16s/it] 92%|█████████▏| 2517/2742 [1:03:09<04:12,  1.12s/it] 92%|█████████▏| 2518/2742 [1:03:10<04:20,  1.16s/it] 92%|█████████▏| 2519/2742 [1:03:11<04:06,  1.10s/it] 92%|█████████▏| 2520/2742 [1:03:12<04:03,  1.09s/it]                                                     {'loss': 1.0947, 'learning_rate': 8.261015964465934e-07, 'epoch': 2.75}
 92%|█████████▏| 2520/2742 [1:03:12<04:03,  1.09s/it] 92%|█████████▏| 2521/2742 [1:03:13<03:52,  1.05s/it] 92%|█████████▏| 2522/2742 [1:03:14<03:49,  1.04s/it] 92%|█████████▏| 2523/2742 [1:03:15<03:41,  1.01s/it] 92%|█████████▏| 2524/2742 [1:03:16<03:38,  1.00s/it] 92%|█████████▏| 2525/2742 [1:03:17<03:33,  1.01it/s]                                                     {'loss': 1.0835, 'learning_rate': 7.899863861090428e-07, 'epoch': 2.76}
 92%|█████████▏| 2525/2742 [1:03:17<03:33,  1.01it/s] 92%|█████████▏| 2526/2742 [1:03:18<03:31,  1.02it/s] 92%|█████████▏| 2527/2742 [1:03:19<03:27,  1.04it/s] 92%|█████████▏| 2528/2742 [1:03:20<03:30,  1.02it/s] 92%|█████████▏| 2529/2742 [1:03:21<03:28,  1.02it/s] 92%|█████████▏| 2530/2742 [1:03:22<03:43,  1.05s/it]                                                     {'loss': 0.9725, 'learning_rate': 7.546656845482197e-07, 'epoch': 2.77}
 92%|█████████▏| 2530/2742 [1:03:22<03:43,  1.05s/it] 92%|█████████▏| 2531/2742 [1:03:24<03:52,  1.10s/it] 92%|█████████▏| 2532/2742 [1:03:25<03:44,  1.07s/it] 92%|█████████▏| 2533/2742 [1:03:26<04:04,  1.17s/it] 92%|█████████▏| 2534/2742 [1:03:27<03:50,  1.11s/it] 92%|█████████▏| 2535/2742 [1:03:28<03:38,  1.05s/it]                                                     {'loss': 1.0406, 'learning_rate': 7.201406508963698e-07, 'epoch': 2.77}
 92%|█████████▏| 2535/2742 [1:03:28<03:38,  1.05s/it] 92%|█████████▏| 2536/2742 [1:03:29<03:30,  1.02s/it] 93%|█████████▎| 2537/2742 [1:03:30<03:28,  1.02s/it] 93%|█████████▎| 2538/2742 [1:03:31<03:21,  1.01it/s] 93%|█████████▎| 2539/2742 [1:03:32<03:19,  1.02it/s] 93%|█████████▎| 2540/2742 [1:03:33<03:16,  1.03it/s]                                                     {'loss': 1.0837, 'learning_rate': 6.86412418174015e-07, 'epoch': 2.78}
 93%|█████████▎| 2540/2742 [1:03:33<03:16,  1.03it/s] 93%|█████████▎| 2541/2742 [1:03:34<03:15,  1.03it/s] 93%|█████████▎| 2542/2742 [1:03:35<03:14,  1.03it/s] 93%|█████████▎| 2543/2742 [1:03:36<03:12,  1.03it/s] 93%|█████████▎| 2544/2742 [1:03:37<03:15,  1.02it/s] 93%|█████████▎| 2545/2742 [1:03:38<03:15,  1.01it/s]                                                     {'loss': 0.9325, 'learning_rate': 6.534820932527663e-07, 'epoch': 2.78}
 93%|█████████▎| 2545/2742 [1:03:38<03:15,  1.01it/s] 93%|█████████▎| 2546/2742 [1:03:39<03:11,  1.02it/s] 93%|█████████▎| 2547/2742 [1:03:39<03:08,  1.03it/s] 93%|█████████▎| 2548/2742 [1:03:41<03:11,  1.01it/s] 93%|█████████▎| 2549/2742 [1:03:42<03:12,  1.01it/s] 93%|█████████▎| 2550/2742 [1:03:43<03:23,  1.06s/it]                                                     {'loss': 1.1403, 'learning_rate': 6.213507568190196e-07, 'epoch': 2.79}
 93%|█████████▎| 2550/2742 [1:03:43<03:23,  1.06s/it] 93%|█████████▎| 2551/2742 [1:03:44<03:15,  1.02s/it] 93%|█████████▎| 2552/2742 [1:03:45<03:16,  1.04s/it] 93%|█████████▎| 2553/2742 [1:03:46<03:17,  1.05s/it] 93%|█████████▎| 2554/2742 [1:03:47<03:12,  1.02s/it] 93%|█████████▎| 2555/2742 [1:03:48<03:04,  1.01it/s]                                                     {'loss': 1.1685, 'learning_rate': 5.90019463338462e-07, 'epoch': 2.79}
 93%|█████████▎| 2555/2742 [1:03:48<03:04,  1.01it/s] 93%|█████████▎| 2556/2742 [1:03:49<03:11,  1.03s/it] 93%|█████████▎| 2557/2742 [1:03:50<03:03,  1.01it/s] 93%|█████████▎| 2558/2742 [1:03:51<03:00,  1.02it/s] 93%|█████████▎| 2559/2742 [1:03:52<03:06,  1.02s/it] 93%|█████████▎| 2560/2742 [1:03:53<02:57,  1.02it/s]                                                     {'loss': 1.1491, 'learning_rate': 5.594892410214936e-07, 'epoch': 2.8}
 93%|█████████▎| 2560/2742 [1:03:53<02:57,  1.02it/s] 93%|█████████▎| 2561/2742 [1:03:54<02:58,  1.01it/s] 93%|█████████▎| 2562/2742 [1:03:55<02:56,  1.02it/s] 93%|█████████▎| 2563/2742 [1:03:56<03:10,  1.06s/it] 94%|█████████▎| 2564/2742 [1:03:57<03:10,  1.07s/it] 94%|█████████▎| 2565/2742 [1:03:58<03:02,  1.03s/it]                                                     {'loss': 1.1349, 'learning_rate': 5.29761091789463e-07, 'epoch': 2.8}
 94%|█████████▎| 2565/2742 [1:03:58<03:02,  1.03s/it] 94%|█████████▎| 2566/2742 [1:03:59<02:55,  1.00it/s] 94%|█████████▎| 2567/2742 [1:04:00<02:48,  1.04it/s] 94%|█████████▎| 2568/2742 [1:04:01<02:45,  1.05it/s] 94%|█████████▎| 2569/2742 [1:04:02<02:49,  1.02it/s] 94%|█████████▎| 2570/2742 [1:04:03<02:43,  1.05it/s]                                                     {'loss': 0.9695, 'learning_rate': 5.008359912418048e-07, 'epoch': 2.81}
 94%|█████████▎| 2570/2742 [1:04:03<02:43,  1.05it/s] 94%|█████████▍| 2571/2742 [1:04:03<02:41,  1.06it/s] 94%|█████████▍| 2572/2742 [1:04:04<02:42,  1.05it/s] 94%|█████████▍| 2573/2742 [1:04:05<02:41,  1.04it/s] 94%|█████████▍| 2574/2742 [1:04:06<02:42,  1.03it/s] 94%|█████████▍| 2575/2742 [1:04:07<02:39,  1.05it/s]                                                     {'loss': 1.0881, 'learning_rate': 4.727148886240068e-07, 'epoch': 2.81}
 94%|█████████▍| 2575/2742 [1:04:07<02:39,  1.05it/s] 94%|█████████▍| 2576/2742 [1:04:08<02:37,  1.06it/s] 94%|█████████▍| 2577/2742 [1:04:09<02:34,  1.07it/s] 94%|█████████▍| 2578/2742 [1:04:10<02:44,  1.00s/it] 94%|█████████▍| 2579/2742 [1:04:11<02:43,  1.00s/it] 94%|█████████▍| 2580/2742 [1:04:12<02:41,  1.01it/s]                                                     {'loss': 1.168, 'learning_rate': 4.45398706796471e-07, 'epoch': 2.82}
 94%|█████████▍| 2580/2742 [1:04:12<02:41,  1.01it/s] 94%|█████████▍| 2581/2742 [1:04:13<02:43,  1.02s/it] 94%|█████████▍| 2582/2742 [1:04:14<02:36,  1.02it/s] 94%|█████████▍| 2583/2742 [1:04:15<02:34,  1.03it/s] 94%|█████████▍| 2584/2742 [1:04:16<02:30,  1.05it/s] 94%|█████████▍| 2585/2742 [1:04:17<02:38,  1.01s/it]                                                     {'loss': 1.2871, 'learning_rate': 4.188883422042156e-07, 'epoch': 2.83}
 94%|█████████▍| 2585/2742 [1:04:17<02:38,  1.01s/it] 94%|█████████▍| 2586/2742 [1:04:18<02:33,  1.02it/s] 94%|█████████▍| 2587/2742 [1:04:19<02:39,  1.03s/it] 94%|█████████▍| 2588/2742 [1:04:20<02:34,  1.00s/it] 94%|█████████▍| 2589/2742 [1:04:21<02:30,  1.02it/s] 94%|█████████▍| 2590/2742 [1:04:22<02:28,  1.03it/s]                                                     {'loss': 0.9956, 'learning_rate': 3.9318466484747077e-07, 'epoch': 2.83}
 94%|█████████▍| 2590/2742 [1:04:22<02:28,  1.03it/s] 94%|█████████▍| 2591/2742 [1:04:23<02:27,  1.03it/s] 95%|█████████▍| 2592/2742 [1:04:24<02:26,  1.02it/s] 95%|█████████▍| 2593/2742 [1:04:25<02:25,  1.02it/s] 95%|█████████▍| 2594/2742 [1:04:26<02:24,  1.02it/s] 95%|█████████▍| 2595/2742 [1:04:27<02:30,  1.02s/it]                                                     {'loss': 1.1052, 'learning_rate': 3.682885182531154e-07, 'epoch': 2.84}
 95%|█████████▍| 2595/2742 [1:04:27<02:30,  1.02s/it] 95%|█████████▍| 2596/2742 [1:04:28<02:25,  1.00it/s] 95%|█████████▍| 2597/2742 [1:04:29<02:23,  1.01it/s] 95%|█████████▍| 2598/2742 [1:04:30<02:22,  1.01it/s] 95%|█████████▍| 2599/2742 [1:04:31<02:18,  1.03it/s] 95%|█████████▍| 2600/2742 [1:04:32<02:16,  1.04it/s]                                                     {'loss': 1.3037, 'learning_rate': 3.4420071944700484e-07, 'epoch': 2.84}
 95%|█████████▍| 2600/2742 [1:04:32<02:16,  1.04it/s]